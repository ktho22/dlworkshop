{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Train  and test LeNet using caffe\n",
    "Jupyter notebook을 사용하여 Linux Shell 명령어로 caffe를 사용하는 방법에 대해서 배웁니다.\n",
    "\n",
    "본 예제는 크게 두 Part로 나누어져 있습니다.\n",
    "1. Caffe 명령어를 통해 모델을 학습하는 방법\n",
    "2. 학습된 모델의 test data에 대한 Classification 성능을 측정하는 방법\n",
    "\n",
    "### 제공해드린 Docker 이미지 상에 이미 Caffe가 ./caffe에 설치되어 있으며, 이를 기준으로 본 tutorial이 작성되어 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Basic Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Jupyter Notebook에서 Linux Shell 명령어를 사용하기 위해서는 명령앞에 !를 붙여서 실행하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/dlworkshop/2_caffe_intro\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-train-and-test-lenet.ipynb\t 03-PyCaffe-lenet-train.ipynb  image.jpg\r\n",
      "02-PyCaffe-classification.ipynb  caffe\t\t\t       lenet\r\n"
     ]
    }
   ],
   "source": [
    "!ls ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training LeNet 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=./lenet/lenet5.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. Convert MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본 예제에서 사용할 데이터셋은 MNIST로 0~9까지의 숫자 이미지와 이미지에 해당하는 Label을 포함합니다.\n",
    "MNIST 데이터셋에 대해 자세한 내용은 http://yann.lecun.com/exdb/mnist 를 참고해 주세요.\n",
    "\n",
    "Raw MNIST dataset은 제공해드린 docker 이미지상에 이미 다운로드 되어 있으며, 해당 위치는 ./caffe/data/mnist 입니다.\n",
    "하지만 Raw MNIST dataset의 형식은 caffe에서 지원되지 않기에 caffe에서 가장 많이 쓰는 LMDB형식으로 변환되어야 합니다.\n",
    "변환에 필요한 코드는 이미 작성되어 있으며 ./lenet/create_mnist.sh 을 실행하여 변환합니다.\n",
    "\n",
    "스크립트 실행시 발생하는 libdc1394 error는 docker의 driver상의 오류로 무시하셔도 됩니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating lmdb...\n",
      "libdc1394 error: Failed to initialize libdc1394\n",
      "libdc1394 error: Failed to initialize libdc1394\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "!sh ./lenet/create_mnist.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "변환이 완료된 후, !ls ./lenet 명령어를 통해 mnist_test_lmdb와 mnist_train_lmdb가 정상적으로 생성되었음을 확인하실 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./lenet/mnist_test_lmdb:\r\n",
      "data.mdb  lock.mdb\r\n",
      "\r\n",
      "./lenet/mnist_train_lmdb:\r\n",
      "data.mdb  lock.mdb\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./lenet/*_lmdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2. Define model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet5의 모델은 ./lenet/lenet_train_test.prototxt에 정의되어 있습니다. \n",
    "\n",
    "cat 명령어는 Linux에서 text파일을 모니터에 출력해주는 함수로 prototxt 파일을 읽어올 수 있습니다.\n",
    "http://caffe.berkeleyvision.org/tutorial/layers.html 를 참고하여 각 layer가 요구하는 parameter를 참고하세요.\n",
    "\n",
    "prototxt는 jupyter notebook 상에서 해당 파일 ./lenet/lenet_train_test.prototxt 을 클릭하는 것으로 수정하실 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"LeNet\"\r\n",
      "layer {\r\n",
      "  name: \"mnist\"\r\n",
      "  type: \"Data\"\r\n",
      "  top: \"data\"\r\n",
      "  top: \"label\"\r\n",
      "  include {\r\n",
      "    phase: TRAIN\r\n",
      "  }\r\n",
      "  transform_param {\r\n",
      "    scale: 0.00390625\r\n",
      "  }\r\n",
      "  data_param {\r\n",
      "    source: \"./lenet/mnist_train_lmdb\"\r\n",
      "    batch_size: 64\r\n",
      "    backend: LMDB\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"mnist\"\r\n",
      "  type: \"Data\"\r\n",
      "  top: \"data\"\r\n",
      "  top: \"label\"\r\n",
      "  include {\r\n",
      "    phase: TEST\r\n",
      "  }\r\n",
      "  transform_param {\r\n",
      "    scale: 0.00390625\r\n",
      "  }\r\n",
      "  data_param {\r\n",
      "    source: \"./lenet/mnist_test_lmdb\"\r\n",
      "    batch_size: 100\r\n",
      "    backend: LMDB\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"conv1\"\r\n",
      "  type: \"Convolution\"\r\n",
      "  bottom: \"data\"\r\n",
      "  top: \"conv1\"\r\n",
      "  param {\r\n",
      "    lr_mult: 1\r\n",
      "  }\r\n",
      "  param {\r\n",
      "    lr_mult: 2\r\n",
      "  }\r\n",
      "  convolution_param {\r\n",
      "    num_output: 20\r\n",
      "    kernel_size: 5\r\n",
      "    stride: 1\r\n",
      "    weight_filler {\r\n",
      "      type: \"xavier\"\r\n",
      "    }\r\n",
      "    bias_filler {\r\n",
      "      type: \"constant\"\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"pool1\"\r\n",
      "  type: \"Pooling\"\r\n",
      "  bottom: \"conv1\"\r\n",
      "  top: \"pool1\"\r\n",
      "  pooling_param {\r\n",
      "    pool: MAX\r\n",
      "    kernel_size: 2\r\n",
      "    stride: 2\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"conv2\"\r\n",
      "  type: \"Convolution\"\r\n",
      "  bottom: \"pool1\"\r\n",
      "  top: \"conv2\"\r\n",
      "  param {\r\n",
      "    lr_mult: 1\r\n",
      "  }\r\n",
      "  param {\r\n",
      "    lr_mult: 2\r\n",
      "  }\r\n",
      "  convolution_param {\r\n",
      "    num_output: 50\r\n",
      "    kernel_size: 5\r\n",
      "    stride: 1\r\n",
      "    weight_filler {\r\n",
      "      type: \"xavier\"\r\n",
      "    }\r\n",
      "    bias_filler {\r\n",
      "      type: \"constant\"\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"pool2\"\r\n",
      "  type: \"Pooling\"\r\n",
      "  bottom: \"conv2\"\r\n",
      "  top: \"pool2\"\r\n",
      "  pooling_param {\r\n",
      "    pool: MAX\r\n",
      "    kernel_size: 2\r\n",
      "    stride: 2\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"ip1\"\r\n",
      "  type: \"InnerProduct\"\r\n",
      "  bottom: \"pool2\"\r\n",
      "  top: \"ip1\"\r\n",
      "  param {\r\n",
      "    lr_mult: 1\r\n",
      "  }\r\n",
      "  param {\r\n",
      "    lr_mult: 2\r\n",
      "  }\r\n",
      "  inner_product_param {\r\n",
      "    num_output: 500\r\n",
      "    weight_filler {\r\n",
      "      type: \"xavier\"\r\n",
      "    }\r\n",
      "    bias_filler {\r\n",
      "      type: \"constant\"\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"relu1\"\r\n",
      "  type: \"ReLU\"\r\n",
      "  bottom: \"ip1\"\r\n",
      "  top: \"ip1\"\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"ip2\"\r\n",
      "  type: \"InnerProduct\"\r\n",
      "  bottom: \"ip1\"\r\n",
      "  top: \"ip2\"\r\n",
      "  param {\r\n",
      "    lr_mult: 1\r\n",
      "  }\r\n",
      "  param {\r\n",
      "    lr_mult: 2\r\n",
      "  }\r\n",
      "  inner_product_param {\r\n",
      "    num_output: 10\r\n",
      "    weight_filler {\r\n",
      "      type: \"xavier\"\r\n",
      "    }\r\n",
      "    bias_filler {\r\n",
      "      type: \"constant\"\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"accuracy\"\r\n",
      "  type: \"Accuracy\"\r\n",
      "  bottom: \"ip2\"\r\n",
      "  bottom: \"label\"\r\n",
      "  top: \"accuracy\"\r\n",
      "  include {\r\n",
      "    phase: TEST\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"loss\"\r\n",
      "  type: \"SoftmaxWithLoss\"\r\n",
      "  bottom: \"ip2\"\r\n",
      "  bottom: \"label\"\r\n",
      "  top: \"loss\"\r\n",
      "}\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./lenet/lenet_train_test.prototxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-3. Define solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network를 학습하기 위해서 Solver를 prototxt형식으로 정의합니다. \n",
    "\n",
    "Solver hyperparameters\n",
    "- Solver: SGD (Stochastic gradient descent)\n",
    "- Base Learning Rate: 0.01 (Staring learning rate)\n",
    "- Momemtum: 0.9 (SGD parameter)\n",
    "- weight_decay: 0.005\n",
    "- lr_policy: inv \n",
    "- Gamma: 0.0001\n",
    "- Power: 0.75\n",
    "- solver_mode: CPU  \n",
    "\n",
    "Display param\n",
    "- display: 100 (training 100 batch 마다 Loss 출력)\n",
    "- test_iter: 100 (validation data시 100개의 batch를 테스트)\n",
    "- test_interval: 500 (training 500 batch 마다 validation data test)\n",
    "- max_iter: 10000 (10000 batch training 도달시 종료)\n",
    "\n",
    "Snapshot param\n",
    "- snapshot: 5000 (5000 batch training 마다 모델 저장)\n",
    "- snapshot_prefix: ./lenet/lenet (모델 저장 위치와 이름에 대한 prefix)\n",
    "\n",
    "위의 내용을 caffe의 prototxt 형식으로 나타내면 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# The train/test net protocol buffer definition\r\n",
      "net: \"./lenet/lenet_train_test.prototxt\"\r\n",
      "# test_iter specifies how many forward passes the test should carry out.\r\n",
      "# In the case of MNIST, we have test batch size 100 and 100 test iterations,\r\n",
      "# covering the full 10,000 testing images.\r\n",
      "test_iter: 100\r\n",
      "# Carry out testing every 500 training iterations.\r\n",
      "test_interval: 500\r\n",
      "# The base learning rate, momentum and the weight decay of the network.\r\n",
      "base_lr: 0.01\r\n",
      "momentum: 0.9\r\n",
      "weight_decay: 0.0005\r\n",
      "# The learning rate policy\r\n",
      "lr_policy: \"inv\"\r\n",
      "gamma: 0.0001\r\n",
      "power: 0.75\r\n",
      "# Display every 100 iterations\r\n",
      "display: 100\r\n",
      "# The maximum number of iterations\r\n",
      "max_iter: 10000\r\n",
      "# snapshot intermediate results\r\n",
      "snapshot: 5000\r\n",
      "snapshot_prefix: \"./lenet/lenet\"\r\n",
      "# solver mode: CPU or GPU\r\n",
      "solver_mode: CPU\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./lenet/lenet_solver.prototxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-4. Train LeNet5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "빌드된 Caffe의 실행파일은 ./caffe/build/tools/caffe에 존재합니다.\n",
    "\n",
    "caffe 실행파일의 사용법(commands & arguments)을 보기 위해서 아래의 명령어를 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libdc1394 error: Failed to initialize libdc1394\r\n",
      "caffe: command line brew\r\n",
      "usage: caffe <command> <args>\r\n",
      "\r\n",
      "commands:\r\n",
      "  train           train or finetune a model\r\n",
      "  test            score a model\r\n",
      "  device_query    show GPU diagnostic information\r\n",
      "  time            benchmark model execution time\r\n",
      "\r\n",
      "  Flags from /root/caffe/tools/caffe.cpp:\r\n",
      "    -gpu (Optional; run in GPU mode on given device IDs separated by ','.Use\r\n",
      "      '-gpu all' to run on all available GPUs. The effective training batch\r\n",
      "      size is multiplied by the number of devices.) type: string default: \"\"\r\n",
      "    -iterations (The number of iterations to run.) type: int32 default: 50\r\n",
      "    -model (The model definition protocol buffer text file.) type: string\r\n",
      "      default: \"\"\r\n",
      "    -sighup_effect (Optional; action to take when a SIGHUP signal is received:\r\n",
      "      snapshot, stop or none.) type: string default: \"snapshot\"\r\n",
      "    -sigint_effect (Optional; action to take when a SIGINT signal is received:\r\n",
      "      snapshot, stop or none.) type: string default: \"stop\"\r\n",
      "    -snapshot (Optional; the snapshot solver state to resume training.)\r\n",
      "      type: string default: \"\"\r\n",
      "    -solver (The solver definition protocol buffer text file.) type: string\r\n",
      "      default: \"\"\r\n",
      "    -weights (Optional; the pretrained weights to initialize finetuning,\r\n",
      "      separated by ','. Cannot be set simultaneously with snapshot.)\r\n",
      "      type: string default: \"\"\r\n"
     ]
    }
   ],
   "source": [
    "!./caffe/build/tools/caffe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 간단하게는 ./caffe/build/tools/caffe train --solver=/path/to/solver/file.prototxt로 정의된 모델을 정의된 solver로 학습시킬 수 있습니다.\n",
    "\n",
    "Desktop에서 모델을 전부 학습하기 위해서는 약 10분 이상이 소요됩니다. (노트북의 경우 20분 이상이 소요될 수 있습니다.)\n",
    "\n",
    "#### 중단을 원하시면 Jupyter notebook 상단의 Kernel 탭 클릭 후 Interrupt를 클릭해주세요.\n",
    "#### Shell 상에서 중단 (Jupyter notebook 상의 kernel interrupt)이 될 경우 중단 시점에서의 snapshot이 자동저장됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter notebook 상의 ln [\\*] 표시는 현재 처리중임을 의미하며, 완료가 되면 \\* 기호는 숫자로 변경됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libdc1394 error: Failed to initialize libdc1394\n",
      "I0428 05:45:21.979524  2380 caffe.cpp:178] Use CPU.\n",
      "I0428 05:45:21.980160  2380 solver.cpp:48] Initializing solver from parameters: \n",
      "test_iter: 100\n",
      "test_interval: 500\n",
      "base_lr: 0.01\n",
      "display: 100\n",
      "max_iter: 10000\n",
      "lr_policy: \"inv\"\n",
      "gamma: 0.0001\n",
      "power: 0.75\n",
      "momentum: 0.9\n",
      "weight_decay: 0.0005\n",
      "snapshot: 5000\n",
      "snapshot_prefix: \"./lenet/lenet\"\n",
      "solver_mode: CPU\n",
      "net: \"./lenet/lenet_train_test.prototxt\"\n",
      "I0428 05:45:21.980418  2380 solver.cpp:91] Creating training net from net file: ./lenet/lenet_train_test.prototxt\n",
      "I0428 05:45:21.980882  2380 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist\n",
      "I0428 05:45:21.980928  2380 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy\n",
      "I0428 05:45:21.981068  2380 net.cpp:49] Initializing net from parameters: \n",
      "name: \"LeNet\"\n",
      "state {\n",
      "  phase: TRAIN\n",
      "}\n",
      "layer {\n",
      "  name: \"mnist\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TRAIN\n",
      "  }\n",
      "  transform_param {\n",
      "    scale: 0.00390625\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"./lenet/mnist_train_lmdb\"\n",
      "    batch_size: 64\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 20\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 50\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"ip1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"ip1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 500\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"ip2\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 10\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"ip2\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I0428 05:45:21.981879  2380 layer_factory.hpp:77] Creating layer mnist\n",
      "I0428 05:45:21.982650  2380 net.cpp:91] Creating Layer mnist\n",
      "I0428 05:45:21.982691  2380 net.cpp:399] mnist -> data\n",
      "I0428 05:45:21.982743  2380 net.cpp:399] mnist -> label\n",
      "I0428 05:45:21.982913  2381 db_lmdb.cpp:38] Opened lmdb ./lenet/mnist_train_lmdb\n",
      "I0428 05:45:21.983067  2380 data_layer.cpp:41] output data size: 64,1,28,28\n",
      "I0428 05:45:21.983817  2380 net.cpp:141] Setting up mnist\n",
      "I0428 05:45:21.983853  2380 net.cpp:148] Top shape: 64 1 28 28 (50176)\n",
      "I0428 05:45:21.983875  2380 net.cpp:148] Top shape: 64 (64)\n",
      "I0428 05:45:21.983889  2380 net.cpp:156] Memory required for data: 200960\n",
      "I0428 05:45:21.983912  2380 layer_factory.hpp:77] Creating layer conv1\n",
      "I0428 05:45:21.983947  2380 net.cpp:91] Creating Layer conv1\n",
      "I0428 05:45:21.983968  2380 net.cpp:425] conv1 <- data\n",
      "I0428 05:45:21.983996  2380 net.cpp:399] conv1 -> conv1\n",
      "I0428 05:45:21.984089  2380 net.cpp:141] Setting up conv1\n",
      "I0428 05:45:21.984117  2380 net.cpp:148] Top shape: 64 20 24 24 (737280)\n",
      "I0428 05:45:21.984133  2380 net.cpp:156] Memory required for data: 3150080\n",
      "I0428 05:45:21.984169  2380 layer_factory.hpp:77] Creating layer pool1\n",
      "I0428 05:45:21.984192  2380 net.cpp:91] Creating Layer pool1\n",
      "I0428 05:45:21.984212  2380 net.cpp:425] pool1 <- conv1\n",
      "I0428 05:45:21.984233  2380 net.cpp:399] pool1 -> pool1\n",
      "I0428 05:45:21.984275  2380 net.cpp:141] Setting up pool1\n",
      "I0428 05:45:21.984330  2380 net.cpp:148] Top shape: 64 20 12 12 (184320)\n",
      "I0428 05:45:21.984347  2380 net.cpp:156] Memory required for data: 3887360\n",
      "I0428 05:45:21.984365  2380 layer_factory.hpp:77] Creating layer conv2\n",
      "I0428 05:45:21.984391  2380 net.cpp:91] Creating Layer conv2\n",
      "I0428 05:45:21.984410  2380 net.cpp:425] conv2 <- pool1\n",
      "I0428 05:45:21.984431  2380 net.cpp:399] conv2 -> conv2\n",
      "I0428 05:45:21.984828  2380 net.cpp:141] Setting up conv2\n",
      "I0428 05:45:21.984865  2380 net.cpp:148] Top shape: 64 50 8 8 (204800)\n",
      "I0428 05:45:21.984881  2380 net.cpp:156] Memory required for data: 4706560\n",
      "I0428 05:45:21.984908  2380 layer_factory.hpp:77] Creating layer pool2\n",
      "I0428 05:45:21.984946  2380 net.cpp:91] Creating Layer pool2\n",
      "I0428 05:45:21.984963  2380 net.cpp:425] pool2 <- conv2\n",
      "I0428 05:45:21.984983  2380 net.cpp:399] pool2 -> pool2\n",
      "I0428 05:45:21.985009  2380 net.cpp:141] Setting up pool2\n",
      "I0428 05:45:21.985028  2380 net.cpp:148] Top shape: 64 50 4 4 (51200)\n",
      "I0428 05:45:21.985044  2380 net.cpp:156] Memory required for data: 4911360\n",
      "I0428 05:45:21.985061  2380 layer_factory.hpp:77] Creating layer ip1\n",
      "I0428 05:45:21.985082  2380 net.cpp:91] Creating Layer ip1\n",
      "I0428 05:45:21.985100  2380 net.cpp:425] ip1 <- pool2\n",
      "I0428 05:45:21.985121  2380 net.cpp:399] ip1 -> ip1\n",
      "I0428 05:45:21.990738  2380 net.cpp:141] Setting up ip1\n",
      "I0428 05:45:21.990782  2380 net.cpp:148] Top shape: 64 500 (32000)\n",
      "I0428 05:45:21.990800  2380 net.cpp:156] Memory required for data: 5039360\n",
      "I0428 05:45:21.990828  2380 layer_factory.hpp:77] Creating layer relu1\n",
      "I0428 05:45:21.990849  2380 net.cpp:91] Creating Layer relu1\n",
      "I0428 05:45:21.990864  2380 net.cpp:425] relu1 <- ip1\n",
      "I0428 05:45:21.990881  2380 net.cpp:386] relu1 -> ip1 (in-place)\n",
      "I0428 05:45:21.990906  2380 net.cpp:141] Setting up relu1\n",
      "I0428 05:45:21.990922  2380 net.cpp:148] Top shape: 64 500 (32000)\n",
      "I0428 05:45:21.990936  2380 net.cpp:156] Memory required for data: 5167360\n",
      "I0428 05:45:21.990952  2380 layer_factory.hpp:77] Creating layer ip2\n",
      "I0428 05:45:21.990970  2380 net.cpp:91] Creating Layer ip2\n",
      "I0428 05:45:21.990985  2380 net.cpp:425] ip2 <- ip1\n",
      "I0428 05:45:21.991005  2380 net.cpp:399] ip2 -> ip2\n",
      "I0428 05:45:21.991109  2380 net.cpp:141] Setting up ip2\n",
      "I0428 05:45:21.991130  2380 net.cpp:148] Top shape: 64 10 (640)\n",
      "I0428 05:45:21.991145  2380 net.cpp:156] Memory required for data: 5169920\n",
      "I0428 05:45:21.991164  2380 layer_factory.hpp:77] Creating layer loss\n",
      "I0428 05:45:21.991186  2380 net.cpp:91] Creating Layer loss\n",
      "I0428 05:45:21.991202  2380 net.cpp:425] loss <- ip2\n",
      "I0428 05:45:21.991219  2380 net.cpp:425] loss <- label\n",
      "I0428 05:45:21.991237  2380 net.cpp:399] loss -> loss\n",
      "I0428 05:45:21.991266  2380 layer_factory.hpp:77] Creating layer loss\n",
      "I0428 05:45:21.991299  2380 net.cpp:141] Setting up loss\n",
      "I0428 05:45:21.991318  2380 net.cpp:148] Top shape: (1)\n",
      "I0428 05:45:21.991333  2380 net.cpp:151]     with loss weight 1\n",
      "I0428 05:45:21.991364  2380 net.cpp:156] Memory required for data: 5169924\n",
      "I0428 05:45:21.991379  2380 net.cpp:217] loss needs backward computation.\n",
      "I0428 05:45:21.991394  2380 net.cpp:217] ip2 needs backward computation.\n",
      "I0428 05:45:21.991410  2380 net.cpp:217] relu1 needs backward computation.\n",
      "I0428 05:45:21.991423  2380 net.cpp:217] ip1 needs backward computation.\n",
      "I0428 05:45:21.991437  2380 net.cpp:217] pool2 needs backward computation.\n",
      "I0428 05:45:21.991452  2380 net.cpp:217] conv2 needs backward computation.\n",
      "I0428 05:45:21.991467  2380 net.cpp:217] pool1 needs backward computation.\n",
      "I0428 05:45:21.991482  2380 net.cpp:217] conv1 needs backward computation.\n",
      "I0428 05:45:21.991497  2380 net.cpp:219] mnist does not need backward computation.\n",
      "I0428 05:45:21.991511  2380 net.cpp:261] This network produces output loss\n",
      "I0428 05:45:21.991534  2380 net.cpp:274] Network initialization done.\n",
      "I0428 05:45:21.991982  2380 solver.cpp:181] Creating test net (#0) specified by net file: ./lenet/lenet_train_test.prototxt\n",
      "I0428 05:45:21.992044  2380 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist\n",
      "I0428 05:45:21.992188  2380 net.cpp:49] Initializing net from parameters: \n",
      "name: \"LeNet\"\n",
      "state {\n",
      "  phase: TEST\n",
      "}\n",
      "layer {\n",
      "  name: \"mnist\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "  transform_param {\n",
      "    scale: 0.00390625\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"./lenet/mnist_test_lmdb\"\n",
      "    batch_size: 100\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 20\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 50\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"ip1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"ip1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 500\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"ip2\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 10\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"accuracy\"\n",
      "  type: \"Accuracy\"\n",
      "  bottom: \"ip2\"\n",
      "  bottom: \"label\"\n",
      "  top: \"accuracy\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"ip2\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I0428 05:45:21.993011  2380 layer_factory.hpp:77] Creating layer mnist\n",
      "I0428 05:45:21.993819  2380 net.cpp:91] Creating Layer mnist\n",
      "I0428 05:45:21.993855  2380 net.cpp:399] mnist -> data\n",
      "I0428 05:45:21.993883  2380 net.cpp:399] mnist -> label\n",
      "I0428 05:45:21.994035  2383 db_lmdb.cpp:38] Opened lmdb ./lenet/mnist_test_lmdb\n",
      "I0428 05:45:21.994148  2380 data_layer.cpp:41] output data size: 100,1,28,28\n",
      "I0428 05:45:21.995409  2380 net.cpp:141] Setting up mnist\n",
      "I0428 05:45:21.995443  2380 net.cpp:148] Top shape: 100 1 28 28 (78400)\n",
      "I0428 05:45:21.995461  2380 net.cpp:148] Top shape: 100 (100)\n",
      "I0428 05:45:21.995476  2380 net.cpp:156] Memory required for data: 314000\n",
      "I0428 05:45:21.995491  2380 layer_factory.hpp:77] Creating layer label_mnist_1_split\n",
      "I0428 05:45:21.995527  2380 net.cpp:91] Creating Layer label_mnist_1_split\n",
      "I0428 05:45:21.995544  2380 net.cpp:425] label_mnist_1_split <- label\n",
      "I0428 05:45:21.995565  2380 net.cpp:399] label_mnist_1_split -> label_mnist_1_split_0\n",
      "I0428 05:45:21.995587  2380 net.cpp:399] label_mnist_1_split -> label_mnist_1_split_1\n",
      "I0428 05:45:21.995611  2380 net.cpp:141] Setting up label_mnist_1_split\n",
      "I0428 05:45:21.995630  2380 net.cpp:148] Top shape: 100 (100)\n",
      "I0428 05:45:21.995646  2380 net.cpp:148] Top shape: 100 (100)\n",
      "I0428 05:45:21.995661  2380 net.cpp:156] Memory required for data: 314800\n",
      "I0428 05:45:21.995676  2380 layer_factory.hpp:77] Creating layer conv1\n",
      "I0428 05:45:21.995709  2380 net.cpp:91] Creating Layer conv1\n",
      "I0428 05:45:21.995726  2380 net.cpp:425] conv1 <- data\n",
      "I0428 05:45:21.995750  2380 net.cpp:399] conv1 -> conv1\n",
      "I0428 05:45:21.995816  2380 net.cpp:141] Setting up conv1\n",
      "I0428 05:45:21.995853  2380 net.cpp:148] Top shape: 100 20 24 24 (1152000)\n",
      "I0428 05:45:21.995868  2380 net.cpp:156] Memory required for data: 4922800\n",
      "I0428 05:45:21.995894  2380 layer_factory.hpp:77] Creating layer pool1\n",
      "I0428 05:45:21.995919  2380 net.cpp:91] Creating Layer pool1\n",
      "I0428 05:45:21.995935  2380 net.cpp:425] pool1 <- conv1\n",
      "I0428 05:45:21.995954  2380 net.cpp:399] pool1 -> pool1\n",
      "I0428 05:45:21.996007  2380 net.cpp:141] Setting up pool1\n",
      "I0428 05:45:21.996029  2380 net.cpp:148] Top shape: 100 20 12 12 (288000)\n",
      "I0428 05:45:21.996043  2380 net.cpp:156] Memory required for data: 6074800\n",
      "I0428 05:45:21.996058  2380 layer_factory.hpp:77] Creating layer conv2\n",
      "I0428 05:45:21.996088  2380 net.cpp:91] Creating Layer conv2\n",
      "I0428 05:45:21.996116  2380 net.cpp:425] conv2 <- pool1\n",
      "I0428 05:45:21.996140  2380 net.cpp:399] conv2 -> conv2\n",
      "I0428 05:45:21.996486  2380 net.cpp:141] Setting up conv2\n",
      "I0428 05:45:21.996520  2380 net.cpp:148] Top shape: 100 50 8 8 (320000)\n",
      "I0428 05:45:21.996536  2380 net.cpp:156] Memory required for data: 7354800\n",
      "I0428 05:45:21.996561  2380 layer_factory.hpp:77] Creating layer pool2\n",
      "I0428 05:45:21.996580  2380 net.cpp:91] Creating Layer pool2\n",
      "I0428 05:45:21.996597  2380 net.cpp:425] pool2 <- conv2\n",
      "I0428 05:45:21.996614  2380 net.cpp:399] pool2 -> pool2\n",
      "I0428 05:45:21.996639  2380 net.cpp:141] Setting up pool2\n",
      "I0428 05:45:21.996656  2380 net.cpp:148] Top shape: 100 50 4 4 (80000)\n",
      "I0428 05:45:21.996670  2380 net.cpp:156] Memory required for data: 7674800\n",
      "I0428 05:45:21.996685  2380 layer_factory.hpp:77] Creating layer ip1\n",
      "I0428 05:45:21.996709  2380 net.cpp:91] Creating Layer ip1\n",
      "I0428 05:45:21.996726  2380 net.cpp:425] ip1 <- pool2\n",
      "I0428 05:45:21.996752  2380 net.cpp:399] ip1 -> ip1\n",
      "I0428 05:45:22.001524  2380 net.cpp:141] Setting up ip1\n",
      "I0428 05:45:22.001561  2380 net.cpp:148] Top shape: 100 500 (50000)\n",
      "I0428 05:45:22.001576  2380 net.cpp:156] Memory required for data: 7874800\n",
      "I0428 05:45:22.001598  2380 layer_factory.hpp:77] Creating layer relu1\n",
      "I0428 05:45:22.001616  2380 net.cpp:91] Creating Layer relu1\n",
      "I0428 05:45:22.001632  2380 net.cpp:425] relu1 <- ip1\n",
      "I0428 05:45:22.001648  2380 net.cpp:386] relu1 -> ip1 (in-place)\n",
      "I0428 05:45:22.001667  2380 net.cpp:141] Setting up relu1\n",
      "I0428 05:45:22.001682  2380 net.cpp:148] Top shape: 100 500 (50000)\n",
      "I0428 05:45:22.001694  2380 net.cpp:156] Memory required for data: 8074800\n",
      "I0428 05:45:22.001708  2380 layer_factory.hpp:77] Creating layer ip2\n",
      "I0428 05:45:22.001732  2380 net.cpp:91] Creating Layer ip2\n",
      "I0428 05:45:22.001747  2380 net.cpp:425] ip2 <- ip1\n",
      "I0428 05:45:22.001768  2380 net.cpp:399] ip2 -> ip2\n",
      "I0428 05:45:22.001857  2380 net.cpp:141] Setting up ip2\n",
      "I0428 05:45:22.001875  2380 net.cpp:148] Top shape: 100 10 (1000)\n",
      "I0428 05:45:22.001888  2380 net.cpp:156] Memory required for data: 8078800\n",
      "I0428 05:45:22.001905  2380 layer_factory.hpp:77] Creating layer ip2_ip2_0_split\n",
      "I0428 05:45:22.001922  2380 net.cpp:91] Creating Layer ip2_ip2_0_split\n",
      "I0428 05:45:22.001936  2380 net.cpp:425] ip2_ip2_0_split <- ip2\n",
      "I0428 05:45:22.001956  2380 net.cpp:399] ip2_ip2_0_split -> ip2_ip2_0_split_0\n",
      "I0428 05:45:22.001976  2380 net.cpp:399] ip2_ip2_0_split -> ip2_ip2_0_split_1\n",
      "I0428 05:45:22.001994  2380 net.cpp:141] Setting up ip2_ip2_0_split\n",
      "I0428 05:45:22.002010  2380 net.cpp:148] Top shape: 100 10 (1000)\n",
      "I0428 05:45:22.002025  2380 net.cpp:148] Top shape: 100 10 (1000)\n",
      "I0428 05:45:22.002038  2380 net.cpp:156] Memory required for data: 8086800\n",
      "I0428 05:45:22.002051  2380 layer_factory.hpp:77] Creating layer accuracy\n",
      "I0428 05:45:22.002068  2380 net.cpp:91] Creating Layer accuracy\n",
      "I0428 05:45:22.002081  2380 net.cpp:425] accuracy <- ip2_ip2_0_split_0\n",
      "I0428 05:45:22.002096  2380 net.cpp:425] accuracy <- label_mnist_1_split_0\n",
      "I0428 05:45:22.002113  2380 net.cpp:399] accuracy -> accuracy\n",
      "I0428 05:45:22.002132  2380 net.cpp:141] Setting up accuracy\n",
      "I0428 05:45:22.002148  2380 net.cpp:148] Top shape: (1)\n",
      "I0428 05:45:22.002161  2380 net.cpp:156] Memory required for data: 8086804\n",
      "I0428 05:45:22.002176  2380 layer_factory.hpp:77] Creating layer loss\n",
      "I0428 05:45:22.002195  2380 net.cpp:91] Creating Layer loss\n",
      "I0428 05:45:22.002209  2380 net.cpp:425] loss <- ip2_ip2_0_split_1\n",
      "I0428 05:45:22.002224  2380 net.cpp:425] loss <- label_mnist_1_split_1\n",
      "I0428 05:45:22.002240  2380 net.cpp:399] loss -> loss\n",
      "I0428 05:45:22.002260  2380 layer_factory.hpp:77] Creating layer loss\n",
      "I0428 05:45:22.002296  2380 net.cpp:141] Setting up loss\n",
      "I0428 05:45:22.002315  2380 net.cpp:148] Top shape: (1)\n",
      "I0428 05:45:22.002351  2380 net.cpp:151]     with loss weight 1\n",
      "I0428 05:45:22.002372  2380 net.cpp:156] Memory required for data: 8086808\n",
      "I0428 05:45:22.002385  2380 net.cpp:217] loss needs backward computation.\n",
      "I0428 05:45:22.002399  2380 net.cpp:219] accuracy does not need backward computation.\n",
      "I0428 05:45:22.002413  2380 net.cpp:217] ip2_ip2_0_split needs backward computation.\n",
      "I0428 05:45:22.002427  2380 net.cpp:217] ip2 needs backward computation.\n",
      "I0428 05:45:22.002440  2380 net.cpp:217] relu1 needs backward computation.\n",
      "I0428 05:45:22.002454  2380 net.cpp:217] ip1 needs backward computation.\n",
      "I0428 05:45:22.002466  2380 net.cpp:217] pool2 needs backward computation.\n",
      "I0428 05:45:22.002480  2380 net.cpp:217] conv2 needs backward computation.\n",
      "I0428 05:45:22.002493  2380 net.cpp:217] pool1 needs backward computation.\n",
      "I0428 05:45:22.002507  2380 net.cpp:217] conv1 needs backward computation.\n",
      "I0428 05:45:22.002521  2380 net.cpp:219] label_mnist_1_split does not need backward computation.\n",
      "I0428 05:45:22.002537  2380 net.cpp:219] mnist does not need backward computation.\n",
      "I0428 05:45:22.002549  2380 net.cpp:261] This network produces output accuracy\n",
      "I0428 05:45:22.002563  2380 net.cpp:261] This network produces output loss\n",
      "I0428 05:45:22.002590  2380 net.cpp:274] Network initialization done.\n",
      "I0428 05:45:22.002660  2380 solver.cpp:60] Solver scaffolding done.\n",
      "I0428 05:45:22.002701  2380 caffe.cpp:219] Starting Optimization\n",
      "I0428 05:45:22.002717  2380 solver.cpp:279] Solving LeNet\n",
      "I0428 05:45:22.002729  2380 solver.cpp:280] Learning Rate Policy: inv\n",
      "I0428 05:45:22.003887  2380 solver.cpp:337] Iteration 0, Testing net (#0)\n",
      "I0428 05:45:26.270261  2380 solver.cpp:404]     Test net output #0: accuracy = 0.0841\n",
      "I0428 05:45:26.270308  2380 solver.cpp:404]     Test net output #1: loss = 2.36111 (* 1 = 2.36111 loss)\n",
      "I0428 05:45:26.339987  2380 solver.cpp:228] Iteration 0, loss = 2.3882\n",
      "I0428 05:45:26.340042  2380 solver.cpp:244]     Train net output #0: loss = 2.3882 (* 1 = 2.3882 loss)\n",
      "I0428 05:45:26.340061  2380 sgd_solver.cpp:106] Iteration 0, lr = 0.01\n",
      "I0428 05:45:32.977524  2380 solver.cpp:228] Iteration 100, loss = 0.238312\n",
      "I0428 05:45:32.977597  2380 solver.cpp:244]     Train net output #0: loss = 0.238312 (* 1 = 0.238312 loss)\n",
      "I0428 05:45:32.977609  2380 sgd_solver.cpp:106] Iteration 100, lr = 0.00992565\n",
      "I0428 05:45:39.632803  2380 solver.cpp:228] Iteration 200, loss = 0.141801\n",
      "I0428 05:45:39.632874  2380 solver.cpp:244]     Train net output #0: loss = 0.141801 (* 1 = 0.141801 loss)\n",
      "I0428 05:45:39.632885  2380 sgd_solver.cpp:106] Iteration 200, lr = 0.00985258\n",
      "I0428 05:45:46.230016  2380 solver.cpp:228] Iteration 300, loss = 0.123719\n",
      "I0428 05:45:46.230089  2380 solver.cpp:244]     Train net output #0: loss = 0.123719 (* 1 = 0.123719 loss)\n",
      "I0428 05:45:46.230101  2380 sgd_solver.cpp:106] Iteration 300, lr = 0.00978075\n",
      "I0428 05:45:52.780756  2380 solver.cpp:228] Iteration 400, loss = 0.0934127\n",
      "I0428 05:45:52.781078  2380 solver.cpp:244]     Train net output #0: loss = 0.0934127 (* 1 = 0.0934127 loss)\n",
      "I0428 05:45:52.781126  2380 sgd_solver.cpp:106] Iteration 400, lr = 0.00971013\n",
      "I0428 05:45:59.773756  2380 solver.cpp:337] Iteration 500, Testing net (#0)\n",
      "I0428 05:46:04.094835  2380 solver.cpp:404]     Test net output #0: accuracy = 0.9722\n",
      "I0428 05:46:04.094893  2380 solver.cpp:404]     Test net output #1: loss = 0.0887219 (* 1 = 0.0887219 loss)\n",
      "I0428 05:46:04.161162  2380 solver.cpp:228] Iteration 500, loss = 0.0989967\n",
      "I0428 05:46:04.161219  2380 solver.cpp:244]     Train net output #0: loss = 0.0989968 (* 1 = 0.0989968 loss)\n",
      "I0428 05:46:04.161232  2380 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069\n",
      "I0428 05:46:10.824635  2380 solver.cpp:228] Iteration 600, loss = 0.109277\n",
      "I0428 05:46:10.824708  2380 solver.cpp:244]     Train net output #0: loss = 0.109277 (* 1 = 0.109277 loss)\n",
      "I0428 05:46:10.824720  2380 sgd_solver.cpp:106] Iteration 600, lr = 0.0095724\n",
      "I0428 05:46:17.514292  2380 solver.cpp:228] Iteration 700, loss = 0.0911279\n",
      "I0428 05:46:17.514358  2380 solver.cpp:244]     Train net output #0: loss = 0.0911281 (* 1 = 0.0911281 loss)\n",
      "I0428 05:46:17.514370  2380 sgd_solver.cpp:106] Iteration 700, lr = 0.00950522\n",
      "I0428 05:46:24.203431  2380 solver.cpp:228] Iteration 800, loss = 0.227425\n",
      "I0428 05:46:24.203564  2380 solver.cpp:244]     Train net output #0: loss = 0.227426 (* 1 = 0.227426 loss)\n",
      "I0428 05:46:24.203577  2380 sgd_solver.cpp:106] Iteration 800, lr = 0.00943913\n",
      "I0428 05:46:30.894045  2380 solver.cpp:228] Iteration 900, loss = 0.156954\n",
      "I0428 05:46:30.894114  2380 solver.cpp:244]     Train net output #0: loss = 0.156954 (* 1 = 0.156954 loss)\n",
      "I0428 05:46:30.894125  2380 sgd_solver.cpp:106] Iteration 900, lr = 0.00937411\n",
      "I0428 05:46:37.527935  2380 solver.cpp:337] Iteration 1000, Testing net (#0)\n",
      "I0428 05:46:41.635179  2380 solver.cpp:404]     Test net output #0: accuracy = 0.9809\n",
      "I0428 05:46:41.635251  2380 solver.cpp:404]     Test net output #1: loss = 0.0549933 (* 1 = 0.0549933 loss)\n",
      "I0428 05:46:41.702129  2380 solver.cpp:228] Iteration 1000, loss = 0.0784216\n",
      "I0428 05:46:41.702181  2380 solver.cpp:244]     Train net output #0: loss = 0.0784218 (* 1 = 0.0784218 loss)\n",
      "I0428 05:46:41.702195  2380 sgd_solver.cpp:106] Iteration 1000, lr = 0.00931012\n",
      "I0428 05:46:48.361709  2380 solver.cpp:228] Iteration 1100, loss = 0.00443439\n",
      "I0428 05:46:48.361781  2380 solver.cpp:244]     Train net output #0: loss = 0.00443451 (* 1 = 0.00443451 loss)\n",
      "I0428 05:46:48.361799  2380 sgd_solver.cpp:106] Iteration 1100, lr = 0.00924715\n",
      "I0428 05:46:55.014277  2380 solver.cpp:228] Iteration 1200, loss = 0.0146376\n",
      "I0428 05:46:55.014401  2380 solver.cpp:244]     Train net output #0: loss = 0.0146377 (* 1 = 0.0146377 loss)\n",
      "I0428 05:46:55.014415  2380 sgd_solver.cpp:106] Iteration 1200, lr = 0.00918515\n",
      "I0428 05:47:01.703491  2380 solver.cpp:228] Iteration 1300, loss = 0.0242595\n",
      "I0428 05:47:01.703547  2380 solver.cpp:244]     Train net output #0: loss = 0.0242596 (* 1 = 0.0242596 loss)\n",
      "I0428 05:47:01.703559  2380 sgd_solver.cpp:106] Iteration 1300, lr = 0.00912412\n",
      "I0428 05:47:08.345376  2380 solver.cpp:228] Iteration 1400, loss = 0.00847893\n",
      "I0428 05:47:08.345432  2380 solver.cpp:244]     Train net output #0: loss = 0.00847905 (* 1 = 0.00847905 loss)\n",
      "I0428 05:47:08.345444  2380 sgd_solver.cpp:106] Iteration 1400, lr = 0.00906403\n",
      "I0428 05:47:14.934092  2380 solver.cpp:337] Iteration 1500, Testing net (#0)\n",
      "I0428 05:47:19.100594  2380 solver.cpp:404]     Test net output #0: accuracy = 0.9858\n",
      "I0428 05:47:19.100661  2380 solver.cpp:404]     Test net output #1: loss = 0.0458119 (* 1 = 0.0458119 loss)\n",
      "I0428 05:47:19.169544  2380 solver.cpp:228] Iteration 1500, loss = 0.0933175\n",
      "I0428 05:47:19.169584  2380 solver.cpp:244]     Train net output #0: loss = 0.0933176 (* 1 = 0.0933176 loss)\n",
      "I0428 05:47:19.169600  2380 sgd_solver.cpp:106] Iteration 1500, lr = 0.00900485\n",
      "I0428 05:47:25.841078  2380 solver.cpp:228] Iteration 1600, loss = 0.112908\n",
      "I0428 05:47:25.841158  2380 solver.cpp:244]     Train net output #0: loss = 0.112909 (* 1 = 0.112909 loss)\n",
      "I0428 05:47:25.841171  2380 sgd_solver.cpp:106] Iteration 1600, lr = 0.00894657\n",
      "I0428 05:47:32.504947  2380 solver.cpp:228] Iteration 1700, loss = 0.0210492\n",
      "I0428 05:47:32.505003  2380 solver.cpp:244]     Train net output #0: loss = 0.0210493 (* 1 = 0.0210493 loss)\n",
      "I0428 05:47:32.505017  2380 sgd_solver.cpp:106] Iteration 1700, lr = 0.00888916\n",
      "I0428 05:47:39.350545  2380 solver.cpp:228] Iteration 1800, loss = 0.015007\n",
      "I0428 05:47:39.350605  2380 solver.cpp:244]     Train net output #0: loss = 0.0150072 (* 1 = 0.0150072 loss)\n",
      "I0428 05:47:39.350617  2380 sgd_solver.cpp:106] Iteration 1800, lr = 0.0088326\n",
      "I0428 05:47:46.149593  2380 solver.cpp:228] Iteration 1900, loss = 0.109566\n",
      "I0428 05:47:46.149659  2380 solver.cpp:244]     Train net output #0: loss = 0.109566 (* 1 = 0.109566 loss)\n",
      "I0428 05:47:46.149670  2380 sgd_solver.cpp:106] Iteration 1900, lr = 0.00877687\n",
      "I0428 05:47:52.786942  2380 solver.cpp:337] Iteration 2000, Testing net (#0)\n",
      "I0428 05:47:56.885551  2380 solver.cpp:404]     Test net output #0: accuracy = 0.9864\n",
      "I0428 05:47:56.885653  2380 solver.cpp:404]     Test net output #1: loss = 0.0409908 (* 1 = 0.0409908 loss)\n",
      "I0428 05:47:56.951146  2380 solver.cpp:228] Iteration 2000, loss = 0.0204729\n",
      "I0428 05:47:56.951192  2380 solver.cpp:244]     Train net output #0: loss = 0.020473 (* 1 = 0.020473 loss)\n",
      "I0428 05:47:56.951206  2380 sgd_solver.cpp:106] Iteration 2000, lr = 0.00872196\n",
      "I0428 05:48:03.635849  2380 solver.cpp:228] Iteration 2100, loss = 0.0145452\n",
      "I0428 05:48:03.635913  2380 solver.cpp:244]     Train net output #0: loss = 0.0145453 (* 1 = 0.0145453 loss)\n",
      "I0428 05:48:03.635926  2380 sgd_solver.cpp:106] Iteration 2100, lr = 0.00866784\n",
      "I0428 05:48:10.364491  2380 solver.cpp:228] Iteration 2200, loss = 0.0167382\n",
      "I0428 05:48:10.364536  2380 solver.cpp:244]     Train net output #0: loss = 0.0167384 (* 1 = 0.0167384 loss)\n",
      "I0428 05:48:10.364547  2380 sgd_solver.cpp:106] Iteration 2200, lr = 0.0086145\n",
      "I0428 05:48:17.065009  2380 solver.cpp:228] Iteration 2300, loss = 0.103004\n",
      "I0428 05:48:17.065065  2380 solver.cpp:244]     Train net output #0: loss = 0.103005 (* 1 = 0.103005 loss)\n",
      "I0428 05:48:17.065078  2380 sgd_solver.cpp:106] Iteration 2300, lr = 0.00856192\n",
      "I0428 05:48:23.759938  2380 solver.cpp:228] Iteration 2400, loss = 0.00998721\n",
      "I0428 05:48:23.759991  2380 solver.cpp:244]     Train net output #0: loss = 0.00998738 (* 1 = 0.00998738 loss)\n",
      "I0428 05:48:23.760004  2380 sgd_solver.cpp:106] Iteration 2400, lr = 0.00851008\n",
      "I0428 05:48:30.391449  2380 solver.cpp:337] Iteration 2500, Testing net (#0)\n",
      "I0428 05:48:34.473160  2380 solver.cpp:404]     Test net output #0: accuracy = 0.982\n",
      "I0428 05:48:34.473223  2380 solver.cpp:404]     Test net output #1: loss = 0.0538164 (* 1 = 0.0538164 loss)\n",
      "I0428 05:48:34.540148  2380 solver.cpp:228] Iteration 2500, loss = 0.0424318\n",
      "I0428 05:48:34.540195  2380 solver.cpp:244]     Train net output #0: loss = 0.0424319 (* 1 = 0.0424319 loss)\n",
      "I0428 05:48:34.540210  2380 sgd_solver.cpp:106] Iteration 2500, lr = 0.00845897\n",
      "I0428 05:48:41.258584  2380 solver.cpp:228] Iteration 2600, loss = 0.0593748\n",
      "I0428 05:48:41.258647  2380 solver.cpp:244]     Train net output #0: loss = 0.059375 (* 1 = 0.059375 loss)\n",
      "I0428 05:48:41.258659  2380 sgd_solver.cpp:106] Iteration 2600, lr = 0.00840857\n",
      "I0428 05:48:47.900156  2380 solver.cpp:228] Iteration 2700, loss = 0.0415773\n",
      "I0428 05:48:47.900213  2380 solver.cpp:244]     Train net output #0: loss = 0.0415775 (* 1 = 0.0415775 loss)\n",
      "I0428 05:48:47.900224  2380 sgd_solver.cpp:106] Iteration 2700, lr = 0.00835886\n",
      "I0428 05:48:54.540755  2380 solver.cpp:228] Iteration 2800, loss = 0.00470359\n",
      "I0428 05:48:54.540810  2380 solver.cpp:244]     Train net output #0: loss = 0.00470379 (* 1 = 0.00470379 loss)\n",
      "I0428 05:48:54.540822  2380 sgd_solver.cpp:106] Iteration 2800, lr = 0.00830984\n",
      "I0428 05:49:01.194983  2380 solver.cpp:228] Iteration 2900, loss = 0.0309981\n",
      "I0428 05:49:01.195163  2380 solver.cpp:244]     Train net output #0: loss = 0.0309983 (* 1 = 0.0309983 loss)\n",
      "I0428 05:49:01.195176  2380 sgd_solver.cpp:106] Iteration 2900, lr = 0.00826148\n",
      "I0428 05:49:07.808270  2380 solver.cpp:337] Iteration 3000, Testing net (#0)\n",
      "I0428 05:49:11.910651  2380 solver.cpp:404]     Test net output #0: accuracy = 0.9867\n",
      "I0428 05:49:11.910709  2380 solver.cpp:404]     Test net output #1: loss = 0.0396118 (* 1 = 0.0396118 loss)\n",
      "I0428 05:49:11.978569  2380 solver.cpp:228] Iteration 3000, loss = 0.0265039\n",
      "I0428 05:49:11.978605  2380 solver.cpp:244]     Train net output #0: loss = 0.0265041 (* 1 = 0.0265041 loss)\n",
      "I0428 05:49:11.978618  2380 sgd_solver.cpp:106] Iteration 3000, lr = 0.00821377\n",
      "I0428 05:49:18.627861  2380 solver.cpp:228] Iteration 3100, loss = 0.0236156\n",
      "I0428 05:49:18.627918  2380 solver.cpp:244]     Train net output #0: loss = 0.0236158 (* 1 = 0.0236158 loss)\n",
      "I0428 05:49:18.627929  2380 sgd_solver.cpp:106] Iteration 3100, lr = 0.0081667\n",
      "I0428 05:49:25.279521  2380 solver.cpp:228] Iteration 3200, loss = 0.00913436\n",
      "I0428 05:49:25.279578  2380 solver.cpp:244]     Train net output #0: loss = 0.00913452 (* 1 = 0.00913452 loss)\n",
      "I0428 05:49:25.279590  2380 sgd_solver.cpp:106] Iteration 3200, lr = 0.00812025\n",
      "I0428 05:49:31.916800  2380 solver.cpp:228] Iteration 3300, loss = 0.0180434\n",
      "I0428 05:49:31.916918  2380 solver.cpp:244]     Train net output #0: loss = 0.0180435 (* 1 = 0.0180435 loss)\n",
      "I0428 05:49:31.916930  2380 sgd_solver.cpp:106] Iteration 3300, lr = 0.00807442\n",
      "I0428 05:49:38.600836  2380 solver.cpp:228] Iteration 3400, loss = 0.00805734\n",
      "I0428 05:49:38.600898  2380 solver.cpp:244]     Train net output #0: loss = 0.00805752 (* 1 = 0.00805752 loss)\n",
      "I0428 05:49:38.600910  2380 sgd_solver.cpp:106] Iteration 3400, lr = 0.00802918\n",
      "I0428 05:49:45.241524  2380 solver.cpp:337] Iteration 3500, Testing net (#0)\n",
      "I0428 05:49:49.317360  2380 solver.cpp:404]     Test net output #0: accuracy = 0.9843\n",
      "I0428 05:49:49.317405  2380 solver.cpp:404]     Test net output #1: loss = 0.0453514 (* 1 = 0.0453514 loss)\n",
      "I0428 05:49:49.383033  2380 solver.cpp:228] Iteration 3500, loss = 0.00594274\n",
      "I0428 05:49:49.383081  2380 solver.cpp:244]     Train net output #0: loss = 0.00594294 (* 1 = 0.00594294 loss)\n",
      "I0428 05:49:49.383096  2380 sgd_solver.cpp:106] Iteration 3500, lr = 0.00798454\n",
      "I0428 05:49:56.048795  2380 solver.cpp:228] Iteration 3600, loss = 0.0331554\n",
      "I0428 05:49:56.048845  2380 solver.cpp:244]     Train net output #0: loss = 0.0331556 (* 1 = 0.0331556 loss)\n",
      "I0428 05:49:56.048856  2380 sgd_solver.cpp:106] Iteration 3600, lr = 0.00794046\n",
      "I0428 05:50:02.743015  2380 solver.cpp:228] Iteration 3700, loss = 0.0181992\n",
      "I0428 05:50:02.743122  2380 solver.cpp:244]     Train net output #0: loss = 0.0181994 (* 1 = 0.0181994 loss)\n",
      "I0428 05:50:02.743135  2380 sgd_solver.cpp:106] Iteration 3700, lr = 0.00789695\n",
      "I0428 05:50:09.470780  2380 solver.cpp:228] Iteration 3800, loss = 0.00824103\n",
      "I0428 05:50:09.470839  2380 solver.cpp:244]     Train net output #0: loss = 0.00824126 (* 1 = 0.00824126 loss)\n",
      "I0428 05:50:09.470850  2380 sgd_solver.cpp:106] Iteration 3800, lr = 0.007854\n",
      "I0428 05:50:16.162946  2380 solver.cpp:228] Iteration 3900, loss = 0.0223363\n",
      "I0428 05:50:16.162988  2380 solver.cpp:244]     Train net output #0: loss = 0.0223365 (* 1 = 0.0223365 loss)\n",
      "I0428 05:50:16.163000  2380 sgd_solver.cpp:106] Iteration 3900, lr = 0.00781158\n",
      "I0428 05:50:22.785462  2380 solver.cpp:337] Iteration 4000, Testing net (#0)\n",
      "I0428 05:50:26.855672  2380 solver.cpp:404]     Test net output #0: accuracy = 0.9902\n",
      "I0428 05:50:26.855728  2380 solver.cpp:404]     Test net output #1: loss = 0.029627 (* 1 = 0.029627 loss)\n",
      "I0428 05:50:26.921597  2380 solver.cpp:228] Iteration 4000, loss = 0.0233827\n",
      "I0428 05:50:26.921644  2380 solver.cpp:244]     Train net output #0: loss = 0.0233829 (* 1 = 0.0233829 loss)\n",
      "I0428 05:50:26.921658  2380 sgd_solver.cpp:106] Iteration 4000, lr = 0.0077697\n",
      "I0428 05:50:33.567067  2380 solver.cpp:228] Iteration 4100, loss = 0.0233313\n",
      "I0428 05:50:33.567240  2380 solver.cpp:244]     Train net output #0: loss = 0.0233315 (* 1 = 0.0233315 loss)\n",
      "I0428 05:50:33.567255  2380 sgd_solver.cpp:106] Iteration 4100, lr = 0.00772833\n",
      "I0428 05:50:40.252493  2380 solver.cpp:228] Iteration 4200, loss = 0.0090087\n",
      "I0428 05:50:40.252573  2380 solver.cpp:244]     Train net output #0: loss = 0.00900889 (* 1 = 0.00900889 loss)\n",
      "I0428 05:50:40.252584  2380 sgd_solver.cpp:106] Iteration 4200, lr = 0.00768748\n",
      "I0428 05:50:46.947230  2380 solver.cpp:228] Iteration 4300, loss = 0.0378047\n",
      "I0428 05:50:46.947285  2380 solver.cpp:244]     Train net output #0: loss = 0.0378049 (* 1 = 0.0378049 loss)\n",
      "I0428 05:50:46.947298  2380 sgd_solver.cpp:106] Iteration 4300, lr = 0.00764712\n",
      "I0428 05:50:53.629387  2380 solver.cpp:228] Iteration 4400, loss = 0.0180189\n",
      "I0428 05:50:53.629441  2380 solver.cpp:244]     Train net output #0: loss = 0.018019 (* 1 = 0.018019 loss)\n",
      "I0428 05:50:53.629453  2380 sgd_solver.cpp:106] Iteration 4400, lr = 0.00760726\n",
      "I0428 05:51:00.261061  2380 solver.cpp:337] Iteration 4500, Testing net (#0)\n",
      "I0428 05:51:04.330642  2380 solver.cpp:404]     Test net output #0: accuracy = 0.9885\n",
      "I0428 05:51:04.330754  2380 solver.cpp:404]     Test net output #1: loss = 0.035872 (* 1 = 0.035872 loss)\n",
      "I0428 05:51:04.396414  2380 solver.cpp:228] Iteration 4500, loss = 0.00694972\n",
      "I0428 05:51:04.396463  2380 solver.cpp:244]     Train net output #0: loss = 0.00694991 (* 1 = 0.00694991 loss)\n",
      "I0428 05:51:04.396477  2380 sgd_solver.cpp:106] Iteration 4500, lr = 0.00756788\n",
      "I0428 05:51:11.087265  2380 solver.cpp:228] Iteration 4600, loss = 0.006128\n",
      "I0428 05:51:11.087319  2380 solver.cpp:244]     Train net output #0: loss = 0.00612818 (* 1 = 0.00612818 loss)\n",
      "I0428 05:51:11.087332  2380 sgd_solver.cpp:106] Iteration 4600, lr = 0.00752897\n",
      "I0428 05:51:17.724756  2380 solver.cpp:228] Iteration 4700, loss = 0.00255082\n",
      "I0428 05:51:17.724813  2380 solver.cpp:244]     Train net output #0: loss = 0.00255099 (* 1 = 0.00255099 loss)\n",
      "I0428 05:51:17.724825  2380 sgd_solver.cpp:106] Iteration 4700, lr = 0.00749052\n",
      "I0428 05:51:24.376466  2380 solver.cpp:228] Iteration 4800, loss = 0.0155366\n",
      "I0428 05:51:24.376523  2380 solver.cpp:244]     Train net output #0: loss = 0.0155367 (* 1 = 0.0155367 loss)\n",
      "I0428 05:51:24.376534  2380 sgd_solver.cpp:106] Iteration 4800, lr = 0.00745253\n",
      "I0428 05:51:31.022281  2380 solver.cpp:228] Iteration 4900, loss = 0.00511164\n",
      "I0428 05:51:31.022341  2380 solver.cpp:244]     Train net output #0: loss = 0.00511182 (* 1 = 0.00511182 loss)\n",
      "I0428 05:51:31.022351  2380 sgd_solver.cpp:106] Iteration 4900, lr = 0.00741498\n",
      "I0428 05:51:37.632750  2380 solver.cpp:454] Snapshotting to binary proto file ./lenet/lenet_iter_5000.caffemodel\n",
      "I0428 05:51:37.637846  2380 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./lenet/lenet_iter_5000.solverstate\n",
      "I0428 05:51:37.640859  2380 solver.cpp:337] Iteration 5000, Testing net (#0)\n",
      "I0428 05:51:41.716112  2380 solver.cpp:404]     Test net output #0: accuracy = 0.9901\n",
      "I0428 05:51:41.716166  2380 solver.cpp:404]     Test net output #1: loss = 0.0314304 (* 1 = 0.0314304 loss)\n",
      "I0428 05:51:41.782061  2380 solver.cpp:228] Iteration 5000, loss = 0.0400851\n",
      "I0428 05:51:41.782109  2380 solver.cpp:244]     Train net output #0: loss = 0.0400853 (* 1 = 0.0400853 loss)\n",
      "I0428 05:51:41.782124  2380 sgd_solver.cpp:106] Iteration 5000, lr = 0.00737788\n",
      "I0428 05:51:48.418108  2380 solver.cpp:228] Iteration 5100, loss = 0.02197\n",
      "I0428 05:51:48.418167  2380 solver.cpp:244]     Train net output #0: loss = 0.0219702 (* 1 = 0.0219702 loss)\n",
      "I0428 05:51:48.418179  2380 sgd_solver.cpp:106] Iteration 5100, lr = 0.0073412\n",
      "I0428 05:51:55.059661  2380 solver.cpp:228] Iteration 5200, loss = 0.00559956\n",
      "I0428 05:51:55.059717  2380 solver.cpp:244]     Train net output #0: loss = 0.00559976 (* 1 = 0.00559976 loss)\n",
      "I0428 05:51:55.059730  2380 sgd_solver.cpp:106] Iteration 5200, lr = 0.00730495\n",
      "I0428 05:52:01.686936  2380 solver.cpp:228] Iteration 5300, loss = 0.00176502\n",
      "I0428 05:52:01.686985  2380 solver.cpp:244]     Train net output #0: loss = 0.00176523 (* 1 = 0.00176523 loss)\n",
      "I0428 05:52:01.686996  2380 sgd_solver.cpp:106] Iteration 5300, lr = 0.00726911\n",
      "I0428 05:52:08.280087  2380 solver.cpp:228] Iteration 5400, loss = 0.0059485\n",
      "I0428 05:52:08.280192  2380 solver.cpp:244]     Train net output #0: loss = 0.00594872 (* 1 = 0.00594872 loss)\n",
      "I0428 05:52:08.280205  2380 sgd_solver.cpp:106] Iteration 5400, lr = 0.00723368\n",
      "I0428 05:52:14.810827  2380 solver.cpp:337] Iteration 5500, Testing net (#0)\n",
      "I0428 05:52:18.846484  2380 solver.cpp:404]     Test net output #0: accuracy = 0.9894\n",
      "I0428 05:52:18.846525  2380 solver.cpp:404]     Test net output #1: loss = 0.0336118 (* 1 = 0.0336118 loss)\n",
      "I0428 05:52:18.911589  2380 solver.cpp:228] Iteration 5500, loss = 0.0101893\n",
      "I0428 05:52:18.911623  2380 solver.cpp:244]     Train net output #0: loss = 0.0101896 (* 1 = 0.0101896 loss)\n",
      "I0428 05:52:18.911635  2380 sgd_solver.cpp:106] Iteration 5500, lr = 0.00719865\n",
      "I0428 05:52:25.501641  2380 solver.cpp:228] Iteration 5600, loss = 0.000534829\n",
      "I0428 05:52:25.501683  2380 solver.cpp:244]     Train net output #0: loss = 0.000535049 (* 1 = 0.000535049 loss)\n",
      "I0428 05:52:25.501695  2380 sgd_solver.cpp:106] Iteration 5600, lr = 0.00716402\n",
      "I0428 05:52:32.094722  2380 solver.cpp:228] Iteration 5700, loss = 0.00366381\n",
      "I0428 05:52:32.094764  2380 solver.cpp:244]     Train net output #0: loss = 0.00366403 (* 1 = 0.00366403 loss)\n",
      "I0428 05:52:32.094775  2380 sgd_solver.cpp:106] Iteration 5700, lr = 0.00712977\n",
      "I0428 05:52:38.695744  2380 solver.cpp:228] Iteration 5800, loss = 0.0242665\n",
      "I0428 05:52:38.695832  2380 solver.cpp:244]     Train net output #0: loss = 0.0242667 (* 1 = 0.0242667 loss)\n",
      "I0428 05:52:38.695844  2380 sgd_solver.cpp:106] Iteration 5800, lr = 0.0070959\n",
      "I0428 05:52:45.409124  2380 solver.cpp:228] Iteration 5900, loss = 0.00723433\n",
      "I0428 05:52:45.409188  2380 solver.cpp:244]     Train net output #0: loss = 0.00723455 (* 1 = 0.00723455 loss)\n",
      "I0428 05:52:45.409199  2380 sgd_solver.cpp:106] Iteration 5900, lr = 0.0070624\n",
      "I0428 05:52:51.981050  2380 solver.cpp:337] Iteration 6000, Testing net (#0)\n",
      "I0428 05:52:56.016619  2380 solver.cpp:404]     Test net output #0: accuracy = 0.9906\n",
      "I0428 05:52:56.016661  2380 solver.cpp:404]     Test net output #1: loss = 0.0283629 (* 1 = 0.0283629 loss)\n",
      "I0428 05:52:56.081657  2380 solver.cpp:228] Iteration 6000, loss = 0.00454953\n",
      "I0428 05:52:56.081693  2380 solver.cpp:244]     Train net output #0: loss = 0.00454975 (* 1 = 0.00454975 loss)\n",
      "I0428 05:52:56.081707  2380 sgd_solver.cpp:106] Iteration 6000, lr = 0.00702927\n",
      "I0428 05:53:02.672930  2380 solver.cpp:228] Iteration 6100, loss = 0.00206689\n",
      "I0428 05:53:02.672973  2380 solver.cpp:244]     Train net output #0: loss = 0.0020671 (* 1 = 0.0020671 loss)\n",
      "I0428 05:53:02.672986  2380 sgd_solver.cpp:106] Iteration 6100, lr = 0.0069965\n",
      "I0428 05:53:09.264138  2380 solver.cpp:228] Iteration 6200, loss = 0.0068993\n",
      "I0428 05:53:09.264217  2380 solver.cpp:244]     Train net output #0: loss = 0.0068995 (* 1 = 0.0068995 loss)\n",
      "I0428 05:53:09.264230  2380 sgd_solver.cpp:106] Iteration 6200, lr = 0.00696408\n",
      "I0428 05:53:15.880416  2380 solver.cpp:228] Iteration 6300, loss = 0.00833429\n",
      "I0428 05:53:15.880463  2380 solver.cpp:244]     Train net output #0: loss = 0.00833449 (* 1 = 0.00833449 loss)\n",
      "I0428 05:53:15.880475  2380 sgd_solver.cpp:106] Iteration 6300, lr = 0.00693201\n",
      "I0428 05:53:22.469907  2380 solver.cpp:228] Iteration 6400, loss = 0.00554587\n",
      "I0428 05:53:22.469950  2380 solver.cpp:244]     Train net output #0: loss = 0.00554607 (* 1 = 0.00554607 loss)\n",
      "I0428 05:53:22.469961  2380 sgd_solver.cpp:106] Iteration 6400, lr = 0.00690029\n",
      "I0428 05:53:28.997489  2380 solver.cpp:337] Iteration 6500, Testing net (#0)\n",
      "I0428 05:53:33.034921  2380 solver.cpp:404]     Test net output #0: accuracy = 0.9899\n",
      "I0428 05:53:33.034961  2380 solver.cpp:404]     Test net output #1: loss = 0.0307036 (* 1 = 0.0307036 loss)\n",
      "I0428 05:53:33.100303  2380 solver.cpp:228] Iteration 6500, loss = 0.00725662\n",
      "I0428 05:53:33.100340  2380 solver.cpp:244]     Train net output #0: loss = 0.00725682 (* 1 = 0.00725682 loss)\n",
      "I0428 05:53:33.100353  2380 sgd_solver.cpp:106] Iteration 6500, lr = 0.0068689\n",
      "I0428 05:53:39.693958  2380 solver.cpp:228] Iteration 6600, loss = 0.0408244\n",
      "I0428 05:53:39.694212  2380 solver.cpp:244]     Train net output #0: loss = 0.0408246 (* 1 = 0.0408246 loss)\n",
      "I0428 05:53:39.694262  2380 sgd_solver.cpp:106] Iteration 6600, lr = 0.00683784\n",
      "I0428 05:53:46.702656  2380 solver.cpp:228] Iteration 6700, loss = 0.00663462\n",
      "I0428 05:53:46.702710  2380 solver.cpp:244]     Train net output #0: loss = 0.00663483 (* 1 = 0.00663483 loss)\n",
      "I0428 05:53:46.702723  2380 sgd_solver.cpp:106] Iteration 6700, lr = 0.00680711\n",
      "I0428 05:53:53.728584  2380 solver.cpp:228] Iteration 6800, loss = 0.00293957\n",
      "I0428 05:53:53.728646  2380 solver.cpp:244]     Train net output #0: loss = 0.00293978 (* 1 = 0.00293978 loss)\n",
      "I0428 05:53:53.728657  2380 sgd_solver.cpp:106] Iteration 6800, lr = 0.0067767\n",
      "I0428 05:54:00.412960  2380 solver.cpp:228] Iteration 6900, loss = 0.00597417\n",
      "I0428 05:54:00.413013  2380 solver.cpp:244]     Train net output #0: loss = 0.00597438 (* 1 = 0.00597438 loss)\n",
      "I0428 05:54:00.413025  2380 sgd_solver.cpp:106] Iteration 6900, lr = 0.0067466\n",
      "I0428 05:54:07.049536  2380 solver.cpp:337] Iteration 7000, Testing net (#0)\n",
      "I0428 05:54:11.120652  2380 solver.cpp:404]     Test net output #0: accuracy = 0.9895\n",
      "I0428 05:54:11.120921  2380 solver.cpp:404]     Test net output #1: loss = 0.0307099 (* 1 = 0.0307099 loss)\n",
      "I0428 05:54:11.262655  2380 solver.cpp:228] Iteration 7000, loss = 0.00395964\n",
      "I0428 05:54:11.262708  2380 solver.cpp:244]     Train net output #0: loss = 0.00395985 (* 1 = 0.00395985 loss)\n",
      "I0428 05:54:11.262728  2380 sgd_solver.cpp:106] Iteration 7000, lr = 0.00671681\n",
      "I0428 05:54:18.171576  2380 solver.cpp:228] Iteration 7100, loss = 0.0171716\n",
      "I0428 05:54:18.171628  2380 solver.cpp:244]     Train net output #0: loss = 0.0171718 (* 1 = 0.0171718 loss)\n",
      "I0428 05:54:18.171639  2380 sgd_solver.cpp:106] Iteration 7100, lr = 0.00668733\n",
      "I0428 05:54:24.990274  2380 solver.cpp:228] Iteration 7200, loss = 0.00447711\n",
      "I0428 05:54:24.990329  2380 solver.cpp:244]     Train net output #0: loss = 0.00447732 (* 1 = 0.00447732 loss)\n",
      "I0428 05:54:24.990341  2380 sgd_solver.cpp:106] Iteration 7200, lr = 0.00665815\n",
      "I0428 05:54:31.801363  2380 solver.cpp:228] Iteration 7300, loss = 0.0242194\n",
      "I0428 05:54:31.801415  2380 solver.cpp:244]     Train net output #0: loss = 0.0242196 (* 1 = 0.0242196 loss)\n",
      "I0428 05:54:31.801426  2380 sgd_solver.cpp:106] Iteration 7300, lr = 0.00662927\n",
      "I0428 05:54:38.700393  2380 solver.cpp:228] Iteration 7400, loss = 0.00332531\n",
      "I0428 05:54:38.700456  2380 solver.cpp:244]     Train net output #0: loss = 0.00332552 (* 1 = 0.00332552 loss)\n",
      "I0428 05:54:38.700481  2380 sgd_solver.cpp:106] Iteration 7400, lr = 0.00660067\n",
      "I0428 05:54:45.516300  2380 solver.cpp:337] Iteration 7500, Testing net (#0)\n",
      "I0428 05:54:49.787458  2380 solver.cpp:404]     Test net output #0: accuracy = 0.99\n",
      "I0428 05:54:49.787503  2380 solver.cpp:404]     Test net output #1: loss = 0.0319101 (* 1 = 0.0319101 loss)\n",
      "I0428 05:54:49.853266  2380 solver.cpp:228] Iteration 7500, loss = 0.00156556\n",
      "I0428 05:54:49.853302  2380 solver.cpp:244]     Train net output #0: loss = 0.00156577 (* 1 = 0.00156577 loss)\n",
      "I0428 05:54:49.853317  2380 sgd_solver.cpp:106] Iteration 7500, lr = 0.00657236\n",
      "I0428 05:54:56.441244  2380 solver.cpp:228] Iteration 7600, loss = 0.00677558\n",
      "I0428 05:54:56.441287  2380 solver.cpp:244]     Train net output #0: loss = 0.00677579 (* 1 = 0.00677579 loss)\n",
      "I0428 05:54:56.441298  2380 sgd_solver.cpp:106] Iteration 7600, lr = 0.00654433\n",
      "I0428 05:55:03.029497  2380 solver.cpp:228] Iteration 7700, loss = 0.0284596\n",
      "I0428 05:55:03.029551  2380 solver.cpp:244]     Train net output #0: loss = 0.0284599 (* 1 = 0.0284599 loss)\n",
      "I0428 05:55:03.029569  2380 sgd_solver.cpp:106] Iteration 7700, lr = 0.00651658\n",
      "I0428 05:55:09.637645  2380 solver.cpp:228] Iteration 7800, loss = 0.00379766\n",
      "I0428 05:55:09.637691  2380 solver.cpp:244]     Train net output #0: loss = 0.00379787 (* 1 = 0.00379787 loss)\n",
      "I0428 05:55:09.637702  2380 sgd_solver.cpp:106] Iteration 7800, lr = 0.00648911\n",
      "I0428 05:55:16.267875  2380 solver.cpp:228] Iteration 7900, loss = 0.00646883\n",
      "I0428 05:55:16.268096  2380 solver.cpp:244]     Train net output #0: loss = 0.00646904 (* 1 = 0.00646904 loss)\n",
      "I0428 05:55:16.268147  2380 sgd_solver.cpp:106] Iteration 7900, lr = 0.0064619\n",
      "I0428 05:55:23.331354  2380 solver.cpp:337] Iteration 8000, Testing net (#0)\n",
      "I0428 05:55:27.572556  2380 solver.cpp:404]     Test net output #0: accuracy = 0.9904\n",
      "I0428 05:55:27.572589  2380 solver.cpp:404]     Test net output #1: loss = 0.0288581 (* 1 = 0.0288581 loss)\n",
      "I0428 05:55:27.638087  2380 solver.cpp:228] Iteration 8000, loss = 0.00459704\n",
      "I0428 05:55:27.638128  2380 solver.cpp:244]     Train net output #0: loss = 0.00459725 (* 1 = 0.00459725 loss)\n",
      "I0428 05:55:27.638142  2380 sgd_solver.cpp:106] Iteration 8000, lr = 0.00643496\n",
      "I0428 05:55:34.228054  2380 solver.cpp:228] Iteration 8100, loss = 0.0100578\n",
      "I0428 05:55:34.228092  2380 solver.cpp:244]     Train net output #0: loss = 0.010058 (* 1 = 0.010058 loss)\n",
      "I0428 05:55:34.228104  2380 sgd_solver.cpp:106] Iteration 8100, lr = 0.00640827\n",
      "I0428 05:55:40.814352  2380 solver.cpp:228] Iteration 8200, loss = 0.0103289\n",
      "I0428 05:55:40.814393  2380 solver.cpp:244]     Train net output #0: loss = 0.0103291 (* 1 = 0.0103291 loss)\n",
      "I0428 05:55:40.814404  2380 sgd_solver.cpp:106] Iteration 8200, lr = 0.00638185\n",
      "I0428 05:55:47.404109  2380 solver.cpp:228] Iteration 8300, loss = 0.0372101\n",
      "I0428 05:55:47.404333  2380 solver.cpp:244]     Train net output #0: loss = 0.0372103 (* 1 = 0.0372103 loss)\n",
      "I0428 05:55:47.404383  2380 sgd_solver.cpp:106] Iteration 8300, lr = 0.00635567\n",
      "I0428 05:55:54.451987  2380 solver.cpp:228] Iteration 8400, loss = 0.00477631\n",
      "I0428 05:55:54.452051  2380 solver.cpp:244]     Train net output #0: loss = 0.00477652 (* 1 = 0.00477652 loss)\n",
      "I0428 05:55:54.452064  2380 sgd_solver.cpp:106] Iteration 8400, lr = 0.00632975\n",
      "I0428 05:56:01.289558  2380 solver.cpp:337] Iteration 8500, Testing net (#0)\n",
      "I0428 05:56:05.541523  2380 solver.cpp:404]     Test net output #0: accuracy = 0.9904\n",
      "I0428 05:56:05.541574  2380 solver.cpp:404]     Test net output #1: loss = 0.0291148 (* 1 = 0.0291148 loss)\n",
      "I0428 05:56:05.610817  2380 solver.cpp:228] Iteration 8500, loss = 0.00568629\n",
      "I0428 05:56:05.610864  2380 solver.cpp:244]     Train net output #0: loss = 0.00568651 (* 1 = 0.00568651 loss)\n",
      "I0428 05:56:05.610877  2380 sgd_solver.cpp:106] Iteration 8500, lr = 0.00630407\n",
      "I0428 05:56:12.507684  2380 solver.cpp:228] Iteration 8600, loss = 0.000717126\n",
      "I0428 05:56:12.507750  2380 solver.cpp:244]     Train net output #0: loss = 0.000717347 (* 1 = 0.000717347 loss)\n",
      "I0428 05:56:12.507761  2380 sgd_solver.cpp:106] Iteration 8600, lr = 0.00627864\n",
      "I0428 05:56:19.397382  2380 solver.cpp:228] Iteration 8700, loss = 0.00363903\n",
      "I0428 05:56:19.397516  2380 solver.cpp:244]     Train net output #0: loss = 0.00363925 (* 1 = 0.00363925 loss)\n",
      "I0428 05:56:19.397528  2380 sgd_solver.cpp:106] Iteration 8700, lr = 0.00625344\n",
      "I0428 05:56:26.377511  2380 solver.cpp:228] Iteration 8800, loss = 0.00152786\n",
      "I0428 05:56:26.377567  2380 solver.cpp:244]     Train net output #0: loss = 0.00152808 (* 1 = 0.00152808 loss)\n",
      "I0428 05:56:26.377578  2380 sgd_solver.cpp:106] Iteration 8800, lr = 0.00622847\n",
      "I0428 05:56:33.012220  2380 solver.cpp:228] Iteration 8900, loss = 0.000990079\n",
      "I0428 05:56:33.012261  2380 solver.cpp:244]     Train net output #0: loss = 0.000990301 (* 1 = 0.000990301 loss)\n",
      "I0428 05:56:33.012274  2380 sgd_solver.cpp:106] Iteration 8900, lr = 0.00620374\n",
      "I0428 05:56:39.580847  2380 solver.cpp:337] Iteration 9000, Testing net (#0)\n",
      "I0428 05:56:43.657820  2380 solver.cpp:404]     Test net output #0: accuracy = 0.9917\n",
      "I0428 05:56:43.657883  2380 solver.cpp:404]     Test net output #1: loss = 0.0275251 (* 1 = 0.0275251 loss)\n",
      "I0428 05:56:43.724750  2380 solver.cpp:228] Iteration 9000, loss = 0.0104375\n",
      "I0428 05:56:43.724786  2380 solver.cpp:244]     Train net output #0: loss = 0.0104377 (* 1 = 0.0104377 loss)\n",
      "I0428 05:56:43.724799  2380 sgd_solver.cpp:106] Iteration 9000, lr = 0.00617924\n",
      "I0428 05:56:50.318616  2380 solver.cpp:228] Iteration 9100, loss = 0.0071233\n",
      "I0428 05:56:50.318737  2380 solver.cpp:244]     Train net output #0: loss = 0.00712352 (* 1 = 0.00712352 loss)\n",
      "I0428 05:56:50.318749  2380 sgd_solver.cpp:106] Iteration 9100, lr = 0.00615496\n",
      "I0428 05:56:56.909345  2380 solver.cpp:228] Iteration 9200, loss = 0.00194002\n",
      "I0428 05:56:56.909387  2380 solver.cpp:244]     Train net output #0: loss = 0.00194024 (* 1 = 0.00194024 loss)\n",
      "I0428 05:56:56.909399  2380 sgd_solver.cpp:106] Iteration 9200, lr = 0.0061309\n",
      "I0428 05:57:03.499279  2380 solver.cpp:228] Iteration 9300, loss = 0.00723963\n",
      "I0428 05:57:03.499320  2380 solver.cpp:244]     Train net output #0: loss = 0.00723987 (* 1 = 0.00723987 loss)\n",
      "I0428 05:57:03.499332  2380 sgd_solver.cpp:106] Iteration 9300, lr = 0.00610706\n",
      "I0428 05:57:10.088009  2380 solver.cpp:228] Iteration 9400, loss = 0.0469242\n",
      "I0428 05:57:10.088048  2380 solver.cpp:244]     Train net output #0: loss = 0.0469245 (* 1 = 0.0469245 loss)\n",
      "I0428 05:57:10.088059  2380 sgd_solver.cpp:106] Iteration 9400, lr = 0.00608343\n",
      "I0428 05:57:16.613143  2380 solver.cpp:337] Iteration 9500, Testing net (#0)\n",
      "I0428 05:57:20.644996  2380 solver.cpp:404]     Test net output #0: accuracy = 0.988\n",
      "I0428 05:57:20.645090  2380 solver.cpp:404]     Test net output #1: loss = 0.036109 (* 1 = 0.036109 loss)\n",
      "I0428 05:57:20.710041  2380 solver.cpp:228] Iteration 9500, loss = 0.00279456\n",
      "I0428 05:57:20.710075  2380 solver.cpp:244]     Train net output #0: loss = 0.0027948 (* 1 = 0.0027948 loss)\n",
      "I0428 05:57:20.710088  2380 sgd_solver.cpp:106] Iteration 9500, lr = 0.00606002\n",
      "I0428 05:57:27.387028  2380 solver.cpp:228] Iteration 9600, loss = 0.00177477\n",
      "I0428 05:57:27.387078  2380 solver.cpp:244]     Train net output #0: loss = 0.00177502 (* 1 = 0.00177502 loss)\n",
      "I0428 05:57:27.387089  2380 sgd_solver.cpp:106] Iteration 9600, lr = 0.00603682\n",
      "I0428 05:57:33.978732  2380 solver.cpp:228] Iteration 9700, loss = 0.0038613\n",
      "I0428 05:57:33.978772  2380 solver.cpp:244]     Train net output #0: loss = 0.00386155 (* 1 = 0.00386155 loss)\n",
      "I0428 05:57:33.978785  2380 sgd_solver.cpp:106] Iteration 9700, lr = 0.00601382\n",
      "I0428 05:57:40.577646  2380 solver.cpp:228] Iteration 9800, loss = 0.0147714\n",
      "I0428 05:57:40.577690  2380 solver.cpp:244]     Train net output #0: loss = 0.0147716 (* 1 = 0.0147716 loss)\n",
      "I0428 05:57:40.577702  2380 sgd_solver.cpp:106] Iteration 9800, lr = 0.00599102\n",
      "I0428 05:57:47.208891  2380 solver.cpp:228] Iteration 9900, loss = 0.00370828\n",
      "I0428 05:57:47.208930  2380 solver.cpp:244]     Train net output #0: loss = 0.00370854 (* 1 = 0.00370854 loss)\n",
      "I0428 05:57:47.208940  2380 sgd_solver.cpp:106] Iteration 9900, lr = 0.00596843\n",
      "I0428 05:57:53.781157  2380 solver.cpp:454] Snapshotting to binary proto file ./lenet/lenet_iter_10000.caffemodel\n",
      "I0428 05:57:53.797415  2380 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./lenet/lenet_iter_10000.solverstate\n",
      "I0428 05:57:53.867831  2380 solver.cpp:317] Iteration 10000, loss = 0.00400323\n",
      "I0428 05:57:53.867882  2380 solver.cpp:337] Iteration 10000, Testing net (#0)\n",
      "I0428 05:57:58.039299  2380 solver.cpp:404]     Test net output #0: accuracy = 0.9917\n",
      "I0428 05:57:58.039336  2380 solver.cpp:404]     Test net output #1: loss = 0.0268446 (* 1 = 0.0268446 loss)\n",
      "I0428 05:57:58.039345  2380 solver.cpp:322] Optimization Done.\n",
      "I0428 05:57:58.039352  2380 caffe.cpp:222] Optimization Done.\n"
     ]
    }
   ],
   "source": [
    "!./caffe/build/tools/caffe train --solver=./lenet/lenet_solver.prototxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caffe에서는 예기치 않은 이유로 학습이 중단되었을 경우 저장된 지점(.solverstate 파일)이 있다면 이어서 학습이 가능합니다. (Snapshot resume 기능)\n",
    "\n",
    "이미 5000번까지 학습된 Solver state가 저장되어 있으므로 아래의 명령어를 통해 iteration 5000번부터 이어서 학습이 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libdc1394 error: Failed to initialize libdc1394\n",
      "I0428 05:58:01.580147  2386 caffe.cpp:178] Use CPU.\n",
      "I0428 05:58:01.580804  2386 solver.cpp:48] Initializing solver from parameters: \n",
      "test_iter: 100\n",
      "test_interval: 500\n",
      "base_lr: 0.01\n",
      "display: 100\n",
      "max_iter: 10000\n",
      "lr_policy: \"inv\"\n",
      "gamma: 0.0001\n",
      "power: 0.75\n",
      "momentum: 0.9\n",
      "weight_decay: 0.0005\n",
      "snapshot: 5000\n",
      "snapshot_prefix: \"./lenet/lenet\"\n",
      "solver_mode: CPU\n",
      "net: \"./lenet/lenet_train_test.prototxt\"\n",
      "I0428 05:58:01.581079  2386 solver.cpp:91] Creating training net from net file: ./lenet/lenet_train_test.prototxt\n",
      "I0428 05:58:01.581626  2386 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist\n",
      "I0428 05:58:01.581687  2386 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy\n",
      "I0428 05:58:01.581850  2386 net.cpp:49] Initializing net from parameters: \n",
      "name: \"LeNet\"\n",
      "state {\n",
      "  phase: TRAIN\n",
      "}\n",
      "layer {\n",
      "  name: \"mnist\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TRAIN\n",
      "  }\n",
      "  transform_param {\n",
      "    scale: 0.00390625\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"./lenet/mnist_train_lmdb\"\n",
      "    batch_size: 64\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 20\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 50\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"ip1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"ip1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 500\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"ip2\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 10\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"ip2\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I0428 05:58:01.582666  2386 layer_factory.hpp:77] Creating layer mnist\n",
      "I0428 05:58:01.583439  2386 net.cpp:91] Creating Layer mnist\n",
      "I0428 05:58:01.583480  2386 net.cpp:399] mnist -> data\n",
      "I0428 05:58:01.583529  2386 net.cpp:399] mnist -> label\n",
      "I0428 05:58:01.583778  2387 db_lmdb.cpp:38] Opened lmdb ./lenet/mnist_train_lmdb\n",
      "I0428 05:58:01.584067  2386 data_layer.cpp:41] output data size: 64,1,28,28\n",
      "I0428 05:58:01.584985  2386 net.cpp:141] Setting up mnist\n",
      "I0428 05:58:01.585029  2386 net.cpp:148] Top shape: 64 1 28 28 (50176)\n",
      "I0428 05:58:01.585052  2386 net.cpp:148] Top shape: 64 (64)\n",
      "I0428 05:58:01.585068  2386 net.cpp:156] Memory required for data: 200960\n",
      "I0428 05:58:01.585090  2386 layer_factory.hpp:77] Creating layer conv1\n",
      "I0428 05:58:01.585129  2386 net.cpp:91] Creating Layer conv1\n",
      "I0428 05:58:01.585149  2386 net.cpp:425] conv1 <- data\n",
      "I0428 05:58:01.585178  2386 net.cpp:399] conv1 -> conv1\n",
      "I0428 05:58:01.585274  2386 net.cpp:141] Setting up conv1\n",
      "I0428 05:58:01.585304  2386 net.cpp:148] Top shape: 64 20 24 24 (737280)\n",
      "I0428 05:58:01.585320  2386 net.cpp:156] Memory required for data: 3150080\n",
      "I0428 05:58:01.585355  2386 layer_factory.hpp:77] Creating layer pool1\n",
      "I0428 05:58:01.585381  2386 net.cpp:91] Creating Layer pool1\n",
      "I0428 05:58:01.585398  2386 net.cpp:425] pool1 <- conv1\n",
      "I0428 05:58:01.585417  2386 net.cpp:399] pool1 -> pool1\n",
      "I0428 05:58:01.585459  2386 net.cpp:141] Setting up pool1\n",
      "I0428 05:58:01.585520  2386 net.cpp:148] Top shape: 64 20 12 12 (184320)\n",
      "I0428 05:58:01.585537  2386 net.cpp:156] Memory required for data: 3887360\n",
      "I0428 05:58:01.585575  2386 layer_factory.hpp:77] Creating layer conv2\n",
      "I0428 05:58:01.585619  2386 net.cpp:91] Creating Layer conv2\n",
      "I0428 05:58:01.585649  2386 net.cpp:425] conv2 <- pool1\n",
      "I0428 05:58:01.585686  2386 net.cpp:399] conv2 -> conv2\n",
      "I0428 05:58:01.586091  2386 net.cpp:141] Setting up conv2\n",
      "I0428 05:58:01.586141  2386 net.cpp:148] Top shape: 64 50 8 8 (204800)\n",
      "I0428 05:58:01.586160  2386 net.cpp:156] Memory required for data: 4706560\n",
      "I0428 05:58:01.586187  2386 layer_factory.hpp:77] Creating layer pool2\n",
      "I0428 05:58:01.586212  2386 net.cpp:91] Creating Layer pool2\n",
      "I0428 05:58:01.586230  2386 net.cpp:425] pool2 <- conv2\n",
      "I0428 05:58:01.586251  2386 net.cpp:399] pool2 -> pool2\n",
      "I0428 05:58:01.586279  2386 net.cpp:141] Setting up pool2\n",
      "I0428 05:58:01.586299  2386 net.cpp:148] Top shape: 64 50 4 4 (51200)\n",
      "I0428 05:58:01.586315  2386 net.cpp:156] Memory required for data: 4911360\n",
      "I0428 05:58:01.586331  2386 layer_factory.hpp:77] Creating layer ip1\n",
      "I0428 05:58:01.586354  2386 net.cpp:91] Creating Layer ip1\n",
      "I0428 05:58:01.586371  2386 net.cpp:425] ip1 <- pool2\n",
      "I0428 05:58:01.586391  2386 net.cpp:399] ip1 -> ip1\n",
      "I0428 05:58:01.591675  2386 net.cpp:141] Setting up ip1\n",
      "I0428 05:58:01.591711  2386 net.cpp:148] Top shape: 64 500 (32000)\n",
      "I0428 05:58:01.591727  2386 net.cpp:156] Memory required for data: 5039360\n",
      "I0428 05:58:01.591755  2386 layer_factory.hpp:77] Creating layer relu1\n",
      "I0428 05:58:01.591778  2386 net.cpp:91] Creating Layer relu1\n",
      "I0428 05:58:01.591795  2386 net.cpp:425] relu1 <- ip1\n",
      "I0428 05:58:01.591814  2386 net.cpp:386] relu1 -> ip1 (in-place)\n",
      "I0428 05:58:01.591838  2386 net.cpp:141] Setting up relu1\n",
      "I0428 05:58:01.591857  2386 net.cpp:148] Top shape: 64 500 (32000)\n",
      "I0428 05:58:01.591872  2386 net.cpp:156] Memory required for data: 5167360\n",
      "I0428 05:58:01.591887  2386 layer_factory.hpp:77] Creating layer ip2\n",
      "I0428 05:58:01.591908  2386 net.cpp:91] Creating Layer ip2\n",
      "I0428 05:58:01.591924  2386 net.cpp:425] ip2 <- ip1\n",
      "I0428 05:58:01.591945  2386 net.cpp:399] ip2 -> ip2\n",
      "I0428 05:58:01.592051  2386 net.cpp:141] Setting up ip2\n",
      "I0428 05:58:01.592077  2386 net.cpp:148] Top shape: 64 10 (640)\n",
      "I0428 05:58:01.592092  2386 net.cpp:156] Memory required for data: 5169920\n",
      "I0428 05:58:01.592113  2386 layer_factory.hpp:77] Creating layer loss\n",
      "I0428 05:58:01.592135  2386 net.cpp:91] Creating Layer loss\n",
      "I0428 05:58:01.592151  2386 net.cpp:425] loss <- ip2\n",
      "I0428 05:58:01.592170  2386 net.cpp:425] loss <- label\n",
      "I0428 05:58:01.592190  2386 net.cpp:399] loss -> loss\n",
      "I0428 05:58:01.592224  2386 layer_factory.hpp:77] Creating layer loss\n",
      "I0428 05:58:01.592262  2386 net.cpp:141] Setting up loss\n",
      "I0428 05:58:01.592285  2386 net.cpp:148] Top shape: (1)\n",
      "I0428 05:58:01.592301  2386 net.cpp:151]     with loss weight 1\n",
      "I0428 05:58:01.592332  2386 net.cpp:156] Memory required for data: 5169924\n",
      "I0428 05:58:01.592350  2386 net.cpp:217] loss needs backward computation.\n",
      "I0428 05:58:01.592366  2386 net.cpp:217] ip2 needs backward computation.\n",
      "I0428 05:58:01.592382  2386 net.cpp:217] relu1 needs backward computation.\n",
      "I0428 05:58:01.592398  2386 net.cpp:217] ip1 needs backward computation.\n",
      "I0428 05:58:01.592414  2386 net.cpp:217] pool2 needs backward computation.\n",
      "I0428 05:58:01.592430  2386 net.cpp:217] conv2 needs backward computation.\n",
      "I0428 05:58:01.592447  2386 net.cpp:217] pool1 needs backward computation.\n",
      "I0428 05:58:01.592463  2386 net.cpp:217] conv1 needs backward computation.\n",
      "I0428 05:58:01.592478  2386 net.cpp:219] mnist does not need backward computation.\n",
      "I0428 05:58:01.592494  2386 net.cpp:261] This network produces output loss\n",
      "I0428 05:58:01.592520  2386 net.cpp:274] Network initialization done.\n",
      "I0428 05:58:01.593008  2386 solver.cpp:181] Creating test net (#0) specified by net file: ./lenet/lenet_train_test.prototxt\n",
      "I0428 05:58:01.593075  2386 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist\n",
      "I0428 05:58:01.593232  2386 net.cpp:49] Initializing net from parameters: \n",
      "name: \"LeNet\"\n",
      "state {\n",
      "  phase: TEST\n",
      "}\n",
      "layer {\n",
      "  name: \"mnist\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "  transform_param {\n",
      "    scale: 0.00390625\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"./lenet/mnist_test_lmdb\"\n",
      "    batch_size: 100\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 20\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 50\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"ip1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"ip1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 500\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"ip2\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 10\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"accuracy\"\n",
      "  type: \"Accuracy\"\n",
      "  bottom: \"ip2\"\n",
      "  bottom: \"label\"\n",
      "  top: \"accuracy\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"ip2\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I0428 05:58:01.594177  2386 layer_factory.hpp:77] Creating layer mnist\n",
      "I0428 05:58:01.594360  2386 net.cpp:91] Creating Layer mnist\n",
      "I0428 05:58:01.594391  2386 net.cpp:399] mnist -> data\n",
      "I0428 05:58:01.594418  2386 net.cpp:399] mnist -> label\n",
      "I0428 05:58:01.594584  2389 db_lmdb.cpp:38] Opened lmdb ./lenet/mnist_test_lmdb\n",
      "I0428 05:58:01.594697  2386 data_layer.cpp:41] output data size: 100,1,28,28\n",
      "I0428 05:58:01.595929  2386 net.cpp:141] Setting up mnist\n",
      "I0428 05:58:01.595963  2386 net.cpp:148] Top shape: 100 1 28 28 (78400)\n",
      "I0428 05:58:01.595983  2386 net.cpp:148] Top shape: 100 (100)\n",
      "I0428 05:58:01.595999  2386 net.cpp:156] Memory required for data: 314000\n",
      "I0428 05:58:01.596016  2386 layer_factory.hpp:77] Creating layer label_mnist_1_split\n",
      "I0428 05:58:01.596053  2386 net.cpp:91] Creating Layer label_mnist_1_split\n",
      "I0428 05:58:01.596072  2386 net.cpp:425] label_mnist_1_split <- label\n",
      "I0428 05:58:01.596094  2386 net.cpp:399] label_mnist_1_split -> label_mnist_1_split_0\n",
      "I0428 05:58:01.596117  2386 net.cpp:399] label_mnist_1_split -> label_mnist_1_split_1\n",
      "I0428 05:58:01.596143  2386 net.cpp:141] Setting up label_mnist_1_split\n",
      "I0428 05:58:01.596168  2386 net.cpp:148] Top shape: 100 (100)\n",
      "I0428 05:58:01.596185  2386 net.cpp:148] Top shape: 100 (100)\n",
      "I0428 05:58:01.596204  2386 net.cpp:156] Memory required for data: 314800\n",
      "I0428 05:58:01.596220  2386 layer_factory.hpp:77] Creating layer conv1\n",
      "I0428 05:58:01.596249  2386 net.cpp:91] Creating Layer conv1\n",
      "I0428 05:58:01.596266  2386 net.cpp:425] conv1 <- data\n",
      "I0428 05:58:01.596297  2386 net.cpp:399] conv1 -> conv1\n",
      "I0428 05:58:01.596362  2386 net.cpp:141] Setting up conv1\n",
      "I0428 05:58:01.596385  2386 net.cpp:148] Top shape: 100 20 24 24 (1152000)\n",
      "I0428 05:58:01.596401  2386 net.cpp:156] Memory required for data: 4922800\n",
      "I0428 05:58:01.596431  2386 layer_factory.hpp:77] Creating layer pool1\n",
      "I0428 05:58:01.596457  2386 net.cpp:91] Creating Layer pool1\n",
      "I0428 05:58:01.596473  2386 net.cpp:425] pool1 <- conv1\n",
      "I0428 05:58:01.596493  2386 net.cpp:399] pool1 -> pool1\n",
      "I0428 05:58:01.596546  2386 net.cpp:141] Setting up pool1\n",
      "I0428 05:58:01.596582  2386 net.cpp:148] Top shape: 100 20 12 12 (288000)\n",
      "I0428 05:58:01.596607  2386 net.cpp:156] Memory required for data: 6074800\n",
      "I0428 05:58:01.596623  2386 layer_factory.hpp:77] Creating layer conv2\n",
      "I0428 05:58:01.596658  2386 net.cpp:91] Creating Layer conv2\n",
      "I0428 05:58:01.596674  2386 net.cpp:425] conv2 <- pool1\n",
      "I0428 05:58:01.596700  2386 net.cpp:399] conv2 -> conv2\n",
      "I0428 05:58:01.597050  2386 net.cpp:141] Setting up conv2\n",
      "I0428 05:58:01.597080  2386 net.cpp:148] Top shape: 100 50 8 8 (320000)\n",
      "I0428 05:58:01.597098  2386 net.cpp:156] Memory required for data: 7354800\n",
      "I0428 05:58:01.597122  2386 layer_factory.hpp:77] Creating layer pool2\n",
      "I0428 05:58:01.597156  2386 net.cpp:91] Creating Layer pool2\n",
      "I0428 05:58:01.597172  2386 net.cpp:425] pool2 <- conv2\n",
      "I0428 05:58:01.597194  2386 net.cpp:399] pool2 -> pool2\n",
      "I0428 05:58:01.597218  2386 net.cpp:141] Setting up pool2\n",
      "I0428 05:58:01.597237  2386 net.cpp:148] Top shape: 100 50 4 4 (80000)\n",
      "I0428 05:58:01.597252  2386 net.cpp:156] Memory required for data: 7674800\n",
      "I0428 05:58:01.597267  2386 layer_factory.hpp:77] Creating layer ip1\n",
      "I0428 05:58:01.597291  2386 net.cpp:91] Creating Layer ip1\n",
      "I0428 05:58:01.597308  2386 net.cpp:425] ip1 <- pool2\n",
      "I0428 05:58:01.597332  2386 net.cpp:399] ip1 -> ip1\n",
      "I0428 05:58:01.602308  2386 net.cpp:141] Setting up ip1\n",
      "I0428 05:58:01.602342  2386 net.cpp:148] Top shape: 100 500 (50000)\n",
      "I0428 05:58:01.602357  2386 net.cpp:156] Memory required for data: 7874800\n",
      "I0428 05:58:01.602382  2386 layer_factory.hpp:77] Creating layer relu1\n",
      "I0428 05:58:01.602402  2386 net.cpp:91] Creating Layer relu1\n",
      "I0428 05:58:01.602422  2386 net.cpp:425] relu1 <- ip1\n",
      "I0428 05:58:01.602444  2386 net.cpp:386] relu1 -> ip1 (in-place)\n",
      "I0428 05:58:01.602466  2386 net.cpp:141] Setting up relu1\n",
      "I0428 05:58:01.602483  2386 net.cpp:148] Top shape: 100 500 (50000)\n",
      "I0428 05:58:01.602497  2386 net.cpp:156] Memory required for data: 8074800\n",
      "I0428 05:58:01.602512  2386 layer_factory.hpp:77] Creating layer ip2\n",
      "I0428 05:58:01.602533  2386 net.cpp:91] Creating Layer ip2\n",
      "I0428 05:58:01.602548  2386 net.cpp:425] ip2 <- ip1\n",
      "I0428 05:58:01.602571  2386 net.cpp:399] ip2 -> ip2\n",
      "I0428 05:58:01.602665  2386 net.cpp:141] Setting up ip2\n",
      "I0428 05:58:01.602689  2386 net.cpp:148] Top shape: 100 10 (1000)\n",
      "I0428 05:58:01.602704  2386 net.cpp:156] Memory required for data: 8078800\n",
      "I0428 05:58:01.602722  2386 layer_factory.hpp:77] Creating layer ip2_ip2_0_split\n",
      "I0428 05:58:01.602744  2386 net.cpp:91] Creating Layer ip2_ip2_0_split\n",
      "I0428 05:58:01.602761  2386 net.cpp:425] ip2_ip2_0_split <- ip2\n",
      "I0428 05:58:01.602778  2386 net.cpp:399] ip2_ip2_0_split -> ip2_ip2_0_split_0\n",
      "I0428 05:58:01.602797  2386 net.cpp:399] ip2_ip2_0_split -> ip2_ip2_0_split_1\n",
      "I0428 05:58:01.602819  2386 net.cpp:141] Setting up ip2_ip2_0_split\n",
      "I0428 05:58:01.602836  2386 net.cpp:148] Top shape: 100 10 (1000)\n",
      "I0428 05:58:01.602852  2386 net.cpp:148] Top shape: 100 10 (1000)\n",
      "I0428 05:58:01.602866  2386 net.cpp:156] Memory required for data: 8086800\n",
      "I0428 05:58:01.602881  2386 layer_factory.hpp:77] Creating layer accuracy\n",
      "I0428 05:58:01.602900  2386 net.cpp:91] Creating Layer accuracy\n",
      "I0428 05:58:01.602916  2386 net.cpp:425] accuracy <- ip2_ip2_0_split_0\n",
      "I0428 05:58:01.602933  2386 net.cpp:425] accuracy <- label_mnist_1_split_0\n",
      "I0428 05:58:01.602952  2386 net.cpp:399] accuracy -> accuracy\n",
      "I0428 05:58:01.602974  2386 net.cpp:141] Setting up accuracy\n",
      "I0428 05:58:01.602993  2386 net.cpp:148] Top shape: (1)\n",
      "I0428 05:58:01.603006  2386 net.cpp:156] Memory required for data: 8086804\n",
      "I0428 05:58:01.603021  2386 layer_factory.hpp:77] Creating layer loss\n",
      "I0428 05:58:01.603042  2386 net.cpp:91] Creating Layer loss\n",
      "I0428 05:58:01.603060  2386 net.cpp:425] loss <- ip2_ip2_0_split_1\n",
      "I0428 05:58:01.603075  2386 net.cpp:425] loss <- label_mnist_1_split_1\n",
      "I0428 05:58:01.603093  2386 net.cpp:399] loss -> loss\n",
      "I0428 05:58:01.603116  2386 layer_factory.hpp:77] Creating layer loss\n",
      "I0428 05:58:01.603150  2386 net.cpp:141] Setting up loss\n",
      "I0428 05:58:01.603173  2386 net.cpp:148] Top shape: (1)\n",
      "I0428 05:58:01.603217  2386 net.cpp:151]     with loss weight 1\n",
      "I0428 05:58:01.603240  2386 net.cpp:156] Memory required for data: 8086808\n",
      "I0428 05:58:01.603255  2386 net.cpp:217] loss needs backward computation.\n",
      "I0428 05:58:01.603271  2386 net.cpp:219] accuracy does not need backward computation.\n",
      "I0428 05:58:01.603286  2386 net.cpp:217] ip2_ip2_0_split needs backward computation.\n",
      "I0428 05:58:01.603302  2386 net.cpp:217] ip2 needs backward computation.\n",
      "I0428 05:58:01.603317  2386 net.cpp:217] relu1 needs backward computation.\n",
      "I0428 05:58:01.603330  2386 net.cpp:217] ip1 needs backward computation.\n",
      "I0428 05:58:01.603345  2386 net.cpp:217] pool2 needs backward computation.\n",
      "I0428 05:58:01.603360  2386 net.cpp:217] conv2 needs backward computation.\n",
      "I0428 05:58:01.603379  2386 net.cpp:217] pool1 needs backward computation.\n",
      "I0428 05:58:01.603397  2386 net.cpp:217] conv1 needs backward computation.\n",
      "I0428 05:58:01.603412  2386 net.cpp:219] label_mnist_1_split does not need backward computation.\n",
      "I0428 05:58:01.603430  2386 net.cpp:219] mnist does not need backward computation.\n",
      "I0428 05:58:01.603443  2386 net.cpp:261] This network produces output accuracy\n",
      "I0428 05:58:01.603458  2386 net.cpp:261] This network produces output loss\n",
      "I0428 05:58:01.603487  2386 net.cpp:274] Network initialization done.\n",
      "I0428 05:58:01.603560  2386 solver.cpp:60] Solver scaffolding done.\n",
      "I0428 05:58:01.603605  2386 caffe.cpp:209] Resuming from ./lenet/lenet_iter_5000.solverstate\n",
      "I0428 05:58:01.615130  2386 sgd_solver.cpp:318] SGDSolver: restoring history\n",
      "I0428 05:58:01.616169  2386 caffe.cpp:219] Starting Optimization\n",
      "I0428 05:58:01.616199  2386 solver.cpp:279] Solving LeNet\n",
      "I0428 05:58:01.616215  2386 solver.cpp:280] Learning Rate Policy: inv\n",
      "I0428 05:58:01.616948  2386 solver.cpp:337] Iteration 5000, Testing net (#0)\n",
      "I0428 05:58:05.863526  2386 solver.cpp:404]     Test net output #0: accuracy = 0.9901\n",
      "I0428 05:58:05.863566  2386 solver.cpp:404]     Test net output #1: loss = 0.0314304 (* 1 = 0.0314304 loss)\n",
      "I0428 05:58:05.934186  2386 solver.cpp:228] Iteration 5000, loss = 0.00546314\n",
      "I0428 05:58:05.934221  2386 solver.cpp:244]     Train net output #0: loss = 0.00546314 (* 1 = 0.00546314 loss)\n",
      "I0428 05:58:05.934240  2386 sgd_solver.cpp:106] Iteration 5000, lr = 0.00737788\n",
      "I0428 05:58:12.798292  2386 solver.cpp:228] Iteration 5100, loss = 0.0255807\n",
      "I0428 05:58:12.798341  2386 solver.cpp:244]     Train net output #0: loss = 0.0255807 (* 1 = 0.0255807 loss)\n",
      "I0428 05:58:12.798352  2386 sgd_solver.cpp:106] Iteration 5100, lr = 0.0073412\n",
      "I0428 05:58:19.642341  2386 solver.cpp:228] Iteration 5200, loss = 0.0330993\n",
      "I0428 05:58:19.642385  2386 solver.cpp:244]     Train net output #0: loss = 0.0330993 (* 1 = 0.0330993 loss)\n",
      "I0428 05:58:19.642396  2386 sgd_solver.cpp:106] Iteration 5200, lr = 0.00730495\n",
      "I0428 05:58:26.523367  2386 solver.cpp:228] Iteration 5300, loss = 0.0041379\n",
      "I0428 05:58:26.523418  2386 solver.cpp:244]     Train net output #0: loss = 0.00413788 (* 1 = 0.00413788 loss)\n",
      "I0428 05:58:26.523429  2386 sgd_solver.cpp:106] Iteration 5300, lr = 0.00726911\n",
      "I0428 05:58:33.379364  2386 solver.cpp:228] Iteration 5400, loss = 0.0172902\n",
      "I0428 05:58:33.379637  2386 solver.cpp:244]     Train net output #0: loss = 0.0172902 (* 1 = 0.0172902 loss)\n",
      "I0428 05:58:33.379688  2386 sgd_solver.cpp:106] Iteration 5400, lr = 0.00723368\n",
      "I0428 05:58:40.586083  2386 solver.cpp:337] Iteration 5500, Testing net (#0)\n",
      "I0428 05:58:44.754672  2386 solver.cpp:404]     Test net output #0: accuracy = 0.99\n",
      "I0428 05:58:44.754736  2386 solver.cpp:404]     Test net output #1: loss = 0.0300011 (* 1 = 0.0300011 loss)\n",
      "I0428 05:58:44.822734  2386 solver.cpp:228] Iteration 5500, loss = 0.00938261\n",
      "I0428 05:58:44.822768  2386 solver.cpp:244]     Train net output #0: loss = 0.00938261 (* 1 = 0.00938261 loss)\n",
      "I0428 05:58:44.822782  2386 sgd_solver.cpp:106] Iteration 5500, lr = 0.00719865\n",
      "I0428 05:58:51.743929  2386 solver.cpp:228] Iteration 5600, loss = 0.0286649\n",
      "I0428 05:58:51.743988  2386 solver.cpp:244]     Train net output #0: loss = 0.0286649 (* 1 = 0.0286649 loss)\n",
      "I0428 05:58:51.743999  2386 sgd_solver.cpp:106] Iteration 5600, lr = 0.00716402\n",
      "I0428 05:58:58.593813  2386 solver.cpp:228] Iteration 5700, loss = 0.0216898\n",
      "I0428 05:58:58.593858  2386 solver.cpp:244]     Train net output #0: loss = 0.0216898 (* 1 = 0.0216898 loss)\n",
      "I0428 05:58:58.593868  2386 sgd_solver.cpp:106] Iteration 5700, lr = 0.00712977\n",
      "I0428 05:59:05.453624  2386 solver.cpp:228] Iteration 5800, loss = 0.0552815\n",
      "I0428 05:59:05.453886  2386 solver.cpp:244]     Train net output #0: loss = 0.0552815 (* 1 = 0.0552815 loss)\n",
      "I0428 05:59:05.453915  2386 sgd_solver.cpp:106] Iteration 5800, lr = 0.0070959\n",
      "I0428 05:59:12.331197  2386 solver.cpp:228] Iteration 5900, loss = 0.0138616\n",
      "I0428 05:59:12.331257  2386 solver.cpp:244]     Train net output #0: loss = 0.0138616 (* 1 = 0.0138616 loss)\n",
      "I0428 05:59:12.331269  2386 sgd_solver.cpp:106] Iteration 5900, lr = 0.0070624\n",
      "I0428 05:59:19.046141  2386 solver.cpp:337] Iteration 6000, Testing net (#0)\n",
      "I0428 05:59:23.358017  2386 solver.cpp:404]     Test net output #0: accuracy = 0.9907\n",
      "I0428 05:59:23.358072  2386 solver.cpp:404]     Test net output #1: loss = 0.0302712 (* 1 = 0.0302712 loss)\n",
      "I0428 05:59:23.426456  2386 solver.cpp:228] Iteration 6000, loss = 0.0082366\n",
      "I0428 05:59:23.426491  2386 solver.cpp:244]     Train net output #0: loss = 0.00823659 (* 1 = 0.00823659 loss)\n",
      "I0428 05:59:23.426506  2386 sgd_solver.cpp:106] Iteration 6000, lr = 0.00702927\n",
      "I0428 05:59:30.318054  2386 solver.cpp:228] Iteration 6100, loss = 0.000798283\n",
      "I0428 05:59:30.318102  2386 solver.cpp:244]     Train net output #0: loss = 0.000798269 (* 1 = 0.000798269 loss)\n",
      "I0428 05:59:30.318114  2386 sgd_solver.cpp:106] Iteration 6100, lr = 0.0069965\n",
      "I0428 05:59:37.230355  2386 solver.cpp:228] Iteration 6200, loss = 0.00328003\n",
      "I0428 05:59:37.230475  2386 solver.cpp:244]     Train net output #0: loss = 0.00328002 (* 1 = 0.00328002 loss)\n",
      "I0428 05:59:37.230487  2386 sgd_solver.cpp:106] Iteration 6200, lr = 0.00696408\n",
      "I0428 05:59:44.090745  2386 solver.cpp:228] Iteration 6300, loss = 0.00205509\n",
      "I0428 05:59:44.090788  2386 solver.cpp:244]     Train net output #0: loss = 0.00205508 (* 1 = 0.00205508 loss)\n",
      "I0428 05:59:44.090798  2386 sgd_solver.cpp:106] Iteration 6300, lr = 0.00693201\n",
      "I0428 05:59:50.947374  2386 solver.cpp:228] Iteration 6400, loss = 0.0012221\n",
      "I0428 05:59:50.947415  2386 solver.cpp:244]     Train net output #0: loss = 0.00122208 (* 1 = 0.00122208 loss)\n",
      "I0428 05:59:50.947425  2386 sgd_solver.cpp:106] Iteration 6400, lr = 0.00690029\n",
      "I0428 05:59:57.745827  2386 solver.cpp:337] Iteration 6500, Testing net (#0)\n",
      "I0428 06:00:01.784561  2386 solver.cpp:404]     Test net output #0: accuracy = 0.991\n",
      "I0428 06:00:01.784610  2386 solver.cpp:404]     Test net output #1: loss = 0.0287005 (* 1 = 0.0287005 loss)\n",
      "I0428 06:00:01.850874  2386 solver.cpp:228] Iteration 6500, loss = 0.0114944\n",
      "I0428 06:00:01.850909  2386 solver.cpp:244]     Train net output #0: loss = 0.0114944 (* 1 = 0.0114944 loss)\n",
      "I0428 06:00:01.850922  2386 sgd_solver.cpp:106] Iteration 6500, lr = 0.0068689\n",
      "I0428 06:00:08.711292  2386 solver.cpp:228] Iteration 6600, loss = 0.0116365\n",
      "I0428 06:00:08.711539  2386 solver.cpp:244]     Train net output #0: loss = 0.0116365 (* 1 = 0.0116365 loss)\n",
      "I0428 06:00:08.711587  2386 sgd_solver.cpp:106] Iteration 6600, lr = 0.00683784\n",
      "I0428 06:00:15.519155  2386 solver.cpp:228] Iteration 6700, loss = 0.00396858\n",
      "I0428 06:00:15.519204  2386 solver.cpp:244]     Train net output #0: loss = 0.00396856 (* 1 = 0.00396856 loss)\n",
      "I0428 06:00:15.519215  2386 sgd_solver.cpp:106] Iteration 6700, lr = 0.00680711\n",
      "I0428 06:00:22.286756  2386 solver.cpp:228] Iteration 6800, loss = 0.0104157\n",
      "I0428 06:00:22.286803  2386 solver.cpp:244]     Train net output #0: loss = 0.0104157 (* 1 = 0.0104157 loss)\n",
      "I0428 06:00:22.286815  2386 sgd_solver.cpp:106] Iteration 6800, lr = 0.0067767\n",
      "I0428 06:00:29.060240  2386 solver.cpp:228] Iteration 6900, loss = 0.0646607\n",
      "I0428 06:00:29.060302  2386 solver.cpp:244]     Train net output #0: loss = 0.0646607 (* 1 = 0.0646607 loss)\n",
      "I0428 06:00:29.060312  2386 sgd_solver.cpp:106] Iteration 6900, lr = 0.0067466\n",
      "I0428 06:00:35.953761  2386 solver.cpp:337] Iteration 7000, Testing net (#0)\n",
      "I0428 06:00:39.991868  2386 solver.cpp:404]     Test net output #0: accuracy = 0.9887\n",
      "I0428 06:00:39.992085  2386 solver.cpp:404]     Test net output #1: loss = 0.0363839 (* 1 = 0.0363839 loss)\n",
      "I0428 06:00:40.136099  2386 solver.cpp:228] Iteration 7000, loss = 0.00382901\n",
      "I0428 06:00:40.136147  2386 solver.cpp:244]     Train net output #0: loss = 0.003829 (* 1 = 0.003829 loss)\n",
      "I0428 06:00:40.136168  2386 sgd_solver.cpp:106] Iteration 7000, lr = 0.00671681\n",
      "I0428 06:00:47.064080  2386 solver.cpp:228] Iteration 7100, loss = 0.00310893\n",
      "I0428 06:00:47.064141  2386 solver.cpp:244]     Train net output #0: loss = 0.00310891 (* 1 = 0.00310891 loss)\n",
      "I0428 06:00:47.064153  2386 sgd_solver.cpp:106] Iteration 7100, lr = 0.00668733\n",
      "I0428 06:00:53.830914  2386 solver.cpp:228] Iteration 7200, loss = 0.0052896\n",
      "I0428 06:00:53.830974  2386 solver.cpp:244]     Train net output #0: loss = 0.00528958 (* 1 = 0.00528958 loss)\n",
      "I0428 06:00:53.830986  2386 sgd_solver.cpp:106] Iteration 7200, lr = 0.00665815\n",
      "I0428 06:01:00.607249  2386 solver.cpp:228] Iteration 7300, loss = 0.0333559\n",
      "I0428 06:01:00.607316  2386 solver.cpp:244]     Train net output #0: loss = 0.0333558 (* 1 = 0.0333558 loss)\n",
      "I0428 06:01:00.607327  2386 sgd_solver.cpp:106] Iteration 7300, lr = 0.00662927\n",
      "I0428 06:01:07.361049  2386 solver.cpp:228] Iteration 7400, loss = 0.0055103\n",
      "I0428 06:01:07.361111  2386 solver.cpp:244]     Train net output #0: loss = 0.00551027 (* 1 = 0.00551027 loss)\n",
      "I0428 06:01:07.361121  2386 sgd_solver.cpp:106] Iteration 7400, lr = 0.00660067\n",
      "I0428 06:01:14.071022  2386 solver.cpp:337] Iteration 7500, Testing net (#0)\n",
      "I0428 06:01:18.289530  2386 solver.cpp:404]     Test net output #0: accuracy = 0.9906\n",
      "I0428 06:01:18.289594  2386 solver.cpp:404]     Test net output #1: loss = 0.0288918 (* 1 = 0.0288918 loss)\n",
      "I0428 06:01:18.361019  2386 solver.cpp:228] Iteration 7500, loss = 0.00556435\n",
      "I0428 06:01:18.361069  2386 solver.cpp:244]     Train net output #0: loss = 0.00556432 (* 1 = 0.00556432 loss)\n",
      "I0428 06:01:18.361083  2386 sgd_solver.cpp:106] Iteration 7500, lr = 0.00657236\n",
      "I0428 06:01:25.297298  2386 solver.cpp:228] Iteration 7600, loss = 0.0146168\n",
      "I0428 06:01:25.297363  2386 solver.cpp:244]     Train net output #0: loss = 0.0146168 (* 1 = 0.0146168 loss)\n",
      "I0428 06:01:25.297374  2386 sgd_solver.cpp:106] Iteration 7600, lr = 0.00654433\n",
      "I0428 06:01:32.108439  2386 solver.cpp:228] Iteration 7700, loss = 0.0167307\n",
      "I0428 06:01:32.108489  2386 solver.cpp:244]     Train net output #0: loss = 0.0167307 (* 1 = 0.0167307 loss)\n",
      "I0428 06:01:32.108500  2386 sgd_solver.cpp:106] Iteration 7700, lr = 0.00651658\n",
      "I0428 06:01:39.139775  2386 solver.cpp:228] Iteration 7800, loss = 0.000430827\n",
      "I0428 06:01:39.139842  2386 solver.cpp:244]     Train net output #0: loss = 0.000430798 (* 1 = 0.000430798 loss)\n",
      "I0428 06:01:39.139853  2386 sgd_solver.cpp:106] Iteration 7800, lr = 0.00648911\n",
      "I0428 06:01:46.001972  2386 solver.cpp:228] Iteration 7900, loss = 0.00459882\n",
      "I0428 06:01:46.002053  2386 solver.cpp:244]     Train net output #0: loss = 0.0045988 (* 1 = 0.0045988 loss)\n",
      "I0428 06:01:46.002063  2386 sgd_solver.cpp:106] Iteration 7900, lr = 0.0064619\n",
      "I0428 06:01:52.797003  2386 solver.cpp:337] Iteration 8000, Testing net (#0)\n",
      "I0428 06:01:56.882338  2386 solver.cpp:404]     Test net output #0: accuracy = 0.9907\n",
      "I0428 06:01:56.882383  2386 solver.cpp:404]     Test net output #1: loss = 0.0307496 (* 1 = 0.0307496 loss)\n",
      "I0428 06:01:56.948331  2386 solver.cpp:228] Iteration 8000, loss = 0.00709109\n",
      "I0428 06:01:56.948361  2386 solver.cpp:244]     Train net output #0: loss = 0.00709108 (* 1 = 0.00709108 loss)\n",
      "I0428 06:01:56.948374  2386 sgd_solver.cpp:106] Iteration 8000, lr = 0.00643496\n",
      "I0428 06:02:03.806399  2386 solver.cpp:228] Iteration 8100, loss = 0.00772273\n",
      "I0428 06:02:03.806442  2386 solver.cpp:244]     Train net output #0: loss = 0.00772271 (* 1 = 0.00772271 loss)\n",
      "I0428 06:02:03.806452  2386 sgd_solver.cpp:106] Iteration 8100, lr = 0.00640827\n",
      "I0428 06:02:10.644801  2386 solver.cpp:228] Iteration 8200, loss = 0.00813401\n",
      "I0428 06:02:10.644842  2386 solver.cpp:244]     Train net output #0: loss = 0.00813399 (* 1 = 0.00813399 loss)\n",
      "I0428 06:02:10.644853  2386 sgd_solver.cpp:106] Iteration 8200, lr = 0.00638185\n",
      "I0428 06:02:17.488075  2386 solver.cpp:228] Iteration 8300, loss = 0.00274406\n",
      "I0428 06:02:17.488339  2386 solver.cpp:244]     Train net output #0: loss = 0.00274404 (* 1 = 0.00274404 loss)\n",
      "I0428 06:02:17.488387  2386 sgd_solver.cpp:106] Iteration 8300, lr = 0.00635567\n",
      "I0428 06:02:24.345340  2386 solver.cpp:228] Iteration 8400, loss = 0.00299824\n",
      "I0428 06:02:24.345402  2386 solver.cpp:244]     Train net output #0: loss = 0.00299822 (* 1 = 0.00299822 loss)\n",
      "I0428 06:02:24.345412  2386 sgd_solver.cpp:106] Iteration 8400, lr = 0.00632975\n",
      "I0428 06:02:31.054219  2386 solver.cpp:337] Iteration 8500, Testing net (#0)\n",
      "I0428 06:02:35.230111  2386 solver.cpp:404]     Test net output #0: accuracy = 0.9893\n",
      "I0428 06:02:35.230170  2386 solver.cpp:404]     Test net output #1: loss = 0.0343413 (* 1 = 0.0343413 loss)\n",
      "I0428 06:02:35.301555  2386 solver.cpp:228] Iteration 8500, loss = 0.0016279\n",
      "I0428 06:02:35.301590  2386 solver.cpp:244]     Train net output #0: loss = 0.00162787 (* 1 = 0.00162787 loss)\n",
      "I0428 06:02:35.301604  2386 sgd_solver.cpp:106] Iteration 8500, lr = 0.00630407\n",
      "I0428 06:02:42.080709  2386 solver.cpp:228] Iteration 8600, loss = 0.0126201\n",
      "I0428 06:02:42.080768  2386 solver.cpp:244]     Train net output #0: loss = 0.0126201 (* 1 = 0.0126201 loss)\n",
      "I0428 06:02:42.080780  2386 sgd_solver.cpp:106] Iteration 8600, lr = 0.00627864\n",
      "I0428 06:02:48.842985  2386 solver.cpp:228] Iteration 8700, loss = 0.0138885\n",
      "I0428 06:02:48.843204  2386 solver.cpp:244]     Train net output #0: loss = 0.0138885 (* 1 = 0.0138885 loss)\n",
      "I0428 06:02:48.843232  2386 sgd_solver.cpp:106] Iteration 8700, lr = 0.00625344\n",
      "I0428 06:02:55.829509  2386 solver.cpp:228] Iteration 8800, loss = 0.00352914\n",
      "I0428 06:02:55.829565  2386 solver.cpp:244]     Train net output #0: loss = 0.00352913 (* 1 = 0.00352913 loss)\n",
      "I0428 06:02:55.829576  2386 sgd_solver.cpp:106] Iteration 8800, lr = 0.00622847\n",
      "I0428 06:03:02.696120  2386 solver.cpp:228] Iteration 8900, loss = 0.00517341\n",
      "I0428 06:03:02.696157  2386 solver.cpp:244]     Train net output #0: loss = 0.0051734 (* 1 = 0.0051734 loss)\n",
      "I0428 06:03:02.696167  2386 sgd_solver.cpp:106] Iteration 8900, lr = 0.00620374\n",
      "I0428 06:03:09.496553  2386 solver.cpp:337] Iteration 9000, Testing net (#0)\n",
      "I0428 06:03:13.601557  2386 solver.cpp:404]     Test net output #0: accuracy = 0.9908\n",
      "I0428 06:03:13.601599  2386 solver.cpp:404]     Test net output #1: loss = 0.0294485 (* 1 = 0.0294485 loss)\n",
      "I0428 06:03:13.667687  2386 solver.cpp:228] Iteration 9000, loss = 0.00545233\n",
      "I0428 06:03:13.667716  2386 solver.cpp:244]     Train net output #0: loss = 0.00545232 (* 1 = 0.00545232 loss)\n",
      "I0428 06:03:13.667729  2386 sgd_solver.cpp:106] Iteration 9000, lr = 0.00617924\n",
      "I0428 06:03:20.527153  2386 solver.cpp:228] Iteration 9100, loss = 0.00638571\n",
      "I0428 06:03:20.527231  2386 solver.cpp:244]     Train net output #0: loss = 0.00638569 (* 1 = 0.00638569 loss)\n",
      "I0428 06:03:20.527242  2386 sgd_solver.cpp:106] Iteration 9100, lr = 0.00615496\n",
      "I0428 06:03:27.377689  2386 solver.cpp:228] Iteration 9200, loss = 0.00377774\n",
      "I0428 06:03:27.377729  2386 solver.cpp:244]     Train net output #0: loss = 0.00377773 (* 1 = 0.00377773 loss)\n",
      "I0428 06:03:27.377739  2386 sgd_solver.cpp:106] Iteration 9200, lr = 0.0061309\n",
      "I0428 06:03:34.230720  2386 solver.cpp:228] Iteration 9300, loss = 0.0078482\n",
      "I0428 06:03:34.230762  2386 solver.cpp:244]     Train net output #0: loss = 0.00784819 (* 1 = 0.00784819 loss)\n",
      "I0428 06:03:34.230772  2386 sgd_solver.cpp:106] Iteration 9300, lr = 0.00610706\n",
      "I0428 06:03:41.112684  2386 solver.cpp:228] Iteration 9400, loss = 0.00653077\n",
      "I0428 06:03:41.112730  2386 solver.cpp:244]     Train net output #0: loss = 0.00653076 (* 1 = 0.00653076 loss)\n",
      "I0428 06:03:41.112740  2386 sgd_solver.cpp:106] Iteration 9400, lr = 0.00608343\n",
      "I0428 06:03:47.884905  2386 solver.cpp:337] Iteration 9500, Testing net (#0)\n",
      "I0428 06:03:51.926209  2386 solver.cpp:404]     Test net output #0: accuracy = 0.991\n",
      "I0428 06:03:51.926300  2386 solver.cpp:404]     Test net output #1: loss = 0.0299083 (* 1 = 0.0299083 loss)\n",
      "I0428 06:03:51.992489  2386 solver.cpp:228] Iteration 9500, loss = 0.00408367\n",
      "I0428 06:03:51.992519  2386 solver.cpp:244]     Train net output #0: loss = 0.00408366 (* 1 = 0.00408366 loss)\n",
      "I0428 06:03:51.992532  2386 sgd_solver.cpp:106] Iteration 9500, lr = 0.00606002\n",
      "I0428 06:03:58.854127  2386 solver.cpp:228] Iteration 9600, loss = 0.00800292\n",
      "I0428 06:03:58.854169  2386 solver.cpp:244]     Train net output #0: loss = 0.00800291 (* 1 = 0.00800291 loss)\n",
      "I0428 06:03:58.854181  2386 sgd_solver.cpp:106] Iteration 9600, lr = 0.00603682\n",
      "I0428 06:04:05.715946  2386 solver.cpp:228] Iteration 9700, loss = 0.000601528\n",
      "I0428 06:04:05.715988  2386 solver.cpp:244]     Train net output #0: loss = 0.000601512 (* 1 = 0.000601512 loss)\n",
      "I0428 06:04:05.715999  2386 sgd_solver.cpp:106] Iteration 9700, lr = 0.00601382\n",
      "I0428 06:04:12.584722  2386 solver.cpp:228] Iteration 9800, loss = 0.00914702\n",
      "I0428 06:04:12.584769  2386 solver.cpp:244]     Train net output #0: loss = 0.009147 (* 1 = 0.009147 loss)\n",
      "I0428 06:04:12.584780  2386 sgd_solver.cpp:106] Iteration 9800, lr = 0.00599102\n",
      "I0428 06:04:19.448992  2386 solver.cpp:228] Iteration 9900, loss = 0.00212794\n",
      "I0428 06:04:19.449033  2386 solver.cpp:244]     Train net output #0: loss = 0.00212792 (* 1 = 0.00212792 loss)\n",
      "I0428 06:04:19.449043  2386 sgd_solver.cpp:106] Iteration 9900, lr = 0.00596843\n",
      "I0428 06:04:26.245391  2386 solver.cpp:454] Snapshotting to binary proto file ./lenet/lenet_iter_10000.caffemodel\n",
      "I0428 06:04:26.250882  2386 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./lenet/lenet_iter_10000.solverstate\n",
      "I0428 06:04:26.280791  2386 solver.cpp:317] Iteration 10000, loss = 0.0111849\n",
      "I0428 06:04:26.280822  2386 solver.cpp:337] Iteration 10000, Testing net (#0)\n",
      "I0428 06:04:30.380791  2386 solver.cpp:404]     Test net output #0: accuracy = 0.9911\n",
      "I0428 06:04:30.380820  2386 solver.cpp:404]     Test net output #1: loss = 0.0278391 (* 1 = 0.0278391 loss)\n",
      "I0428 06:04:30.380831  2386 solver.cpp:322] Optimization Done.\n",
      "I0428 06:04:30.380837  2386 caffe.cpp:222] Optimization Done.\n"
     ]
    }
   ],
   "source": [
    "!./caffe/build/tools/caffe train --solver=./lenet/lenet_solver.prototxt --snapshot=./lenet/lenet_iter_5000.solverstate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본 과정이 모두 완료되면 10000 minibatch의 학습 후 Test dataset에서 약 99.1%의 인식 성능을 갖는 모델(./lenet/lenet_iter_10000.caffemodel)을 얻을 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_mnist.sh\t\t      lenet_python_solver.prototxt\r\n",
      "lenet.prototxt\t\t      lenet_python_te.prototxt\r\n",
      "lenet5.png\t\t      lenet_python_tr.prototxt\r\n",
      "lenet_iter_10000.caffemodel   lenet_solver.prototxt\r\n",
      "lenet_iter_10000.solverstate  lenet_train_test.prototxt\r\n",
      "lenet_iter_1210.caffemodel    mnist.py\r\n",
      "lenet_iter_1210.solverstate   mnist.pyc\r\n",
      "lenet_iter_5000.caffemodel    mnist_test_lmdb\r\n",
      "lenet_iter_5000.solverstate   mnist_train_lmdb\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./lenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Testing LeNet 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "학습된 모델으로부터 Test set에 대한 성능을 측정해보겠습니다.\n",
    "MNIST dataset의 test sample 개수가 10000개이며, lenet_train_test.prototxt의 test batchsize가 100개로 설정되어 있으므로 총 100번의 Iteration을 수행하면 됩니다.\n",
    "\n",
    "즉 ./caffe/build/tools/caffe를 실행시킬때,\n",
    "- 학습된 모델을 사용하여 inference를 하기에 __test__의 인자를 넣어야 하며,\n",
    "- 학습된 모델의 definition prototxt __./lenet/lenet_train_test.prototxt__\n",
    "- 학습된 모델의 weights __./lenet/lenet_iter_10000.caffemodel__\n",
    "을 차례로 적용하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libdc1394 error: Failed to initialize libdc1394\n",
      "I0428 06:06:07.356339  2394 caffe.cpp:246] Use CPU.\n",
      "I0428 06:06:07.360182  2394 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist\n",
      "I0428 06:06:07.360401  2394 net.cpp:49] Initializing net from parameters: \n",
      "name: \"LeNet\"\n",
      "state {\n",
      "  phase: TEST\n",
      "}\n",
      "layer {\n",
      "  name: \"mnist\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "  transform_param {\n",
      "    scale: 0.00390625\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"./lenet/mnist_test_lmdb\"\n",
      "    batch_size: 100\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 20\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 50\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"ip1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"ip1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 500\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"ip2\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 10\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"accuracy\"\n",
      "  type: \"Accuracy\"\n",
      "  bottom: \"ip2\"\n",
      "  bottom: \"label\"\n",
      "  top: \"accuracy\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"ip2\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I0428 06:06:07.361582  2394 layer_factory.hpp:77] Creating layer mnist\n",
      "I0428 06:06:07.362486  2394 net.cpp:91] Creating Layer mnist\n",
      "I0428 06:06:07.362543  2394 net.cpp:399] mnist -> data\n",
      "I0428 06:06:07.362630  2394 net.cpp:399] mnist -> label\n",
      "I0428 06:06:07.362803  2395 db_lmdb.cpp:38] Opened lmdb ./lenet/mnist_test_lmdb\n",
      "I0428 06:06:07.362987  2394 data_layer.cpp:41] output data size: 100,1,28,28\n",
      "I0428 06:06:07.364974  2394 net.cpp:141] Setting up mnist\n",
      "I0428 06:06:07.365027  2394 net.cpp:148] Top shape: 100 1 28 28 (78400)\n",
      "I0428 06:06:07.365063  2394 net.cpp:148] Top shape: 100 (100)\n",
      "I0428 06:06:07.365082  2394 net.cpp:156] Memory required for data: 314000\n",
      "I0428 06:06:07.365105  2394 layer_factory.hpp:77] Creating layer label_mnist_1_split\n",
      "I0428 06:06:07.365133  2394 net.cpp:91] Creating Layer label_mnist_1_split\n",
      "I0428 06:06:07.365151  2394 net.cpp:425] label_mnist_1_split <- label\n",
      "I0428 06:06:07.365178  2394 net.cpp:399] label_mnist_1_split -> label_mnist_1_split_0\n",
      "I0428 06:06:07.365222  2394 net.cpp:399] label_mnist_1_split -> label_mnist_1_split_1\n",
      "I0428 06:06:07.365278  2394 net.cpp:141] Setting up label_mnist_1_split\n",
      "I0428 06:06:07.365310  2394 net.cpp:148] Top shape: 100 (100)\n",
      "I0428 06:06:07.365330  2394 net.cpp:148] Top shape: 100 (100)\n",
      "I0428 06:06:07.365353  2394 net.cpp:156] Memory required for data: 314800\n",
      "I0428 06:06:07.365371  2394 layer_factory.hpp:77] Creating layer conv1\n",
      "I0428 06:06:07.365407  2394 net.cpp:91] Creating Layer conv1\n",
      "I0428 06:06:07.365434  2394 net.cpp:425] conv1 <- data\n",
      "I0428 06:06:07.365478  2394 net.cpp:399] conv1 -> conv1\n",
      "I0428 06:06:07.365631  2394 net.cpp:141] Setting up conv1\n",
      "I0428 06:06:07.365679  2394 net.cpp:148] Top shape: 100 20 24 24 (1152000)\n",
      "I0428 06:06:07.365721  2394 net.cpp:156] Memory required for data: 4922800\n",
      "I0428 06:06:07.365793  2394 layer_factory.hpp:77] Creating layer pool1\n",
      "I0428 06:06:07.365855  2394 net.cpp:91] Creating Layer pool1\n",
      "I0428 06:06:07.365875  2394 net.cpp:425] pool1 <- conv1\n",
      "I0428 06:06:07.365898  2394 net.cpp:399] pool1 -> pool1\n",
      "I0428 06:06:07.365964  2394 net.cpp:141] Setting up pool1\n",
      "I0428 06:06:07.366019  2394 net.cpp:148] Top shape: 100 20 12 12 (288000)\n",
      "I0428 06:06:07.366040  2394 net.cpp:156] Memory required for data: 6074800\n",
      "I0428 06:06:07.366058  2394 layer_factory.hpp:77] Creating layer conv2\n",
      "I0428 06:06:07.366088  2394 net.cpp:91] Creating Layer conv2\n",
      "I0428 06:06:07.366106  2394 net.cpp:425] conv2 <- pool1\n",
      "I0428 06:06:07.366129  2394 net.cpp:399] conv2 -> conv2\n",
      "I0428 06:06:07.366582  2394 net.cpp:141] Setting up conv2\n",
      "I0428 06:06:07.366626  2394 net.cpp:148] Top shape: 100 50 8 8 (320000)\n",
      "I0428 06:06:07.366662  2394 net.cpp:156] Memory required for data: 7354800\n",
      "I0428 06:06:07.366691  2394 layer_factory.hpp:77] Creating layer pool2\n",
      "I0428 06:06:07.366714  2394 net.cpp:91] Creating Layer pool2\n",
      "I0428 06:06:07.366732  2394 net.cpp:425] pool2 <- conv2\n",
      "I0428 06:06:07.366752  2394 net.cpp:399] pool2 -> pool2\n",
      "I0428 06:06:07.366781  2394 net.cpp:141] Setting up pool2\n",
      "I0428 06:06:07.366816  2394 net.cpp:148] Top shape: 100 50 4 4 (80000)\n",
      "I0428 06:06:07.366852  2394 net.cpp:156] Memory required for data: 7674800\n",
      "I0428 06:06:07.366883  2394 layer_factory.hpp:77] Creating layer ip1\n",
      "I0428 06:06:07.366921  2394 net.cpp:91] Creating Layer ip1\n",
      "I0428 06:06:07.366940  2394 net.cpp:425] ip1 <- pool2\n",
      "I0428 06:06:07.366961  2394 net.cpp:399] ip1 -> ip1\n",
      "I0428 06:06:07.372351  2394 net.cpp:141] Setting up ip1\n",
      "I0428 06:06:07.372400  2394 net.cpp:148] Top shape: 100 500 (50000)\n",
      "I0428 06:06:07.372429  2394 net.cpp:156] Memory required for data: 7874800\n",
      "I0428 06:06:07.372463  2394 layer_factory.hpp:77] Creating layer relu1\n",
      "I0428 06:06:07.372488  2394 net.cpp:91] Creating Layer relu1\n",
      "I0428 06:06:07.372504  2394 net.cpp:425] relu1 <- ip1\n",
      "I0428 06:06:07.372524  2394 net.cpp:386] relu1 -> ip1 (in-place)\n",
      "I0428 06:06:07.372546  2394 net.cpp:141] Setting up relu1\n",
      "I0428 06:06:07.372568  2394 net.cpp:148] Top shape: 100 500 (50000)\n",
      "I0428 06:06:07.372596  2394 net.cpp:156] Memory required for data: 8074800\n",
      "I0428 06:06:07.372628  2394 layer_factory.hpp:77] Creating layer ip2\n",
      "I0428 06:06:07.372673  2394 net.cpp:91] Creating Layer ip2\n",
      "I0428 06:06:07.372696  2394 net.cpp:425] ip2 <- ip1\n",
      "I0428 06:06:07.372720  2394 net.cpp:399] ip2 -> ip2\n",
      "I0428 06:06:07.372854  2394 net.cpp:141] Setting up ip2\n",
      "I0428 06:06:07.372897  2394 net.cpp:148] Top shape: 100 10 (1000)\n",
      "I0428 06:06:07.372923  2394 net.cpp:156] Memory required for data: 8078800\n",
      "I0428 06:06:07.372946  2394 layer_factory.hpp:77] Creating layer ip2_ip2_0_split\n",
      "I0428 06:06:07.372967  2394 net.cpp:91] Creating Layer ip2_ip2_0_split\n",
      "I0428 06:06:07.372984  2394 net.cpp:425] ip2_ip2_0_split <- ip2\n",
      "I0428 06:06:07.373003  2394 net.cpp:399] ip2_ip2_0_split -> ip2_ip2_0_split_0\n",
      "I0428 06:06:07.373026  2394 net.cpp:399] ip2_ip2_0_split -> ip2_ip2_0_split_1\n",
      "I0428 06:06:07.373059  2394 net.cpp:141] Setting up ip2_ip2_0_split\n",
      "I0428 06:06:07.373096  2394 net.cpp:148] Top shape: 100 10 (1000)\n",
      "I0428 06:06:07.373132  2394 net.cpp:148] Top shape: 100 10 (1000)\n",
      "I0428 06:06:07.373160  2394 net.cpp:156] Memory required for data: 8086800\n",
      "I0428 06:06:07.373178  2394 layer_factory.hpp:77] Creating layer accuracy\n",
      "I0428 06:06:07.373205  2394 net.cpp:91] Creating Layer accuracy\n",
      "I0428 06:06:07.373224  2394 net.cpp:425] accuracy <- ip2_ip2_0_split_0\n",
      "I0428 06:06:07.373242  2394 net.cpp:425] accuracy <- label_mnist_1_split_0\n",
      "I0428 06:06:07.373261  2394 net.cpp:399] accuracy -> accuracy\n",
      "I0428 06:06:07.373291  2394 net.cpp:141] Setting up accuracy\n",
      "I0428 06:06:07.373325  2394 net.cpp:148] Top shape: (1)\n",
      "I0428 06:06:07.373356  2394 net.cpp:156] Memory required for data: 8086804\n",
      "I0428 06:06:07.373389  2394 layer_factory.hpp:77] Creating layer loss\n",
      "I0428 06:06:07.373426  2394 net.cpp:91] Creating Layer loss\n",
      "I0428 06:06:07.373445  2394 net.cpp:425] loss <- ip2_ip2_0_split_1\n",
      "I0428 06:06:07.373463  2394 net.cpp:425] loss <- label_mnist_1_split_1\n",
      "I0428 06:06:07.373482  2394 net.cpp:399] loss -> loss\n",
      "I0428 06:06:07.373549  2394 layer_factory.hpp:77] Creating layer loss\n",
      "I0428 06:06:07.373662  2394 net.cpp:141] Setting up loss\n",
      "I0428 06:06:07.373703  2394 net.cpp:148] Top shape: (1)\n",
      "I0428 06:06:07.373720  2394 net.cpp:151]     with loss weight 1\n",
      "I0428 06:06:07.373749  2394 net.cpp:156] Memory required for data: 8086808\n",
      "I0428 06:06:07.373764  2394 net.cpp:217] loss needs backward computation.\n",
      "I0428 06:06:07.373780  2394 net.cpp:219] accuracy does not need backward computation.\n",
      "I0428 06:06:07.373818  2394 net.cpp:217] ip2_ip2_0_split needs backward computation.\n",
      "I0428 06:06:07.373847  2394 net.cpp:217] ip2 needs backward computation.\n",
      "I0428 06:06:07.373873  2394 net.cpp:217] relu1 needs backward computation.\n",
      "I0428 06:06:07.373895  2394 net.cpp:217] ip1 needs backward computation.\n",
      "I0428 06:06:07.373924  2394 net.cpp:217] pool2 needs backward computation.\n",
      "I0428 06:06:07.373939  2394 net.cpp:217] conv2 needs backward computation.\n",
      "I0428 06:06:07.373953  2394 net.cpp:217] pool1 needs backward computation.\n",
      "I0428 06:06:07.373968  2394 net.cpp:217] conv1 needs backward computation.\n",
      "I0428 06:06:07.373985  2394 net.cpp:219] label_mnist_1_split does not need backward computation.\n",
      "I0428 06:06:07.374001  2394 net.cpp:219] mnist does not need backward computation.\n",
      "I0428 06:06:07.374018  2394 net.cpp:261] This network produces output accuracy\n",
      "I0428 06:06:07.374040  2394 net.cpp:261] This network produces output loss\n",
      "I0428 06:06:07.374096  2394 net.cpp:274] Network initialization done.\n",
      "I0428 06:06:07.380594  2394 caffe.cpp:252] Running for 100 iterations.\n",
      "I0428 06:06:07.461609  2394 caffe.cpp:275] Batch 0, accuracy = 1\n",
      "I0428 06:06:07.461648  2394 caffe.cpp:275] Batch 0, loss = 0.00494327\n",
      "I0428 06:06:07.521216  2394 caffe.cpp:275] Batch 1, accuracy = 0.99\n",
      "I0428 06:06:07.521246  2394 caffe.cpp:275] Batch 1, loss = 0.0189633\n",
      "I0428 06:06:07.571770  2394 caffe.cpp:275] Batch 2, accuracy = 0.99\n",
      "I0428 06:06:07.571795  2394 caffe.cpp:275] Batch 2, loss = 0.0146241\n",
      "I0428 06:06:07.617885  2394 caffe.cpp:275] Batch 3, accuracy = 1\n",
      "I0428 06:06:07.617909  2394 caffe.cpp:275] Batch 3, loss = 0.0088448\n",
      "I0428 06:06:07.663769  2394 caffe.cpp:275] Batch 4, accuracy = 0.98\n",
      "I0428 06:06:07.663792  2394 caffe.cpp:275] Batch 4, loss = 0.0911784\n",
      "I0428 06:06:07.709805  2394 caffe.cpp:275] Batch 5, accuracy = 0.99\n",
      "I0428 06:06:07.709828  2394 caffe.cpp:275] Batch 5, loss = 0.0494568\n",
      "I0428 06:06:07.755723  2394 caffe.cpp:275] Batch 6, accuracy = 0.99\n",
      "I0428 06:06:07.755748  2394 caffe.cpp:275] Batch 6, loss = 0.0533162\n",
      "I0428 06:06:07.801676  2394 caffe.cpp:275] Batch 7, accuracy = 0.99\n",
      "I0428 06:06:07.801699  2394 caffe.cpp:275] Batch 7, loss = 0.023383\n",
      "I0428 06:06:07.847693  2394 caffe.cpp:275] Batch 8, accuracy = 1\n",
      "I0428 06:06:07.847718  2394 caffe.cpp:275] Batch 8, loss = 0.0106201\n",
      "I0428 06:06:07.905448  2394 caffe.cpp:275] Batch 9, accuracy = 0.98\n",
      "I0428 06:06:07.905484  2394 caffe.cpp:275] Batch 9, loss = 0.057824\n",
      "I0428 06:06:07.951571  2394 caffe.cpp:275] Batch 10, accuracy = 0.98\n",
      "I0428 06:06:07.951593  2394 caffe.cpp:275] Batch 10, loss = 0.0527672\n",
      "I0428 06:06:07.996744  2394 caffe.cpp:275] Batch 11, accuracy = 0.98\n",
      "I0428 06:06:07.996764  2394 caffe.cpp:275] Batch 11, loss = 0.0374847\n",
      "I0428 06:06:08.041934  2394 caffe.cpp:275] Batch 12, accuracy = 0.94\n",
      "I0428 06:06:08.041952  2394 caffe.cpp:275] Batch 12, loss = 0.151759\n",
      "I0428 06:06:08.087049  2394 caffe.cpp:275] Batch 13, accuracy = 0.98\n",
      "I0428 06:06:08.087067  2394 caffe.cpp:275] Batch 13, loss = 0.0389786\n",
      "I0428 06:06:08.132228  2394 caffe.cpp:275] Batch 14, accuracy = 1\n",
      "I0428 06:06:08.132249  2394 caffe.cpp:275] Batch 14, loss = 0.00773707\n",
      "I0428 06:06:08.177376  2394 caffe.cpp:275] Batch 15, accuracy = 0.98\n",
      "I0428 06:06:08.177397  2394 caffe.cpp:275] Batch 15, loss = 0.060505\n",
      "I0428 06:06:08.222519  2394 caffe.cpp:275] Batch 16, accuracy = 0.98\n",
      "I0428 06:06:08.222537  2394 caffe.cpp:275] Batch 16, loss = 0.0277202\n",
      "I0428 06:06:08.267662  2394 caffe.cpp:275] Batch 17, accuracy = 0.98\n",
      "I0428 06:06:08.267680  2394 caffe.cpp:275] Batch 17, loss = 0.0332677\n",
      "I0428 06:06:08.312788  2394 caffe.cpp:275] Batch 18, accuracy = 0.99\n",
      "I0428 06:06:08.312832  2394 caffe.cpp:275] Batch 18, loss = 0.0271193\n",
      "I0428 06:06:08.357944  2394 caffe.cpp:275] Batch 19, accuracy = 0.99\n",
      "I0428 06:06:08.357961  2394 caffe.cpp:275] Batch 19, loss = 0.0437079\n",
      "I0428 06:06:08.403092  2394 caffe.cpp:275] Batch 20, accuracy = 0.98\n",
      "I0428 06:06:08.403111  2394 caffe.cpp:275] Batch 20, loss = 0.0682979\n",
      "I0428 06:06:08.448321  2394 caffe.cpp:275] Batch 21, accuracy = 0.97\n",
      "I0428 06:06:08.448339  2394 caffe.cpp:275] Batch 21, loss = 0.116541\n",
      "I0428 06:06:08.493448  2394 caffe.cpp:275] Batch 22, accuracy = 1\n",
      "I0428 06:06:08.493466  2394 caffe.cpp:275] Batch 22, loss = 0.0169224\n",
      "I0428 06:06:08.538594  2394 caffe.cpp:275] Batch 23, accuracy = 1\n",
      "I0428 06:06:08.538611  2394 caffe.cpp:275] Batch 23, loss = 0.0216001\n",
      "I0428 06:06:08.582753  2394 caffe.cpp:275] Batch 24, accuracy = 0.98\n",
      "I0428 06:06:08.582772  2394 caffe.cpp:275] Batch 24, loss = 0.0640744\n",
      "I0428 06:06:08.623275  2394 caffe.cpp:275] Batch 25, accuracy = 0.99\n",
      "I0428 06:06:08.623289  2394 caffe.cpp:275] Batch 25, loss = 0.0756259\n",
      "I0428 06:06:08.663650  2394 caffe.cpp:275] Batch 26, accuracy = 0.99\n",
      "I0428 06:06:08.663666  2394 caffe.cpp:275] Batch 26, loss = 0.0820825\n",
      "I0428 06:06:08.703774  2394 caffe.cpp:275] Batch 27, accuracy = 0.99\n",
      "I0428 06:06:08.703793  2394 caffe.cpp:275] Batch 27, loss = 0.0315131\n",
      "I0428 06:06:08.743217  2394 caffe.cpp:275] Batch 28, accuracy = 0.99\n",
      "I0428 06:06:08.743232  2394 caffe.cpp:275] Batch 28, loss = 0.0523398\n",
      "I0428 06:06:08.782753  2394 caffe.cpp:275] Batch 29, accuracy = 0.97\n",
      "I0428 06:06:08.782770  2394 caffe.cpp:275] Batch 29, loss = 0.100395\n",
      "I0428 06:06:08.822120  2394 caffe.cpp:275] Batch 30, accuracy = 0.98\n",
      "I0428 06:06:08.822149  2394 caffe.cpp:275] Batch 30, loss = 0.0433287\n",
      "I0428 06:06:08.861536  2394 caffe.cpp:275] Batch 31, accuracy = 1\n",
      "I0428 06:06:08.861568  2394 caffe.cpp:275] Batch 31, loss = 0.00342688\n",
      "I0428 06:06:08.901592  2394 caffe.cpp:275] Batch 32, accuracy = 0.99\n",
      "I0428 06:06:08.901608  2394 caffe.cpp:275] Batch 32, loss = 0.0184945\n",
      "I0428 06:06:08.941035  2394 caffe.cpp:275] Batch 33, accuracy = 1\n",
      "I0428 06:06:08.941051  2394 caffe.cpp:275] Batch 33, loss = 0.00238307\n",
      "I0428 06:06:08.980414  2394 caffe.cpp:275] Batch 34, accuracy = 0.99\n",
      "I0428 06:06:08.980430  2394 caffe.cpp:275] Batch 34, loss = 0.0432942\n",
      "I0428 06:06:09.019799  2394 caffe.cpp:275] Batch 35, accuracy = 0.96\n",
      "I0428 06:06:09.019827  2394 caffe.cpp:275] Batch 35, loss = 0.12808\n",
      "I0428 06:06:09.059195  2394 caffe.cpp:275] Batch 36, accuracy = 1\n",
      "I0428 06:06:09.059211  2394 caffe.cpp:275] Batch 36, loss = 0.00177845\n",
      "I0428 06:06:09.098690  2394 caffe.cpp:275] Batch 37, accuracy = 0.99\n",
      "I0428 06:06:09.098706  2394 caffe.cpp:275] Batch 37, loss = 0.0240449\n",
      "I0428 06:06:09.138200  2394 caffe.cpp:275] Batch 38, accuracy = 0.99\n",
      "I0428 06:06:09.138226  2394 caffe.cpp:275] Batch 38, loss = 0.0270761\n",
      "I0428 06:06:09.177589  2394 caffe.cpp:275] Batch 39, accuracy = 0.97\n",
      "I0428 06:06:09.177616  2394 caffe.cpp:275] Batch 39, loss = 0.0363672\n",
      "I0428 06:06:09.217949  2394 caffe.cpp:275] Batch 40, accuracy = 0.99\n",
      "I0428 06:06:09.217967  2394 caffe.cpp:275] Batch 40, loss = 0.0171617\n",
      "I0428 06:06:09.258008  2394 caffe.cpp:275] Batch 41, accuracy = 0.99\n",
      "I0428 06:06:09.258025  2394 caffe.cpp:275] Batch 41, loss = 0.0596849\n",
      "I0428 06:06:09.297456  2394 caffe.cpp:275] Batch 42, accuracy = 0.98\n",
      "I0428 06:06:09.297484  2394 caffe.cpp:275] Batch 42, loss = 0.0456337\n",
      "I0428 06:06:09.338023  2394 caffe.cpp:275] Batch 43, accuracy = 1\n",
      "I0428 06:06:09.338047  2394 caffe.cpp:275] Batch 43, loss = 0.00865824\n",
      "I0428 06:06:09.377836  2394 caffe.cpp:275] Batch 44, accuracy = 0.99\n",
      "I0428 06:06:09.377866  2394 caffe.cpp:275] Batch 44, loss = 0.0123933\n",
      "I0428 06:06:09.423136  2394 caffe.cpp:275] Batch 45, accuracy = 0.99\n",
      "I0428 06:06:09.423177  2394 caffe.cpp:275] Batch 45, loss = 0.0266847\n",
      "I0428 06:06:09.466338  2394 caffe.cpp:275] Batch 46, accuracy = 0.99\n",
      "I0428 06:06:09.466358  2394 caffe.cpp:275] Batch 46, loss = 0.0154558\n",
      "I0428 06:06:09.504817  2394 caffe.cpp:275] Batch 47, accuracy = 0.99\n",
      "I0428 06:06:09.504832  2394 caffe.cpp:275] Batch 47, loss = 0.0517456\n",
      "I0428 06:06:09.543527  2394 caffe.cpp:275] Batch 48, accuracy = 0.97\n",
      "I0428 06:06:09.543546  2394 caffe.cpp:275] Batch 48, loss = 0.0798449\n",
      "I0428 06:06:09.582780  2394 caffe.cpp:275] Batch 49, accuracy = 1\n",
      "I0428 06:06:09.582795  2394 caffe.cpp:275] Batch 49, loss = 0.00382511\n",
      "I0428 06:06:09.621181  2394 caffe.cpp:275] Batch 50, accuracy = 1\n",
      "I0428 06:06:09.621196  2394 caffe.cpp:275] Batch 50, loss = 0.000296271\n",
      "I0428 06:06:09.659637  2394 caffe.cpp:275] Batch 51, accuracy = 1\n",
      "I0428 06:06:09.659652  2394 caffe.cpp:275] Batch 51, loss = 0.0031314\n",
      "I0428 06:06:09.698431  2394 caffe.cpp:275] Batch 52, accuracy = 1\n",
      "I0428 06:06:09.698447  2394 caffe.cpp:275] Batch 52, loss = 0.00767586\n",
      "I0428 06:06:09.737767  2394 caffe.cpp:275] Batch 53, accuracy = 1\n",
      "I0428 06:06:09.737782  2394 caffe.cpp:275] Batch 53, loss = 0.000780314\n",
      "I0428 06:06:09.776135  2394 caffe.cpp:275] Batch 54, accuracy = 1\n",
      "I0428 06:06:09.776151  2394 caffe.cpp:275] Batch 54, loss = 0.00684002\n",
      "I0428 06:06:09.814589  2394 caffe.cpp:275] Batch 55, accuracy = 1\n",
      "I0428 06:06:09.814604  2394 caffe.cpp:275] Batch 55, loss = 0.000136379\n",
      "I0428 06:06:09.853047  2394 caffe.cpp:275] Batch 56, accuracy = 1\n",
      "I0428 06:06:09.853076  2394 caffe.cpp:275] Batch 56, loss = 0.00886536\n",
      "I0428 06:06:09.891556  2394 caffe.cpp:275] Batch 57, accuracy = 0.98\n",
      "I0428 06:06:09.891571  2394 caffe.cpp:275] Batch 57, loss = 0.0204316\n",
      "I0428 06:06:09.929993  2394 caffe.cpp:275] Batch 58, accuracy = 1\n",
      "I0428 06:06:09.930022  2394 caffe.cpp:275] Batch 58, loss = 0.00235976\n",
      "I0428 06:06:09.969187  2394 caffe.cpp:275] Batch 59, accuracy = 0.97\n",
      "I0428 06:06:09.969203  2394 caffe.cpp:275] Batch 59, loss = 0.0738728\n",
      "I0428 06:06:10.008568  2394 caffe.cpp:275] Batch 60, accuracy = 1\n",
      "I0428 06:06:10.008584  2394 caffe.cpp:275] Batch 60, loss = 0.0047878\n",
      "I0428 06:06:10.048326  2394 caffe.cpp:275] Batch 61, accuracy = 1\n",
      "I0428 06:06:10.048343  2394 caffe.cpp:275] Batch 61, loss = 0.000941834\n",
      "I0428 06:06:10.087735  2394 caffe.cpp:275] Batch 62, accuracy = 1\n",
      "I0428 06:06:10.087764  2394 caffe.cpp:275] Batch 62, loss = 9.3863e-06\n",
      "I0428 06:06:10.127173  2394 caffe.cpp:275] Batch 63, accuracy = 1\n",
      "I0428 06:06:10.127203  2394 caffe.cpp:275] Batch 63, loss = 0.000128427\n",
      "I0428 06:06:10.166589  2394 caffe.cpp:275] Batch 64, accuracy = 1\n",
      "I0428 06:06:10.166607  2394 caffe.cpp:275] Batch 64, loss = 0.000231012\n",
      "I0428 06:06:10.206354  2394 caffe.cpp:275] Batch 65, accuracy = 0.96\n",
      "I0428 06:06:10.206372  2394 caffe.cpp:275] Batch 65, loss = 0.136174\n",
      "I0428 06:06:10.245668  2394 caffe.cpp:275] Batch 66, accuracy = 0.98\n",
      "I0428 06:06:10.245683  2394 caffe.cpp:275] Batch 66, loss = 0.0640969\n",
      "I0428 06:06:10.285010  2394 caffe.cpp:275] Batch 67, accuracy = 0.99\n",
      "I0428 06:06:10.285038  2394 caffe.cpp:275] Batch 67, loss = 0.0216977\n",
      "I0428 06:06:10.325223  2394 caffe.cpp:275] Batch 68, accuracy = 1\n",
      "I0428 06:06:10.325240  2394 caffe.cpp:275] Batch 68, loss = 0.00214106\n",
      "I0428 06:06:10.364511  2394 caffe.cpp:275] Batch 69, accuracy = 1\n",
      "I0428 06:06:10.364526  2394 caffe.cpp:275] Batch 69, loss = 0.000247845\n",
      "I0428 06:06:10.403854  2394 caffe.cpp:275] Batch 70, accuracy = 1\n",
      "I0428 06:06:10.403870  2394 caffe.cpp:275] Batch 70, loss = 0.000118912\n",
      "I0428 06:06:10.443162  2394 caffe.cpp:275] Batch 71, accuracy = 1\n",
      "I0428 06:06:10.443178  2394 caffe.cpp:275] Batch 71, loss = 0.000528041\n",
      "I0428 06:06:10.482470  2394 caffe.cpp:275] Batch 72, accuracy = 1\n",
      "I0428 06:06:10.482486  2394 caffe.cpp:275] Batch 72, loss = 0.00409706\n",
      "I0428 06:06:10.521801  2394 caffe.cpp:275] Batch 73, accuracy = 1\n",
      "I0428 06:06:10.521817  2394 caffe.cpp:275] Batch 73, loss = 3.81317e-05\n",
      "I0428 06:06:10.561575  2394 caffe.cpp:275] Batch 74, accuracy = 1\n",
      "I0428 06:06:10.561591  2394 caffe.cpp:275] Batch 74, loss = 0.00272738\n",
      "I0428 06:06:10.600924  2394 caffe.cpp:275] Batch 75, accuracy = 1\n",
      "I0428 06:06:10.600939  2394 caffe.cpp:275] Batch 75, loss = 0.00195222\n",
      "I0428 06:06:10.640244  2394 caffe.cpp:275] Batch 76, accuracy = 1\n",
      "I0428 06:06:10.640260  2394 caffe.cpp:275] Batch 76, loss = 0.000177744\n",
      "I0428 06:06:10.679703  2394 caffe.cpp:275] Batch 77, accuracy = 1\n",
      "I0428 06:06:10.679718  2394 caffe.cpp:275] Batch 77, loss = 0.000271953\n",
      "I0428 06:06:10.719174  2394 caffe.cpp:275] Batch 78, accuracy = 1\n",
      "I0428 06:06:10.719192  2394 caffe.cpp:275] Batch 78, loss = 0.00898034\n",
      "I0428 06:06:10.758643  2394 caffe.cpp:275] Batch 79, accuracy = 1\n",
      "I0428 06:06:10.758659  2394 caffe.cpp:275] Batch 79, loss = 0.00225186\n",
      "I0428 06:06:10.798037  2394 caffe.cpp:275] Batch 80, accuracy = 0.99\n",
      "I0428 06:06:10.798051  2394 caffe.cpp:275] Batch 80, loss = 0.0414357\n",
      "I0428 06:06:10.837412  2394 caffe.cpp:275] Batch 81, accuracy = 1\n",
      "I0428 06:06:10.837430  2394 caffe.cpp:275] Batch 81, loss = 0.00146957\n",
      "I0428 06:06:10.877616  2394 caffe.cpp:275] Batch 82, accuracy = 1\n",
      "I0428 06:06:10.877632  2394 caffe.cpp:275] Batch 82, loss = 0.00397586\n",
      "I0428 06:06:10.917542  2394 caffe.cpp:275] Batch 83, accuracy = 1\n",
      "I0428 06:06:10.917563  2394 caffe.cpp:275] Batch 83, loss = 0.00772217\n",
      "I0428 06:06:10.967082  2394 caffe.cpp:275] Batch 84, accuracy = 0.99\n",
      "I0428 06:06:10.967116  2394 caffe.cpp:275] Batch 84, loss = 0.0197697\n",
      "I0428 06:06:11.006059  2394 caffe.cpp:275] Batch 85, accuracy = 0.99\n",
      "I0428 06:06:11.006075  2394 caffe.cpp:275] Batch 85, loss = 0.0208383\n",
      "I0428 06:06:11.044534  2394 caffe.cpp:275] Batch 86, accuracy = 1\n",
      "I0428 06:06:11.044562  2394 caffe.cpp:275] Batch 86, loss = 0.000659456\n",
      "I0428 06:06:11.083125  2394 caffe.cpp:275] Batch 87, accuracy = 1\n",
      "I0428 06:06:11.083154  2394 caffe.cpp:275] Batch 87, loss = 1.14733e-05\n",
      "I0428 06:06:11.122265  2394 caffe.cpp:275] Batch 88, accuracy = 1\n",
      "I0428 06:06:11.122280  2394 caffe.cpp:275] Batch 88, loss = 1.3558e-05\n",
      "I0428 06:06:11.161733  2394 caffe.cpp:275] Batch 89, accuracy = 1\n",
      "I0428 06:06:11.161749  2394 caffe.cpp:275] Batch 89, loss = 3.74321e-05\n",
      "I0428 06:06:11.201208  2394 caffe.cpp:275] Batch 90, accuracy = 0.98\n",
      "I0428 06:06:11.201225  2394 caffe.cpp:275] Batch 90, loss = 0.0447415\n",
      "I0428 06:06:11.239712  2394 caffe.cpp:275] Batch 91, accuracy = 1\n",
      "I0428 06:06:11.239742  2394 caffe.cpp:275] Batch 91, loss = 6.40143e-05\n",
      "I0428 06:06:11.278276  2394 caffe.cpp:275] Batch 92, accuracy = 1\n",
      "I0428 06:06:11.278303  2394 caffe.cpp:275] Batch 92, loss = 0.00131172\n",
      "I0428 06:06:11.317172  2394 caffe.cpp:275] Batch 93, accuracy = 1\n",
      "I0428 06:06:11.317186  2394 caffe.cpp:275] Batch 93, loss = 0.000712099\n",
      "I0428 06:06:11.357442  2394 caffe.cpp:275] Batch 94, accuracy = 1\n",
      "I0428 06:06:11.357465  2394 caffe.cpp:275] Batch 94, loss = 0.000460868\n",
      "I0428 06:06:11.397270  2394 caffe.cpp:275] Batch 95, accuracy = 1\n",
      "I0428 06:06:11.397286  2394 caffe.cpp:275] Batch 95, loss = 0.00202469\n",
      "I0428 06:06:11.436767  2394 caffe.cpp:275] Batch 96, accuracy = 0.97\n",
      "I0428 06:06:11.436784  2394 caffe.cpp:275] Batch 96, loss = 0.0709401\n",
      "I0428 06:06:11.476235  2394 caffe.cpp:275] Batch 97, accuracy = 0.98\n",
      "I0428 06:06:11.476263  2394 caffe.cpp:275] Batch 97, loss = 0.0885168\n",
      "I0428 06:06:11.515554  2394 caffe.cpp:275] Batch 98, accuracy = 1\n",
      "I0428 06:06:11.515569  2394 caffe.cpp:275] Batch 98, loss = 0.00598084\n",
      "I0428 06:06:11.554971  2394 caffe.cpp:275] Batch 99, accuracy = 1\n",
      "I0428 06:06:11.554999  2394 caffe.cpp:275] Batch 99, loss = 0.00428552\n",
      "I0428 06:06:11.555008  2394 caffe.cpp:280] Loss: 0.0278391\n",
      "I0428 06:06:11.555025  2394 caffe.cpp:292] accuracy = 0.9911\n",
      "I0428 06:06:11.555039  2394 caffe.cpp:292] loss = 0.0278391 (* 1 = 0.0278391 loss)\n"
     ]
    }
   ],
   "source": [
    "!./caffe/build/tools/caffe test --model=./lenet/lenet_train_test.prototxt --weights=./lenet/lenet_iter_10000.caffemodel --iterations=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "위의 코드의 수행 결과 가장 아래쪽에 100개의 minibatch상에서의 평균 성능이 나오며, 정상적으로 본 tutorial을 수행한다면 99.1%의 accuracy를 보실 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "description": "Define, train, and test the classic LeNet with the Python interface.",
  "example_name": "Learning LeNet",
  "include_in_docs": true,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "priority": 2
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
