{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Train  and test LeNet using caffe\n",
    "Jupyter notebook을 사용하여 Linux Shell 명령어로 caffe를 사용하는 방법에 대해서 배웁니다.\n",
    "\n",
    "본 예제는 크게 두 Part로 나누어져 있습니다.\n",
    "1. Caffe 명령어를 통해 모델을 학습하는 방법\n",
    "2. 학습된 모델의 test data에 대한 Classification 성능을 측정하는 방법\n",
    "\n",
    "### 제공해드린 Docker 이미지 상에 이미 Caffe가 ./caffe에 설치되어 있으며, 이를 기준으로 본 tutorial이 작성되어 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Basic Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Jupyter Notebook에서 Linux Shell 명령어를 사용하기 위해서는 명령앞에 !를 붙여서 실행하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/dlworkshop/2_caffe_intro\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00-classification.ipynb        02-fine-tuning.ipynb  caffe\r\n",
      "01-train-and-test-lenet.ipynb  02-test-lenet.ipynb   lenet\r\n"
     ]
    }
   ],
   "source": [
    "!ls ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training LeNet 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=./lenet/lenet5.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. Convert MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본 예제에서 사용할 데이터셋은 MNIST로 0~9까지의 숫자 이미지와 이미지에 해당하는 Label을 포함합니다.\n",
    "MNIST 데이터셋에 대해 자세한 내용은 http://yann.lecun.com/exdb/mnist 를 참고해 주세요.\n",
    "\n",
    "Raw MNIST dataset은 제공해드린 docker 이미지상에 이미 다운로드 되어 있으며, 해당 위치는 ./caffe/data/mnist 입니다.\n",
    "하지만 Raw MNIST dataset의 형식은 caffe에서 지원되지 않기에 caffe에서 가장 많이 쓰는 LMDB형식으로 변환되어야 합니다.\n",
    "변환에 필요한 코드는 이미 작성되어 있으며 ./lenet/create_mnist.sh 을 실행하여 변환합니다.\n",
    "\n",
    "스크립트 실행시 발생하는 libdc1394 error는 docker의 driver상의 오류로 무시하셔도 됩니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating lmdb...\n",
      "libdc1394 error: Failed to initialize libdc1394\n",
      "libdc1394 error: Failed to initialize libdc1394\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "!sh ./lenet/create_mnist.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "변환이 완료된 후, !ls ./lenet 명령어를 통해 mnist_test_lmdb와 mnist_train_lmdb가 정상적으로 생성되었음을 확인하실 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_mnist.sh\t\t      lenet_iter_5000.caffemodel   mnist.pyc\r\n",
      "lenet.prototxt\t\t      lenet_iter_5000.solverstate  mnist_test_lmdb\r\n",
      "lenet5.png\t\t      lenet_solver.prototxt\t   mnist_train_lmdb\r\n",
      "lenet_iter_10000.caffemodel   lenet_train_test.prototxt\r\n",
      "lenet_iter_10000.solverstate  mnist.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./lenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2. Define model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet5의 모델은 ./lenet/lenet_train_test.prototxt에 정의되어 있습니다. \n",
    "\n",
    "cat 명령어는 Linux에서 text파일을 모니터에 출력해주는 함수로 prototxt 파일을 읽어올 수 있습니다.\n",
    "http://caffe.berkeleyvision.org/tutorial/layers.html 를 참고하여 각 layer가 요구하는 parameter를 참고하세요.\n",
    "\n",
    "prototxt는 jupyter notebook 상에서 해당 파일 ./lenet/lenet_train_test.prototxt 을 클릭하는 것으로 수정하실 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"LeNet\"\r\n",
      "layer {\r\n",
      "  name: \"mnist\"\r\n",
      "  type: \"Data\"\r\n",
      "  top: \"data\"\r\n",
      "  top: \"label\"\r\n",
      "  include {\r\n",
      "    phase: TRAIN\r\n",
      "  }\r\n",
      "  transform_param {\r\n",
      "    scale: 0.00390625\r\n",
      "  }\r\n",
      "  data_param {\r\n",
      "    source: \"./lenet/mnist_train_lmdb\"\r\n",
      "    batch_size: 64\r\n",
      "    backend: LMDB\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"mnist\"\r\n",
      "  type: \"Data\"\r\n",
      "  top: \"data\"\r\n",
      "  top: \"label\"\r\n",
      "  include {\r\n",
      "    phase: TEST\r\n",
      "  }\r\n",
      "  transform_param {\r\n",
      "    scale: 0.00390625\r\n",
      "  }\r\n",
      "  data_param {\r\n",
      "    source: \"./lenet/mnist_test_lmdb\"\r\n",
      "    batch_size: 100\r\n",
      "    backend: LMDB\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"conv1\"\r\n",
      "  type: \"Convolution\"\r\n",
      "  bottom: \"data\"\r\n",
      "  top: \"conv1\"\r\n",
      "  param {\r\n",
      "    lr_mult: 1\r\n",
      "  }\r\n",
      "  param {\r\n",
      "    lr_mult: 2\r\n",
      "  }\r\n",
      "  convolution_param {\r\n",
      "    num_output: 20\r\n",
      "    kernel_size: 5\r\n",
      "    stride: 1\r\n",
      "    weight_filler {\r\n",
      "      type: \"xavier\"\r\n",
      "    }\r\n",
      "    bias_filler {\r\n",
      "      type: \"constant\"\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"pool1\"\r\n",
      "  type: \"Pooling\"\r\n",
      "  bottom: \"conv1\"\r\n",
      "  top: \"pool1\"\r\n",
      "  pooling_param {\r\n",
      "    pool: MAX\r\n",
      "    kernel_size: 2\r\n",
      "    stride: 2\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"conv2\"\r\n",
      "  type: \"Convolution\"\r\n",
      "  bottom: \"pool1\"\r\n",
      "  top: \"conv2\"\r\n",
      "  param {\r\n",
      "    lr_mult: 1\r\n",
      "  }\r\n",
      "  param {\r\n",
      "    lr_mult: 2\r\n",
      "  }\r\n",
      "  convolution_param {\r\n",
      "    num_output: 50\r\n",
      "    kernel_size: 5\r\n",
      "    stride: 1\r\n",
      "    weight_filler {\r\n",
      "      type: \"xavier\"\r\n",
      "    }\r\n",
      "    bias_filler {\r\n",
      "      type: \"constant\"\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"pool2\"\r\n",
      "  type: \"Pooling\"\r\n",
      "  bottom: \"conv2\"\r\n",
      "  top: \"pool2\"\r\n",
      "  pooling_param {\r\n",
      "    pool: MAX\r\n",
      "    kernel_size: 2\r\n",
      "    stride: 2\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"ip1\"\r\n",
      "  type: \"InnerProduct\"\r\n",
      "  bottom: \"pool2\"\r\n",
      "  top: \"ip1\"\r\n",
      "  param {\r\n",
      "    lr_mult: 1\r\n",
      "  }\r\n",
      "  param {\r\n",
      "    lr_mult: 2\r\n",
      "  }\r\n",
      "  inner_product_param {\r\n",
      "    num_output: 500\r\n",
      "    weight_filler {\r\n",
      "      type: \"xavier\"\r\n",
      "    }\r\n",
      "    bias_filler {\r\n",
      "      type: \"constant\"\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"relu1\"\r\n",
      "  type: \"ReLU\"\r\n",
      "  bottom: \"ip1\"\r\n",
      "  top: \"ip1\"\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"ip2\"\r\n",
      "  type: \"InnerProduct\"\r\n",
      "  bottom: \"ip1\"\r\n",
      "  top: \"ip2\"\r\n",
      "  param {\r\n",
      "    lr_mult: 1\r\n",
      "  }\r\n",
      "  param {\r\n",
      "    lr_mult: 2\r\n",
      "  }\r\n",
      "  inner_product_param {\r\n",
      "    num_output: 10\r\n",
      "    weight_filler {\r\n",
      "      type: \"xavier\"\r\n",
      "    }\r\n",
      "    bias_filler {\r\n",
      "      type: \"constant\"\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"accuracy\"\r\n",
      "  type: \"Accuracy\"\r\n",
      "  bottom: \"ip2\"\r\n",
      "  bottom: \"label\"\r\n",
      "  top: \"accuracy\"\r\n",
      "  include {\r\n",
      "    phase: TEST\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"loss\"\r\n",
      "  type: \"SoftmaxWithLoss\"\r\n",
      "  bottom: \"ip2\"\r\n",
      "  bottom: \"label\"\r\n",
      "  top: \"loss\"\r\n",
      "}\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./lenet/lenet_train_test.prototxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-3. Define solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network를 학습하기 위해서 Solver를 prototxt형식으로 정의합니다. \n",
    "\n",
    "Solver hyperparameters\n",
    "- Solver: SGD (Stochastic gradient descent)\n",
    "- Base Learning Rate: 0.01 (Staring learning rate)\n",
    "- Momemtum: 0.9 (SGD parameter)\n",
    "- weight_decay: 0.005\n",
    "- lr_policy: inv \n",
    "- Gamma: 0.0001\n",
    "- Power: 0.75\n",
    "- solver_mode: CPU  \n",
    "\n",
    "Display param\n",
    "- display: 100 (training 100 batch 마다 Loss 출력)\n",
    "- test_iter: 100 (validation data시 100개의 batch를 테스트)\n",
    "- test_interval: 500 (training 500 batch 마다 validation data test)\n",
    "- max_iter: 10000 (10000 batch training 도달시 종료)\n",
    "\n",
    "Snapshot param\n",
    "- snapshot: 5000 (5000 batch training 마다 모델 저장)\n",
    "- snapshot_prefix: ./lenet/lenet (모델 저장 위치와 이름에 대한 prefix)\n",
    "\n",
    "위의 내용을 caffe의 prototxt 형식으로 나타내면 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# The train/test net protocol buffer definition\r\n",
      "net: \"./lenet/lenet_train_test.prototxt\"\r\n",
      "# test_iter specifies how many forward passes the test should carry out.\r\n",
      "# In the case of MNIST, we have test batch size 100 and 100 test iterations,\r\n",
      "# covering the full 10,000 testing images.\r\n",
      "test_iter: 100\r\n",
      "# Carry out testing every 500 training iterations.\r\n",
      "test_interval: 500\r\n",
      "# The base learning rate, momentum and the weight decay of the network.\r\n",
      "base_lr: 0.01\r\n",
      "momentum: 0.9\r\n",
      "weight_decay: 0.0005\r\n",
      "# The learning rate policy\r\n",
      "lr_policy: \"inv\"\r\n",
      "gamma: 0.0001\r\n",
      "power: 0.75\r\n",
      "# Display every 100 iterations\r\n",
      "display: 100\r\n",
      "# The maximum number of iterations\r\n",
      "max_iter: 10000\r\n",
      "# snapshot intermediate results\r\n",
      "snapshot: 5000\r\n",
      "snapshot_prefix: \"./lenet/lenet\"\r\n",
      "# solver mode: CPU or GPU\r\n",
      "solver_mode: CPU\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./lenet/lenet_solver.prototxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-4. Train LeNet5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "빌드된 Caffe의 실행파일은 ./caffe/build/tools/caffe에 존재합니다.\n",
    "\n",
    "caffe 실행파일의 사용법(commands & arguments)을 보기 위해서 아래의 명령어를 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libdc1394 error: Failed to initialize libdc1394\r\n",
      "caffe: command line brew\r\n",
      "usage: caffe <command> <args>\r\n",
      "\r\n",
      "commands:\r\n",
      "  train           train or finetune a model\r\n",
      "  test            score a model\r\n",
      "  device_query    show GPU diagnostic information\r\n",
      "  time            benchmark model execution time\r\n",
      "\r\n",
      "  Flags from /root/caffe/tools/caffe.cpp:\r\n",
      "    -gpu (Optional; run in GPU mode on given device IDs separated by ','.Use\r\n",
      "      '-gpu all' to run on all available GPUs. The effective training batch\r\n",
      "      size is multiplied by the number of devices.) type: string default: \"\"\r\n",
      "    -iterations (The number of iterations to run.) type: int32 default: 50\r\n",
      "    -model (The model definition protocol buffer text file.) type: string\r\n",
      "      default: \"\"\r\n",
      "    -sighup_effect (Optional; action to take when a SIGHUP signal is received:\r\n",
      "      snapshot, stop or none.) type: string default: \"snapshot\"\r\n",
      "    -sigint_effect (Optional; action to take when a SIGINT signal is received:\r\n",
      "      snapshot, stop or none.) type: string default: \"stop\"\r\n",
      "    -snapshot (Optional; the snapshot solver state to resume training.)\r\n",
      "      type: string default: \"\"\r\n",
      "    -solver (The solver definition protocol buffer text file.) type: string\r\n",
      "      default: \"\"\r\n",
      "    -weights (Optional; the pretrained weights to initialize finetuning,\r\n",
      "      separated by ','. Cannot be set simultaneously with snapshot.)\r\n",
      "      type: string default: \"\"\r\n"
     ]
    }
   ],
   "source": [
    "!./caffe/build/tools/caffe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 간단하게는 ./caffe/build/tools/caffe train --solver=/path/to/solver/file.prototxt로 정의된 모델을 정의된 solver로 학습시킬 수 있습니다.\n",
    "\n",
    "Desktop에서 모델을 전부 학습하기 위해서는 약 10분 이상이 소요됩니다. (노트북의 경우 20분 이상이 소요될 수 있습니다.)\n",
    "\n",
    "#### 중단을 원하시면 Jupyter notebook 상단의 Kernel 탭 클릭 후 Interrupt를 클릭해주세요.\n",
    "#### Shell 상에서 중단 (Jupyter notebook 상의 kernel interrupt)이 될 경우 중단 시점에서의 snapshot이 자동저장됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter notebook 상의 ln [\\*] 표시는 현재 처리중임을 의미하며, 완료가 되면 \\* 기호는 숫자로 변경됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libdc1394 error: Failed to initialize libdc1394\n",
      "I0427 17:32:42.151243  1495 caffe.cpp:178] Use CPU.\n",
      "I0427 17:32:42.151581  1495 solver.cpp:48] Initializing solver from parameters: \n",
      "test_iter: 100\n",
      "test_interval: 500\n",
      "base_lr: 0.01\n",
      "display: 100\n",
      "max_iter: 10000\n",
      "lr_policy: \"inv\"\n",
      "gamma: 0.0001\n",
      "power: 0.75\n",
      "momentum: 0.9\n",
      "weight_decay: 0.0005\n",
      "snapshot: 5000\n",
      "snapshot_prefix: \"./lenet/lenet\"\n",
      "solver_mode: CPU\n",
      "net: \"./lenet/lenet_train_test.prototxt\"\n",
      "I0427 17:32:42.151696  1495 solver.cpp:91] Creating training net from net file: ./lenet/lenet_train_test.prototxt\n",
      "I0427 17:32:42.151908  1495 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist\n",
      "I0427 17:32:42.151927  1495 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy\n",
      "I0427 17:32:42.151988  1495 net.cpp:49] Initializing net from parameters: \n",
      "name: \"LeNet\"\n",
      "state {\n",
      "  phase: TRAIN\n",
      "}\n",
      "layer {\n",
      "  name: \"mnist\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TRAIN\n",
      "  }\n",
      "  transform_param {\n",
      "    scale: 0.00390625\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"./lenet/mnist_train_lmdb\"\n",
      "    batch_size: 64\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 20\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 50\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"ip1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"ip1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 500\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"ip2\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 10\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"ip2\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I0427 17:32:42.152330  1495 layer_factory.hpp:77] Creating layer mnist\n",
      "I0427 17:32:42.152699  1495 net.cpp:91] Creating Layer mnist\n",
      "I0427 17:32:42.152725  1495 net.cpp:399] mnist -> data\n",
      "I0427 17:32:42.152770  1495 net.cpp:399] mnist -> label\n",
      "I0427 17:32:42.152859  1496 db_lmdb.cpp:38] Opened lmdb ./lenet/mnist_train_lmdb\n",
      "I0427 17:32:42.152947  1495 data_layer.cpp:41] output data size: 64,1,28,28\n",
      "I0427 17:32:42.153340  1495 net.cpp:141] Setting up mnist\n",
      "I0427 17:32:42.153358  1495 net.cpp:148] Top shape: 64 1 28 28 (50176)\n",
      "I0427 17:32:42.153367  1495 net.cpp:148] Top shape: 64 (64)\n",
      "I0427 17:32:42.153374  1495 net.cpp:156] Memory required for data: 200960\n",
      "I0427 17:32:42.153385  1495 layer_factory.hpp:77] Creating layer conv1\n",
      "I0427 17:32:42.153403  1495 net.cpp:91] Creating Layer conv1\n",
      "I0427 17:32:42.153411  1495 net.cpp:425] conv1 <- data\n",
      "I0427 17:32:42.153424  1495 net.cpp:399] conv1 -> conv1\n",
      "I0427 17:32:42.153472  1495 net.cpp:141] Setting up conv1\n",
      "I0427 17:32:42.153484  1495 net.cpp:148] Top shape: 64 20 24 24 (737280)\n",
      "I0427 17:32:42.153492  1495 net.cpp:156] Memory required for data: 3150080\n",
      "I0427 17:32:42.153509  1495 layer_factory.hpp:77] Creating layer pool1\n",
      "I0427 17:32:42.153520  1495 net.cpp:91] Creating Layer pool1\n",
      "I0427 17:32:42.153528  1495 net.cpp:425] pool1 <- conv1\n",
      "I0427 17:32:42.153547  1495 net.cpp:399] pool1 -> pool1\n",
      "I0427 17:32:42.153573  1495 net.cpp:141] Setting up pool1\n",
      "I0427 17:32:42.153597  1495 net.cpp:148] Top shape: 64 20 12 12 (184320)\n",
      "I0427 17:32:42.153614  1495 net.cpp:156] Memory required for data: 3887360\n",
      "I0427 17:32:42.153620  1495 layer_factory.hpp:77] Creating layer conv2\n",
      "I0427 17:32:42.153635  1495 net.cpp:91] Creating Layer conv2\n",
      "I0427 17:32:42.153642  1495 net.cpp:425] conv2 <- pool1\n",
      "I0427 17:32:42.153652  1495 net.cpp:399] conv2 -> conv2\n",
      "I0427 17:32:42.153823  1495 net.cpp:141] Setting up conv2\n",
      "I0427 17:32:42.153836  1495 net.cpp:148] Top shape: 64 50 8 8 (204800)\n",
      "I0427 17:32:42.153844  1495 net.cpp:156] Memory required for data: 4706560\n",
      "I0427 17:32:42.153856  1495 layer_factory.hpp:77] Creating layer pool2\n",
      "I0427 17:32:42.153866  1495 net.cpp:91] Creating Layer pool2\n",
      "I0427 17:32:42.153873  1495 net.cpp:425] pool2 <- conv2\n",
      "I0427 17:32:42.153882  1495 net.cpp:399] pool2 -> pool2\n",
      "I0427 17:32:42.153894  1495 net.cpp:141] Setting up pool2\n",
      "I0427 17:32:42.153903  1495 net.cpp:148] Top shape: 64 50 4 4 (51200)\n",
      "I0427 17:32:42.153910  1495 net.cpp:156] Memory required for data: 4911360\n",
      "I0427 17:32:42.153924  1495 layer_factory.hpp:77] Creating layer ip1\n",
      "I0427 17:32:42.153934  1495 net.cpp:91] Creating Layer ip1\n",
      "I0427 17:32:42.153942  1495 net.cpp:425] ip1 <- pool2\n",
      "I0427 17:32:42.153951  1495 net.cpp:399] ip1 -> ip1\n",
      "I0427 17:32:42.156267  1495 net.cpp:141] Setting up ip1\n",
      "I0427 17:32:42.156283  1495 net.cpp:148] Top shape: 64 500 (32000)\n",
      "I0427 17:32:42.156303  1495 net.cpp:156] Memory required for data: 5039360\n",
      "I0427 17:32:42.156314  1495 layer_factory.hpp:77] Creating layer relu1\n",
      "I0427 17:32:42.156324  1495 net.cpp:91] Creating Layer relu1\n",
      "I0427 17:32:42.156332  1495 net.cpp:425] relu1 <- ip1\n",
      "I0427 17:32:42.156338  1495 net.cpp:386] relu1 -> ip1 (in-place)\n",
      "I0427 17:32:42.156348  1495 net.cpp:141] Setting up relu1\n",
      "I0427 17:32:42.156357  1495 net.cpp:148] Top shape: 64 500 (32000)\n",
      "I0427 17:32:42.156363  1495 net.cpp:156] Memory required for data: 5167360\n",
      "I0427 17:32:42.156369  1495 layer_factory.hpp:77] Creating layer ip2\n",
      "I0427 17:32:42.156378  1495 net.cpp:91] Creating Layer ip2\n",
      "I0427 17:32:42.156384  1495 net.cpp:425] ip2 <- ip1\n",
      "I0427 17:32:42.156394  1495 net.cpp:399] ip2 -> ip2\n",
      "I0427 17:32:42.156440  1495 net.cpp:141] Setting up ip2\n",
      "I0427 17:32:42.156448  1495 net.cpp:148] Top shape: 64 10 (640)\n",
      "I0427 17:32:42.156455  1495 net.cpp:156] Memory required for data: 5169920\n",
      "I0427 17:32:42.156463  1495 layer_factory.hpp:77] Creating layer loss\n",
      "I0427 17:32:42.156477  1495 net.cpp:91] Creating Layer loss\n",
      "I0427 17:32:42.156484  1495 net.cpp:425] loss <- ip2\n",
      "I0427 17:32:42.156491  1495 net.cpp:425] loss <- label\n",
      "I0427 17:32:42.156500  1495 net.cpp:399] loss -> loss\n",
      "I0427 17:32:42.156513  1495 layer_factory.hpp:77] Creating layer loss\n",
      "I0427 17:32:42.156530  1495 net.cpp:141] Setting up loss\n",
      "I0427 17:32:42.156539  1495 net.cpp:148] Top shape: (1)\n",
      "I0427 17:32:42.156545  1495 net.cpp:151]     with loss weight 1\n",
      "I0427 17:32:42.156559  1495 net.cpp:156] Memory required for data: 5169924\n",
      "I0427 17:32:42.156566  1495 net.cpp:217] loss needs backward computation.\n",
      "I0427 17:32:42.156574  1495 net.cpp:217] ip2 needs backward computation.\n",
      "I0427 17:32:42.156580  1495 net.cpp:217] relu1 needs backward computation.\n",
      "I0427 17:32:42.156586  1495 net.cpp:217] ip1 needs backward computation.\n",
      "I0427 17:32:42.156594  1495 net.cpp:217] pool2 needs backward computation.\n",
      "I0427 17:32:42.156600  1495 net.cpp:217] conv2 needs backward computation.\n",
      "I0427 17:32:42.156607  1495 net.cpp:217] pool1 needs backward computation.\n",
      "I0427 17:32:42.156613  1495 net.cpp:217] conv1 needs backward computation.\n",
      "I0427 17:32:42.156620  1495 net.cpp:219] mnist does not need backward computation.\n",
      "I0427 17:32:42.156627  1495 net.cpp:261] This network produces output loss\n",
      "I0427 17:32:42.156638  1495 net.cpp:274] Network initialization done.\n",
      "I0427 17:32:42.156844  1495 solver.cpp:181] Creating test net (#0) specified by net file: ./lenet/lenet_train_test.prototxt\n",
      "I0427 17:32:42.156872  1495 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist\n",
      "I0427 17:32:42.156937  1495 net.cpp:49] Initializing net from parameters: \n",
      "name: \"LeNet\"\n",
      "state {\n",
      "  phase: TEST\n",
      "}\n",
      "layer {\n",
      "  name: \"mnist\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "  transform_param {\n",
      "    scale: 0.00390625\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"./lenet/mnist_test_lmdb\"\n",
      "    batch_size: 100\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 20\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 50\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"ip1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"ip1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 500\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"ip2\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 10\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"accuracy\"\n",
      "  type: \"Accuracy\"\n",
      "  bottom: \"ip2\"\n",
      "  bottom: \"label\"\n",
      "  top: \"accuracy\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"ip2\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I0427 17:32:42.157311  1495 layer_factory.hpp:77] Creating layer mnist\n",
      "I0427 17:32:42.157402  1495 net.cpp:91] Creating Layer mnist\n",
      "I0427 17:32:42.157415  1495 net.cpp:399] mnist -> data\n",
      "I0427 17:32:42.157428  1495 net.cpp:399] mnist -> label\n",
      "I0427 17:32:42.157490  1498 db_lmdb.cpp:38] Opened lmdb ./lenet/mnist_test_lmdb\n",
      "I0427 17:32:42.157533  1495 data_layer.cpp:41] output data size: 100,1,28,28\n",
      "I0427 17:32:42.158298  1495 net.cpp:141] Setting up mnist\n",
      "I0427 17:32:42.158313  1495 net.cpp:148] Top shape: 100 1 28 28 (78400)\n",
      "I0427 17:32:42.158323  1495 net.cpp:148] Top shape: 100 (100)\n",
      "I0427 17:32:42.158329  1495 net.cpp:156] Memory required for data: 314000\n",
      "I0427 17:32:42.158336  1495 layer_factory.hpp:77] Creating layer label_mnist_1_split\n",
      "I0427 17:32:42.158347  1495 net.cpp:91] Creating Layer label_mnist_1_split\n",
      "I0427 17:32:42.158355  1495 net.cpp:425] label_mnist_1_split <- label\n",
      "I0427 17:32:42.158367  1495 net.cpp:399] label_mnist_1_split -> label_mnist_1_split_0\n",
      "I0427 17:32:42.158380  1495 net.cpp:399] label_mnist_1_split -> label_mnist_1_split_1\n",
      "I0427 17:32:42.158390  1495 net.cpp:141] Setting up label_mnist_1_split\n",
      "I0427 17:32:42.158399  1495 net.cpp:148] Top shape: 100 (100)\n",
      "I0427 17:32:42.158407  1495 net.cpp:148] Top shape: 100 (100)\n",
      "I0427 17:32:42.158413  1495 net.cpp:156] Memory required for data: 314800\n",
      "I0427 17:32:42.158421  1495 layer_factory.hpp:77] Creating layer conv1\n",
      "I0427 17:32:42.158434  1495 net.cpp:91] Creating Layer conv1\n",
      "I0427 17:32:42.158443  1495 net.cpp:425] conv1 <- data\n",
      "I0427 17:32:42.158454  1495 net.cpp:399] conv1 -> conv1\n",
      "I0427 17:32:42.158484  1495 net.cpp:141] Setting up conv1\n",
      "I0427 17:32:42.158495  1495 net.cpp:148] Top shape: 100 20 24 24 (1152000)\n",
      "I0427 17:32:42.158515  1495 net.cpp:156] Memory required for data: 4922800\n",
      "I0427 17:32:42.158529  1495 layer_factory.hpp:77] Creating layer pool1\n",
      "I0427 17:32:42.158540  1495 net.cpp:91] Creating Layer pool1\n",
      "I0427 17:32:42.158547  1495 net.cpp:425] pool1 <- conv1\n",
      "I0427 17:32:42.158558  1495 net.cpp:399] pool1 -> pool1\n",
      "I0427 17:32:42.158583  1495 net.cpp:141] Setting up pool1\n",
      "I0427 17:32:42.158593  1495 net.cpp:148] Top shape: 100 20 12 12 (288000)\n",
      "I0427 17:32:42.158601  1495 net.cpp:156] Memory required for data: 6074800\n",
      "I0427 17:32:42.158607  1495 layer_factory.hpp:77] Creating layer conv2\n",
      "I0427 17:32:42.158619  1495 net.cpp:91] Creating Layer conv2\n",
      "I0427 17:32:42.158627  1495 net.cpp:425] conv2 <- pool1\n",
      "I0427 17:32:42.158639  1495 net.cpp:399] conv2 -> conv2\n",
      "I0427 17:32:42.158807  1495 net.cpp:141] Setting up conv2\n",
      "I0427 17:32:42.158821  1495 net.cpp:148] Top shape: 100 50 8 8 (320000)\n",
      "I0427 17:32:42.158828  1495 net.cpp:156] Memory required for data: 7354800\n",
      "I0427 17:32:42.158840  1495 layer_factory.hpp:77] Creating layer pool2\n",
      "I0427 17:32:42.158849  1495 net.cpp:91] Creating Layer pool2\n",
      "I0427 17:32:42.158857  1495 net.cpp:425] pool2 <- conv2\n",
      "I0427 17:32:42.158866  1495 net.cpp:399] pool2 -> pool2\n",
      "I0427 17:32:42.158877  1495 net.cpp:141] Setting up pool2\n",
      "I0427 17:32:42.158885  1495 net.cpp:148] Top shape: 100 50 4 4 (80000)\n",
      "I0427 17:32:42.158892  1495 net.cpp:156] Memory required for data: 7674800\n",
      "I0427 17:32:42.158900  1495 layer_factory.hpp:77] Creating layer ip1\n",
      "I0427 17:32:42.158911  1495 net.cpp:91] Creating Layer ip1\n",
      "I0427 17:32:42.158920  1495 net.cpp:425] ip1 <- pool2\n",
      "I0427 17:32:42.158929  1495 net.cpp:399] ip1 -> ip1\n",
      "I0427 17:32:42.161284  1495 net.cpp:141] Setting up ip1\n",
      "I0427 17:32:42.161299  1495 net.cpp:148] Top shape: 100 500 (50000)\n",
      "I0427 17:32:42.161306  1495 net.cpp:156] Memory required for data: 7874800\n",
      "I0427 17:32:42.161317  1495 layer_factory.hpp:77] Creating layer relu1\n",
      "I0427 17:32:42.161325  1495 net.cpp:91] Creating Layer relu1\n",
      "I0427 17:32:42.161332  1495 net.cpp:425] relu1 <- ip1\n",
      "I0427 17:32:42.161340  1495 net.cpp:386] relu1 -> ip1 (in-place)\n",
      "I0427 17:32:42.161350  1495 net.cpp:141] Setting up relu1\n",
      "I0427 17:32:42.161357  1495 net.cpp:148] Top shape: 100 500 (50000)\n",
      "I0427 17:32:42.161363  1495 net.cpp:156] Memory required for data: 8074800\n",
      "I0427 17:32:42.161370  1495 layer_factory.hpp:77] Creating layer ip2\n",
      "I0427 17:32:42.161381  1495 net.cpp:91] Creating Layer ip2\n",
      "I0427 17:32:42.161388  1495 net.cpp:425] ip2 <- ip1\n",
      "I0427 17:32:42.161398  1495 net.cpp:399] ip2 -> ip2\n",
      "I0427 17:32:42.161442  1495 net.cpp:141] Setting up ip2\n",
      "I0427 17:32:42.161450  1495 net.cpp:148] Top shape: 100 10 (1000)\n",
      "I0427 17:32:42.161458  1495 net.cpp:156] Memory required for data: 8078800\n",
      "I0427 17:32:42.161465  1495 layer_factory.hpp:77] Creating layer ip2_ip2_0_split\n",
      "I0427 17:32:42.161473  1495 net.cpp:91] Creating Layer ip2_ip2_0_split\n",
      "I0427 17:32:42.161480  1495 net.cpp:425] ip2_ip2_0_split <- ip2\n",
      "I0427 17:32:42.161490  1495 net.cpp:399] ip2_ip2_0_split -> ip2_ip2_0_split_0\n",
      "I0427 17:32:42.161499  1495 net.cpp:399] ip2_ip2_0_split -> ip2_ip2_0_split_1\n",
      "I0427 17:32:42.161509  1495 net.cpp:141] Setting up ip2_ip2_0_split\n",
      "I0427 17:32:42.161516  1495 net.cpp:148] Top shape: 100 10 (1000)\n",
      "I0427 17:32:42.161523  1495 net.cpp:148] Top shape: 100 10 (1000)\n",
      "I0427 17:32:42.161530  1495 net.cpp:156] Memory required for data: 8086800\n",
      "I0427 17:32:42.161542  1495 layer_factory.hpp:77] Creating layer accuracy\n",
      "I0427 17:32:42.161551  1495 net.cpp:91] Creating Layer accuracy\n",
      "I0427 17:32:42.161562  1495 net.cpp:425] accuracy <- ip2_ip2_0_split_0\n",
      "I0427 17:32:42.161583  1495 net.cpp:425] accuracy <- label_mnist_1_split_0\n",
      "I0427 17:32:42.161592  1495 net.cpp:399] accuracy -> accuracy\n",
      "I0427 17:32:42.161604  1495 net.cpp:141] Setting up accuracy\n",
      "I0427 17:32:42.161612  1495 net.cpp:148] Top shape: (1)\n",
      "I0427 17:32:42.161619  1495 net.cpp:156] Memory required for data: 8086804\n",
      "I0427 17:32:42.161625  1495 layer_factory.hpp:77] Creating layer loss\n",
      "I0427 17:32:42.161635  1495 net.cpp:91] Creating Layer loss\n",
      "I0427 17:32:42.161643  1495 net.cpp:425] loss <- ip2_ip2_0_split_1\n",
      "I0427 17:32:42.161651  1495 net.cpp:425] loss <- label_mnist_1_split_1\n",
      "I0427 17:32:42.161659  1495 net.cpp:399] loss -> loss\n",
      "I0427 17:32:42.161669  1495 layer_factory.hpp:77] Creating layer loss\n",
      "I0427 17:32:42.161685  1495 net.cpp:141] Setting up loss\n",
      "I0427 17:32:42.161695  1495 net.cpp:148] Top shape: (1)\n",
      "I0427 17:32:42.161712  1495 net.cpp:151]     with loss weight 1\n",
      "I0427 17:32:42.161722  1495 net.cpp:156] Memory required for data: 8086808\n",
      "I0427 17:32:42.161730  1495 net.cpp:217] loss needs backward computation.\n",
      "I0427 17:32:42.161736  1495 net.cpp:219] accuracy does not need backward computation.\n",
      "I0427 17:32:42.161744  1495 net.cpp:217] ip2_ip2_0_split needs backward computation.\n",
      "I0427 17:32:42.161751  1495 net.cpp:217] ip2 needs backward computation.\n",
      "I0427 17:32:42.161757  1495 net.cpp:217] relu1 needs backward computation.\n",
      "I0427 17:32:42.161763  1495 net.cpp:217] ip1 needs backward computation.\n",
      "I0427 17:32:42.161770  1495 net.cpp:217] pool2 needs backward computation.\n",
      "I0427 17:32:42.161777  1495 net.cpp:217] conv2 needs backward computation.\n",
      "I0427 17:32:42.161784  1495 net.cpp:217] pool1 needs backward computation.\n",
      "I0427 17:32:42.161792  1495 net.cpp:217] conv1 needs backward computation.\n",
      "I0427 17:32:42.161798  1495 net.cpp:219] label_mnist_1_split does not need backward computation.\n",
      "I0427 17:32:42.161805  1495 net.cpp:219] mnist does not need backward computation.\n",
      "I0427 17:32:42.161813  1495 net.cpp:261] This network produces output accuracy\n",
      "I0427 17:32:42.161819  1495 net.cpp:261] This network produces output loss\n",
      "I0427 17:32:42.161833  1495 net.cpp:274] Network initialization done.\n",
      "I0427 17:32:42.161867  1495 solver.cpp:60] Solver scaffolding done.\n",
      "I0427 17:32:42.161896  1495 caffe.cpp:219] Starting Optimization\n",
      "I0427 17:32:42.161905  1495 solver.cpp:279] Solving LeNet\n",
      "I0427 17:32:42.161911  1495 solver.cpp:280] Learning Rate Policy: inv\n",
      "I0427 17:32:42.162545  1495 solver.cpp:337] Iteration 0, Testing net (#0)\n",
      "I0427 17:32:46.353960  1495 solver.cpp:404]     Test net output #0: accuracy = 0.0482\n",
      "I0427 17:32:46.354019  1495 solver.cpp:404]     Test net output #1: loss = 2.30247 (* 1 = 2.30247 loss)\n",
      "I0427 17:32:46.423553  1495 solver.cpp:228] Iteration 0, loss = 2.29686\n",
      "I0427 17:32:46.423599  1495 solver.cpp:244]     Train net output #0: loss = 2.29686 (* 1 = 2.29686 loss)\n",
      "I0427 17:32:46.423616  1495 sgd_solver.cpp:106] Iteration 0, lr = 0.01\n",
      "I0427 17:32:53.087371  1495 solver.cpp:228] Iteration 100, loss = 0.181718\n",
      "I0427 17:32:53.087424  1495 solver.cpp:244]     Train net output #0: loss = 0.181718 (* 1 = 0.181718 loss)\n",
      "I0427 17:32:53.087435  1495 sgd_solver.cpp:106] Iteration 100, lr = 0.00992565\n",
      "I0427 17:32:59.633532  1495 solver.cpp:228] Iteration 200, loss = 0.158129\n",
      "I0427 17:32:59.633594  1495 solver.cpp:244]     Train net output #0: loss = 0.158129 (* 1 = 0.158129 loss)\n",
      "I0427 17:32:59.633605  1495 sgd_solver.cpp:106] Iteration 200, lr = 0.00985258\n",
      "I0427 17:33:06.134630  1495 solver.cpp:228] Iteration 300, loss = 0.18733\n",
      "I0427 17:33:06.134661  1495 solver.cpp:244]     Train net output #0: loss = 0.18733 (* 1 = 0.18733 loss)\n",
      "I0427 17:33:06.134671  1495 sgd_solver.cpp:106] Iteration 300, lr = 0.00978075\n",
      "I0427 17:33:12.639514  1495 solver.cpp:228] Iteration 400, loss = 0.0735634\n",
      "I0427 17:33:12.639713  1495 solver.cpp:244]     Train net output #0: loss = 0.0735633 (* 1 = 0.0735633 loss)\n",
      "I0427 17:33:12.639725  1495 sgd_solver.cpp:106] Iteration 400, lr = 0.00971013\n",
      "I0427 17:33:18.986111  1495 solver.cpp:337] Iteration 500, Testing net (#0)\n",
      "I0427 17:33:22.914928  1495 solver.cpp:404]     Test net output #0: accuracy = 0.9745\n",
      "I0427 17:33:22.914984  1495 solver.cpp:404]     Test net output #1: loss = 0.0828948 (* 1 = 0.0828948 loss)\n",
      "I0427 17:33:22.978227  1495 solver.cpp:228] Iteration 500, loss = 0.0877981\n",
      "I0427 17:33:22.978272  1495 solver.cpp:244]     Train net output #0: loss = 0.0877979 (* 1 = 0.0877979 loss)\n",
      "I0427 17:33:22.978286  1495 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069\n",
      "I0427 17:33:29.371002  1495 solver.cpp:228] Iteration 600, loss = 0.106665\n",
      "I0427 17:33:29.371042  1495 solver.cpp:244]     Train net output #0: loss = 0.106665 (* 1 = 0.106665 loss)\n",
      "I0427 17:33:29.371052  1495 sgd_solver.cpp:106] Iteration 600, lr = 0.0095724\n",
      "I0427 17:33:35.790123  1495 solver.cpp:228] Iteration 700, loss = 0.144663\n",
      "I0427 17:33:35.790153  1495 solver.cpp:244]     Train net output #0: loss = 0.144663 (* 1 = 0.144663 loss)\n",
      "I0427 17:33:35.790163  1495 sgd_solver.cpp:106] Iteration 700, lr = 0.00950522\n",
      "I0427 17:33:42.201269  1495 solver.cpp:228] Iteration 800, loss = 0.215293\n",
      "I0427 17:33:42.201313  1495 solver.cpp:244]     Train net output #0: loss = 0.215292 (* 1 = 0.215292 loss)\n",
      "I0427 17:33:42.201323  1495 sgd_solver.cpp:106] Iteration 800, lr = 0.00943913\n",
      "I0427 17:33:48.621850  1495 solver.cpp:228] Iteration 900, loss = 0.177914\n",
      "I0427 17:33:48.621976  1495 solver.cpp:244]     Train net output #0: loss = 0.177914 (* 1 = 0.177914 loss)\n",
      "I0427 17:33:48.622009  1495 sgd_solver.cpp:106] Iteration 900, lr = 0.00937411\n",
      "I0427 17:33:55.284095  1495 solver.cpp:337] Iteration 1000, Testing net (#0)\n",
      "I0427 17:33:59.359570  1495 solver.cpp:404]     Test net output #0: accuracy = 0.9818\n",
      "I0427 17:33:59.359629  1495 solver.cpp:404]     Test net output #1: loss = 0.0590566 (* 1 = 0.0590566 loss)\n",
      "I0427 17:33:59.426014  1495 solver.cpp:228] Iteration 1000, loss = 0.0771686\n",
      "I0427 17:33:59.426070  1495 solver.cpp:244]     Train net output #0: loss = 0.0771684 (* 1 = 0.0771684 loss)\n",
      "I0427 17:33:59.426082  1495 sgd_solver.cpp:106] Iteration 1000, lr = 0.00931012\n",
      "I0427 17:34:06.081321  1495 solver.cpp:228] Iteration 1100, loss = 0.00832614\n",
      "I0427 17:34:06.081382  1495 solver.cpp:244]     Train net output #0: loss = 0.00832599 (* 1 = 0.00832599 loss)\n",
      "I0427 17:34:06.081393  1495 sgd_solver.cpp:106] Iteration 1100, lr = 0.00924715\n",
      "I0427 17:34:12.757211  1495 solver.cpp:228] Iteration 1200, loss = 0.0328275\n",
      "I0427 17:34:12.757261  1495 solver.cpp:244]     Train net output #0: loss = 0.0328274 (* 1 = 0.0328274 loss)\n",
      "I0427 17:34:12.757272  1495 sgd_solver.cpp:106] Iteration 1200, lr = 0.00918515\n",
      "I0427 17:34:19.449092  1495 solver.cpp:228] Iteration 1300, loss = 0.0257732\n",
      "I0427 17:34:19.449192  1495 solver.cpp:244]     Train net output #0: loss = 0.025773 (* 1 = 0.025773 loss)\n",
      "I0427 17:34:19.449215  1495 sgd_solver.cpp:106] Iteration 1300, lr = 0.00912412\n",
      "I0427 17:34:26.135275  1495 solver.cpp:228] Iteration 1400, loss = 0.0074524\n",
      "I0427 17:34:26.135332  1495 solver.cpp:244]     Train net output #0: loss = 0.00745226 (* 1 = 0.00745226 loss)\n",
      "I0427 17:34:26.135344  1495 sgd_solver.cpp:106] Iteration 1400, lr = 0.00906403\n",
      "I0427 17:34:32.757843  1495 solver.cpp:337] Iteration 1500, Testing net (#0)\n",
      "I0427 17:34:36.836993  1495 solver.cpp:404]     Test net output #0: accuracy = 0.9852\n",
      "I0427 17:34:36.837030  1495 solver.cpp:404]     Test net output #1: loss = 0.0485121 (* 1 = 0.0485121 loss)\n",
      "I0427 17:34:36.902992  1495 solver.cpp:228] Iteration 1500, loss = 0.107774\n",
      "I0427 17:34:36.903036  1495 solver.cpp:244]     Train net output #0: loss = 0.107774 (* 1 = 0.107774 loss)\n",
      "I0427 17:34:36.903049  1495 sgd_solver.cpp:106] Iteration 1500, lr = 0.00900485\n",
      "I0427 17:34:43.531082  1495 solver.cpp:228] Iteration 1600, loss = 0.103244\n",
      "I0427 17:34:43.531131  1495 solver.cpp:244]     Train net output #0: loss = 0.103244 (* 1 = 0.103244 loss)\n",
      "I0427 17:34:43.531141  1495 sgd_solver.cpp:106] Iteration 1600, lr = 0.00894657\n",
      "I0427 17:34:50.185549  1495 solver.cpp:228] Iteration 1700, loss = 0.0117954\n",
      "I0427 17:34:50.185657  1495 solver.cpp:244]     Train net output #0: loss = 0.0117953 (* 1 = 0.0117953 loss)\n",
      "I0427 17:34:50.185669  1495 sgd_solver.cpp:106] Iteration 1700, lr = 0.00888916\n",
      "I0427 17:34:56.823282  1495 solver.cpp:228] Iteration 1800, loss = 0.0220539\n",
      "I0427 17:34:56.823329  1495 solver.cpp:244]     Train net output #0: loss = 0.0220537 (* 1 = 0.0220537 loss)\n",
      "I0427 17:34:56.823339  1495 sgd_solver.cpp:106] Iteration 1800, lr = 0.0088326\n",
      "I0427 17:35:03.476613  1495 solver.cpp:228] Iteration 1900, loss = 0.107266\n",
      "I0427 17:35:03.476656  1495 solver.cpp:244]     Train net output #0: loss = 0.107266 (* 1 = 0.107266 loss)\n",
      "I0427 17:35:03.476667  1495 sgd_solver.cpp:106] Iteration 1900, lr = 0.00877687\n",
      "I0427 17:35:10.091476  1495 solver.cpp:337] Iteration 2000, Testing net (#0)\n",
      "I0427 17:35:14.189911  1495 solver.cpp:404]     Test net output #0: accuracy = 0.9862\n",
      "I0427 17:35:14.189951  1495 solver.cpp:404]     Test net output #1: loss = 0.0439276 (* 1 = 0.0439276 loss)\n",
      "I0427 17:35:14.258404  1495 solver.cpp:228] Iteration 2000, loss = 0.0167336\n",
      "I0427 17:35:14.258450  1495 solver.cpp:244]     Train net output #0: loss = 0.0167334 (* 1 = 0.0167334 loss)\n",
      "I0427 17:35:14.258461  1495 sgd_solver.cpp:106] Iteration 2000, lr = 0.00872196\n",
      "I0427 17:35:20.941560  1495 solver.cpp:228] Iteration 2100, loss = 0.0217562\n",
      "I0427 17:35:20.941679  1495 solver.cpp:244]     Train net output #0: loss = 0.021756 (* 1 = 0.021756 loss)\n",
      "I0427 17:35:20.941691  1495 sgd_solver.cpp:106] Iteration 2100, lr = 0.00866784\n",
      "I0427 17:35:27.563931  1495 solver.cpp:228] Iteration 2200, loss = 0.0118215\n",
      "I0427 17:35:27.563977  1495 solver.cpp:244]     Train net output #0: loss = 0.0118213 (* 1 = 0.0118213 loss)\n",
      "I0427 17:35:27.563987  1495 sgd_solver.cpp:106] Iteration 2200, lr = 0.0086145\n",
      "I0427 17:35:34.195756  1495 solver.cpp:228] Iteration 2300, loss = 0.0930081\n",
      "I0427 17:35:34.195791  1495 solver.cpp:244]     Train net output #0: loss = 0.0930079 (* 1 = 0.0930079 loss)\n",
      "I0427 17:35:34.195801  1495 sgd_solver.cpp:106] Iteration 2300, lr = 0.00856192\n",
      "I0427 17:35:40.815762  1495 solver.cpp:228] Iteration 2400, loss = 0.00990249\n",
      "I0427 17:35:40.815809  1495 solver.cpp:244]     Train net output #0: loss = 0.00990236 (* 1 = 0.00990236 loss)\n",
      "I0427 17:35:40.815819  1495 sgd_solver.cpp:106] Iteration 2400, lr = 0.00851008\n",
      "I0427 17:35:47.388064  1495 solver.cpp:337] Iteration 2500, Testing net (#0)\n",
      "I0427 17:35:51.473690  1495 solver.cpp:404]     Test net output #0: accuracy = 0.9862\n",
      "I0427 17:35:51.473757  1495 solver.cpp:404]     Test net output #1: loss = 0.044675 (* 1 = 0.044675 loss)\n",
      "I0427 17:35:51.540123  1495 solver.cpp:228] Iteration 2500, loss = 0.0350954\n",
      "I0427 17:35:51.540168  1495 solver.cpp:244]     Train net output #0: loss = 0.0350953 (* 1 = 0.0350953 loss)\n",
      "I0427 17:35:51.540179  1495 sgd_solver.cpp:106] Iteration 2500, lr = 0.00845897\n",
      "I0427 17:35:58.157805  1495 solver.cpp:228] Iteration 2600, loss = 0.0607959\n",
      "I0427 17:35:58.157862  1495 solver.cpp:244]     Train net output #0: loss = 0.0607957 (* 1 = 0.0607957 loss)\n",
      "I0427 17:35:58.157872  1495 sgd_solver.cpp:106] Iteration 2600, lr = 0.00840857\n",
      "I0427 17:36:04.786581  1495 solver.cpp:228] Iteration 2700, loss = 0.0738677\n",
      "I0427 17:36:04.786634  1495 solver.cpp:244]     Train net output #0: loss = 0.0738676 (* 1 = 0.0738676 loss)\n",
      "I0427 17:36:04.786645  1495 sgd_solver.cpp:106] Iteration 2700, lr = 0.00835886\n",
      "I0427 17:36:11.473968  1495 solver.cpp:228] Iteration 2800, loss = 0.00529745\n",
      "I0427 17:36:11.474031  1495 solver.cpp:244]     Train net output #0: loss = 0.00529735 (* 1 = 0.00529735 loss)\n",
      "I0427 17:36:11.474042  1495 sgd_solver.cpp:106] Iteration 2800, lr = 0.00830984\n",
      "I0427 17:36:18.139938  1495 solver.cpp:228] Iteration 2900, loss = 0.0277741\n",
      "I0427 17:36:18.139981  1495 solver.cpp:244]     Train net output #0: loss = 0.0277741 (* 1 = 0.0277741 loss)\n",
      "I0427 17:36:18.139992  1495 sgd_solver.cpp:106] Iteration 2900, lr = 0.00826148\n",
      "I0427 17:36:24.754242  1495 solver.cpp:337] Iteration 3000, Testing net (#0)\n",
      "I0427 17:36:28.810317  1495 solver.cpp:404]     Test net output #0: accuracy = 0.9872\n",
      "I0427 17:36:28.810366  1495 solver.cpp:404]     Test net output #1: loss = 0.0367153 (* 1 = 0.0367153 loss)\n",
      "I0427 17:36:28.876085  1495 solver.cpp:228] Iteration 3000, loss = 0.0301018\n",
      "I0427 17:36:28.876130  1495 solver.cpp:244]     Train net output #0: loss = 0.0301017 (* 1 = 0.0301017 loss)\n",
      "I0427 17:36:28.876143  1495 sgd_solver.cpp:106] Iteration 3000, lr = 0.00821377\n",
      "I0427 17:36:35.507388  1495 solver.cpp:228] Iteration 3100, loss = 0.0331343\n",
      "I0427 17:36:35.507422  1495 solver.cpp:244]     Train net output #0: loss = 0.0331342 (* 1 = 0.0331342 loss)\n",
      "I0427 17:36:35.507433  1495 sgd_solver.cpp:106] Iteration 3100, lr = 0.0081667\n",
      "I0427 17:36:42.156379  1495 solver.cpp:228] Iteration 3200, loss = 0.0119551\n",
      "I0427 17:36:42.156426  1495 solver.cpp:244]     Train net output #0: loss = 0.011955 (* 1 = 0.011955 loss)\n",
      "I0427 17:36:42.156437  1495 sgd_solver.cpp:106] Iteration 3200, lr = 0.00812025\n",
      "I0427 17:36:48.811045  1495 solver.cpp:228] Iteration 3300, loss = 0.0116368\n",
      "I0427 17:36:48.811079  1495 solver.cpp:244]     Train net output #0: loss = 0.0116367 (* 1 = 0.0116367 loss)\n",
      "I0427 17:36:48.811089  1495 sgd_solver.cpp:106] Iteration 3300, lr = 0.00807442\n",
      "I0427 17:36:55.442131  1495 solver.cpp:228] Iteration 3400, loss = 0.0164548\n",
      "I0427 17:36:55.442384  1495 solver.cpp:244]     Train net output #0: loss = 0.0164547 (* 1 = 0.0164547 loss)\n",
      "I0427 17:36:55.442399  1495 sgd_solver.cpp:106] Iteration 3400, lr = 0.00802918\n",
      "I0427 17:37:02.200234  1495 solver.cpp:337] Iteration 3500, Testing net (#0)\n",
      "I0427 17:37:06.272744  1495 solver.cpp:404]     Test net output #0: accuracy = 0.9855\n",
      "I0427 17:37:06.272789  1495 solver.cpp:404]     Test net output #1: loss = 0.0421225 (* 1 = 0.0421225 loss)\n",
      "I0427 17:37:06.340175  1495 solver.cpp:228] Iteration 3500, loss = 0.00699938\n",
      "I0427 17:37:06.340215  1495 solver.cpp:244]     Train net output #0: loss = 0.00699931 (* 1 = 0.00699931 loss)\n",
      "I0427 17:37:06.340248  1495 sgd_solver.cpp:106] Iteration 3500, lr = 0.00798454\n",
      "I0427 17:37:13.034693  1495 solver.cpp:228] Iteration 3600, loss = 0.0271443\n",
      "I0427 17:37:13.034735  1495 solver.cpp:244]     Train net output #0: loss = 0.0271443 (* 1 = 0.0271443 loss)\n",
      "I0427 17:37:13.034746  1495 sgd_solver.cpp:106] Iteration 3600, lr = 0.00794046\n",
      "I0427 17:37:19.709159  1495 solver.cpp:228] Iteration 3700, loss = 0.0183969\n",
      "I0427 17:37:19.709202  1495 solver.cpp:244]     Train net output #0: loss = 0.0183968 (* 1 = 0.0183968 loss)\n",
      "I0427 17:37:19.709213  1495 sgd_solver.cpp:106] Iteration 3700, lr = 0.00789695\n",
      "I0427 17:37:26.386559  1495 solver.cpp:228] Iteration 3800, loss = 0.0121861\n",
      "I0427 17:37:26.386724  1495 solver.cpp:244]     Train net output #0: loss = 0.012186 (* 1 = 0.012186 loss)\n",
      "I0427 17:37:26.386737  1495 sgd_solver.cpp:106] Iteration 3800, lr = 0.007854\n",
      "I0427 17:37:33.074100  1495 solver.cpp:228] Iteration 3900, loss = 0.0417444\n",
      "I0427 17:37:33.074141  1495 solver.cpp:244]     Train net output #0: loss = 0.0417444 (* 1 = 0.0417444 loss)\n",
      "I0427 17:37:33.074152  1495 sgd_solver.cpp:106] Iteration 3900, lr = 0.00781158\n",
      "I0427 17:37:39.675565  1495 solver.cpp:337] Iteration 4000, Testing net (#0)\n",
      "I0427 17:37:43.764224  1495 solver.cpp:404]     Test net output #0: accuracy = 0.9897\n",
      "I0427 17:37:43.764268  1495 solver.cpp:404]     Test net output #1: loss = 0.0310566 (* 1 = 0.0310566 loss)\n",
      "I0427 17:37:43.829433  1495 solver.cpp:228] Iteration 4000, loss = 0.0169968\n",
      "I0427 17:37:43.829476  1495 solver.cpp:244]     Train net output #0: loss = 0.0169967 (* 1 = 0.0169967 loss)\n",
      "I0427 17:37:43.829488  1495 sgd_solver.cpp:106] Iteration 4000, lr = 0.0077697\n",
      "I0427 17:37:50.438627  1495 solver.cpp:228] Iteration 4100, loss = 0.0259481\n",
      "I0427 17:37:50.438673  1495 solver.cpp:244]     Train net output #0: loss = 0.025948 (* 1 = 0.025948 loss)\n",
      "I0427 17:37:50.438683  1495 sgd_solver.cpp:106] Iteration 4100, lr = 0.00772833\n",
      "I0427 17:37:57.073946  1495 solver.cpp:228] Iteration 4200, loss = 0.0159325\n",
      "I0427 17:37:57.074115  1495 solver.cpp:244]     Train net output #0: loss = 0.0159324 (* 1 = 0.0159324 loss)\n",
      "I0427 17:37:57.074128  1495 sgd_solver.cpp:106] Iteration 4200, lr = 0.00768748\n",
      "I0427 17:38:03.688393  1495 solver.cpp:228] Iteration 4300, loss = 0.0531067\n",
      "I0427 17:38:03.688437  1495 solver.cpp:244]     Train net output #0: loss = 0.0531066 (* 1 = 0.0531066 loss)\n",
      "I0427 17:38:03.688447  1495 sgd_solver.cpp:106] Iteration 4300, lr = 0.00764712\n",
      "I0427 17:38:10.347057  1495 solver.cpp:228] Iteration 4400, loss = 0.0183948\n",
      "I0427 17:38:10.347108  1495 solver.cpp:244]     Train net output #0: loss = 0.0183947 (* 1 = 0.0183947 loss)\n",
      "I0427 17:38:10.347120  1495 sgd_solver.cpp:106] Iteration 4400, lr = 0.00760726\n",
      "I0427 17:38:16.992246  1495 solver.cpp:337] Iteration 4500, Testing net (#0)\n",
      "I0427 17:38:21.066840  1495 solver.cpp:404]     Test net output #0: accuracy = 0.9876\n",
      "I0427 17:38:21.066885  1495 solver.cpp:404]     Test net output #1: loss = 0.0370598 (* 1 = 0.0370598 loss)\n",
      "I0427 17:38:21.132107  1495 solver.cpp:228] Iteration 4500, loss = 0.00544998\n",
      "I0427 17:38:21.132148  1495 solver.cpp:244]     Train net output #0: loss = 0.0054499 (* 1 = 0.0054499 loss)\n",
      "I0427 17:38:21.132161  1495 sgd_solver.cpp:106] Iteration 4500, lr = 0.00756788\n",
      "I0427 17:38:27.749424  1495 solver.cpp:228] Iteration 4600, loss = 0.0087425\n",
      "I0427 17:38:27.749534  1495 solver.cpp:244]     Train net output #0: loss = 0.00874241 (* 1 = 0.00874241 loss)\n",
      "I0427 17:38:27.749547  1495 sgd_solver.cpp:106] Iteration 4600, lr = 0.00752897\n",
      "I0427 17:38:34.382122  1495 solver.cpp:228] Iteration 4700, loss = 0.00553128\n",
      "I0427 17:38:34.382165  1495 solver.cpp:244]     Train net output #0: loss = 0.00553118 (* 1 = 0.00553118 loss)\n",
      "I0427 17:38:34.382175  1495 sgd_solver.cpp:106] Iteration 4700, lr = 0.00749052\n",
      "I0427 17:38:41.016296  1495 solver.cpp:228] Iteration 4800, loss = 0.0161681\n",
      "I0427 17:38:41.016330  1495 solver.cpp:244]     Train net output #0: loss = 0.016168 (* 1 = 0.016168 loss)\n",
      "I0427 17:38:41.016340  1495 sgd_solver.cpp:106] Iteration 4800, lr = 0.00745253\n",
      "I0427 17:38:47.637630  1495 solver.cpp:228] Iteration 4900, loss = 0.00711196\n",
      "I0427 17:38:47.637662  1495 solver.cpp:244]     Train net output #0: loss = 0.00711185 (* 1 = 0.00711185 loss)\n",
      "I0427 17:38:47.637673  1495 sgd_solver.cpp:106] Iteration 4900, lr = 0.00741498\n",
      "I0427 17:38:54.217818  1495 solver.cpp:454] Snapshotting to binary proto file ./lenet/lenet_iter_5000.caffemodel\n",
      "I0427 17:38:54.222637  1495 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./lenet/lenet_iter_5000.solverstate\n",
      "I0427 17:38:54.225260  1495 solver.cpp:337] Iteration 5000, Testing net (#0)\n",
      "I0427 17:38:58.289919  1495 solver.cpp:404]     Test net output #0: accuracy = 0.9902\n",
      "I0427 17:38:58.290066  1495 solver.cpp:404]     Test net output #1: loss = 0.0304041 (* 1 = 0.0304041 loss)\n",
      "I0427 17:38:58.355993  1495 solver.cpp:228] Iteration 5000, loss = 0.0226117\n",
      "I0427 17:38:58.356036  1495 solver.cpp:244]     Train net output #0: loss = 0.0226116 (* 1 = 0.0226116 loss)\n",
      "I0427 17:38:58.356050  1495 sgd_solver.cpp:106] Iteration 5000, lr = 0.00737788\n",
      "I0427 17:39:04.993969  1495 solver.cpp:228] Iteration 5100, loss = 0.0203164\n",
      "I0427 17:39:04.994014  1495 solver.cpp:244]     Train net output #0: loss = 0.0203163 (* 1 = 0.0203163 loss)\n",
      "I0427 17:39:04.994024  1495 sgd_solver.cpp:106] Iteration 5100, lr = 0.0073412\n",
      "I0427 17:39:11.650763  1495 solver.cpp:228] Iteration 5200, loss = 0.00424009\n",
      "I0427 17:39:11.650800  1495 solver.cpp:244]     Train net output #0: loss = 0.00423998 (* 1 = 0.00423998 loss)\n",
      "I0427 17:39:11.650811  1495 sgd_solver.cpp:106] Iteration 5200, lr = 0.00730495\n",
      "I0427 17:39:18.344132  1495 solver.cpp:228] Iteration 5300, loss = 0.00079958\n",
      "I0427 17:39:18.344166  1495 solver.cpp:244]     Train net output #0: loss = 0.000799469 (* 1 = 0.000799469 loss)\n",
      "I0427 17:39:18.344175  1495 sgd_solver.cpp:106] Iteration 5300, lr = 0.00726911\n",
      "I0427 17:39:25.027117  1495 solver.cpp:228] Iteration 5400, loss = 0.00694105\n",
      "I0427 17:39:25.027159  1495 solver.cpp:244]     Train net output #0: loss = 0.00694095 (* 1 = 0.00694095 loss)\n",
      "I0427 17:39:25.027169  1495 sgd_solver.cpp:106] Iteration 5400, lr = 0.00723368\n",
      "I0427 17:39:31.628196  1495 solver.cpp:337] Iteration 5500, Testing net (#0)\n",
      "I0427 17:39:35.857805  1495 solver.cpp:404]     Test net output #0: accuracy = 0.9889\n",
      "I0427 17:39:35.857843  1495 solver.cpp:404]     Test net output #1: loss = 0.034914 (* 1 = 0.034914 loss)\n",
      "I0427 17:39:35.922648  1495 solver.cpp:228] Iteration 5500, loss = 0.0108889\n",
      "I0427 17:39:35.922685  1495 solver.cpp:244]     Train net output #0: loss = 0.0108888 (* 1 = 0.0108888 loss)\n",
      "I0427 17:39:35.922698  1495 sgd_solver.cpp:106] Iteration 5500, lr = 0.00719865\n",
      "I0427 17:39:42.486873  1495 solver.cpp:228] Iteration 5600, loss = 0.000657603\n",
      "I0427 17:39:42.486914  1495 solver.cpp:244]     Train net output #0: loss = 0.000657526 (* 1 = 0.000657526 loss)\n",
      "I0427 17:39:42.486924  1495 sgd_solver.cpp:106] Iteration 5600, lr = 0.00716402\n",
      "I0427 17:39:49.051311  1495 solver.cpp:228] Iteration 5700, loss = 0.00506177\n",
      "I0427 17:39:49.051343  1495 solver.cpp:244]     Train net output #0: loss = 0.0050617 (* 1 = 0.0050617 loss)\n",
      "I0427 17:39:49.051353  1495 sgd_solver.cpp:106] Iteration 5700, lr = 0.00712977\n",
      "I0427 17:39:55.614873  1495 solver.cpp:228] Iteration 5800, loss = 0.0192222\n",
      "I0427 17:39:55.614913  1495 solver.cpp:244]     Train net output #0: loss = 0.0192221 (* 1 = 0.0192221 loss)\n",
      "I0427 17:39:55.614923  1495 sgd_solver.cpp:106] Iteration 5800, lr = 0.0070959\n",
      "I0427 17:40:02.179733  1495 solver.cpp:228] Iteration 5900, loss = 0.00828354\n",
      "I0427 17:40:02.179940  1495 solver.cpp:244]     Train net output #0: loss = 0.00828346 (* 1 = 0.00828346 loss)\n",
      "I0427 17:40:02.179962  1495 sgd_solver.cpp:106] Iteration 5900, lr = 0.0070624\n",
      "I0427 17:40:08.858412  1495 solver.cpp:337] Iteration 6000, Testing net (#0)\n",
      "I0427 17:40:12.884176  1495 solver.cpp:404]     Test net output #0: accuracy = 0.9914\n",
      "I0427 17:40:12.884232  1495 solver.cpp:404]     Test net output #1: loss = 0.028303 (* 1 = 0.028303 loss)\n",
      "I0427 17:40:12.950361  1495 solver.cpp:228] Iteration 6000, loss = 0.00437655\n",
      "I0427 17:40:12.950400  1495 solver.cpp:244]     Train net output #0: loss = 0.00437647 (* 1 = 0.00437647 loss)\n",
      "I0427 17:40:12.950412  1495 sgd_solver.cpp:106] Iteration 6000, lr = 0.00702927\n",
      "I0427 17:40:19.515029  1495 solver.cpp:228] Iteration 6100, loss = 0.00202968\n",
      "I0427 17:40:19.515075  1495 solver.cpp:244]     Train net output #0: loss = 0.0020296 (* 1 = 0.0020296 loss)\n",
      "I0427 17:40:19.515087  1495 sgd_solver.cpp:106] Iteration 6100, lr = 0.0069965\n",
      "I0427 17:40:26.201392  1495 solver.cpp:228] Iteration 6200, loss = 0.00882109\n",
      "I0427 17:40:26.201445  1495 solver.cpp:244]     Train net output #0: loss = 0.008821 (* 1 = 0.008821 loss)\n",
      "I0427 17:40:26.201457  1495 sgd_solver.cpp:106] Iteration 6200, lr = 0.00696408\n",
      "I0427 17:40:32.893390  1495 solver.cpp:228] Iteration 6300, loss = 0.0078656\n",
      "I0427 17:40:32.893519  1495 solver.cpp:244]     Train net output #0: loss = 0.00786552 (* 1 = 0.00786552 loss)\n",
      "I0427 17:40:32.893537  1495 sgd_solver.cpp:106] Iteration 6300, lr = 0.00693201\n",
      "I0427 17:40:39.743285  1495 solver.cpp:228] Iteration 6400, loss = 0.00667899\n",
      "I0427 17:40:39.743341  1495 solver.cpp:244]     Train net output #0: loss = 0.00667891 (* 1 = 0.00667891 loss)\n",
      "I0427 17:40:39.743351  1495 sgd_solver.cpp:106] Iteration 6400, lr = 0.00690029\n",
      "I0427 17:40:46.480005  1495 solver.cpp:337] Iteration 6500, Testing net (#0)\n",
      "I0427 17:40:50.638074  1495 solver.cpp:404]     Test net output #0: accuracy = 0.9904\n",
      "I0427 17:40:50.638125  1495 solver.cpp:404]     Test net output #1: loss = 0.030342 (* 1 = 0.030342 loss)\n",
      "I0427 17:40:50.703668  1495 solver.cpp:228] Iteration 6500, loss = 0.0108664\n",
      "I0427 17:40:50.703713  1495 solver.cpp:244]     Train net output #0: loss = 0.0108663 (* 1 = 0.0108663 loss)\n",
      "I0427 17:40:50.703727  1495 sgd_solver.cpp:106] Iteration 6500, lr = 0.0068689\n",
      "I0427 17:40:57.341490  1495 solver.cpp:228] Iteration 6600, loss = 0.0125268\n",
      "I0427 17:40:57.341538  1495 solver.cpp:244]     Train net output #0: loss = 0.0125267 (* 1 = 0.0125267 loss)\n",
      "I0427 17:40:57.341549  1495 sgd_solver.cpp:106] Iteration 6600, lr = 0.00683784\n",
      "I0427 17:41:03.970790  1495 solver.cpp:228] Iteration 6700, loss = 0.00755973\n",
      "I0427 17:41:03.971002  1495 solver.cpp:244]     Train net output #0: loss = 0.00755965 (* 1 = 0.00755965 loss)\n",
      "I0427 17:41:03.971019  1495 sgd_solver.cpp:106] Iteration 6700, lr = 0.00680711\n",
      "I0427 17:41:10.844213  1495 solver.cpp:228] Iteration 6800, loss = 0.00384675\n",
      "I0427 17:41:10.844269  1495 solver.cpp:244]     Train net output #0: loss = 0.00384667 (* 1 = 0.00384667 loss)\n",
      "I0427 17:41:10.844280  1495 sgd_solver.cpp:106] Iteration 6800, lr = 0.0067767\n",
      "I0427 17:41:17.539777  1495 solver.cpp:228] Iteration 6900, loss = 0.00795538\n",
      "I0427 17:41:17.539821  1495 solver.cpp:244]     Train net output #0: loss = 0.00795529 (* 1 = 0.00795529 loss)\n",
      "I0427 17:41:17.539832  1495 sgd_solver.cpp:106] Iteration 6900, lr = 0.0067466\n",
      "I0427 17:41:24.170197  1495 solver.cpp:337] Iteration 7000, Testing net (#0)\n",
      "I0427 17:41:28.240911  1495 solver.cpp:404]     Test net output #0: accuracy = 0.9906\n",
      "I0427 17:41:28.240950  1495 solver.cpp:404]     Test net output #1: loss = 0.0292337 (* 1 = 0.0292337 loss)\n",
      "I0427 17:41:28.306912  1495 solver.cpp:228] Iteration 7000, loss = 0.00515609\n",
      "I0427 17:41:28.306957  1495 solver.cpp:244]     Train net output #0: loss = 0.00515599 (* 1 = 0.00515599 loss)\n",
      "I0427 17:41:28.306970  1495 sgd_solver.cpp:106] Iteration 7000, lr = 0.00671681\n",
      "I0427 17:41:34.933495  1495 solver.cpp:228] Iteration 7100, loss = 0.0125389\n",
      "I0427 17:41:34.933687  1495 solver.cpp:244]     Train net output #0: loss = 0.0125388 (* 1 = 0.0125388 loss)\n",
      "I0427 17:41:34.933712  1495 sgd_solver.cpp:106] Iteration 7100, lr = 0.00668733\n",
      "I0427 17:41:41.567168  1495 solver.cpp:228] Iteration 7200, loss = 0.00340143\n",
      "I0427 17:41:41.567216  1495 solver.cpp:244]     Train net output #0: loss = 0.00340133 (* 1 = 0.00340133 loss)\n",
      "I0427 17:41:41.567226  1495 sgd_solver.cpp:106] Iteration 7200, lr = 0.00665815\n",
      "I0427 17:41:48.206933  1495 solver.cpp:228] Iteration 7300, loss = 0.0191603\n",
      "I0427 17:41:48.206979  1495 solver.cpp:244]     Train net output #0: loss = 0.0191602 (* 1 = 0.0191602 loss)\n",
      "I0427 17:41:48.206990  1495 sgd_solver.cpp:106] Iteration 7300, lr = 0.00662927\n",
      "I0427 17:41:54.838984  1495 solver.cpp:228] Iteration 7400, loss = 0.00416207\n",
      "I0427 17:41:54.839030  1495 solver.cpp:244]     Train net output #0: loss = 0.00416198 (* 1 = 0.00416198 loss)\n",
      "I0427 17:41:54.839041  1495 sgd_solver.cpp:106] Iteration 7400, lr = 0.00660067\n",
      "I0427 17:42:01.403267  1495 solver.cpp:337] Iteration 7500, Testing net (#0)\n",
      "I0427 17:42:05.479534  1495 solver.cpp:404]     Test net output #0: accuracy = 0.9906\n",
      "I0427 17:42:05.479718  1495 solver.cpp:404]     Test net output #1: loss = 0.0316378 (* 1 = 0.0316378 loss)\n",
      "I0427 17:42:05.552189  1495 solver.cpp:228] Iteration 7500, loss = 0.00238725\n",
      "I0427 17:42:05.552239  1495 solver.cpp:244]     Train net output #0: loss = 0.00238715 (* 1 = 0.00238715 loss)\n",
      "I0427 17:42:05.552254  1495 sgd_solver.cpp:106] Iteration 7500, lr = 0.00657236\n",
      "I0427 17:42:12.579663  1495 solver.cpp:228] Iteration 7600, loss = 0.00611079\n",
      "I0427 17:42:12.579711  1495 solver.cpp:244]     Train net output #0: loss = 0.0061107 (* 1 = 0.0061107 loss)\n",
      "I0427 17:42:12.579723  1495 sgd_solver.cpp:106] Iteration 7600, lr = 0.00654433\n",
      "I0427 17:42:19.240710  1495 solver.cpp:228] Iteration 7700, loss = 0.0241446\n",
      "I0427 17:42:19.240741  1495 solver.cpp:244]     Train net output #0: loss = 0.0241445 (* 1 = 0.0241445 loss)\n",
      "I0427 17:42:19.240751  1495 sgd_solver.cpp:106] Iteration 7700, lr = 0.00651658\n",
      "I0427 17:42:25.910765  1495 solver.cpp:228] Iteration 7800, loss = 0.00573759\n",
      "I0427 17:42:25.910809  1495 solver.cpp:244]     Train net output #0: loss = 0.00573751 (* 1 = 0.00573751 loss)\n",
      "I0427 17:42:25.910820  1495 sgd_solver.cpp:106] Iteration 7800, lr = 0.00648911\n",
      "I0427 17:42:32.576333  1495 solver.cpp:228] Iteration 7900, loss = 0.00248856\n",
      "I0427 17:42:32.576375  1495 solver.cpp:244]     Train net output #0: loss = 0.00248847 (* 1 = 0.00248847 loss)\n",
      "I0427 17:42:32.576385  1495 sgd_solver.cpp:106] Iteration 7900, lr = 0.0064619\n",
      "I0427 17:42:39.221571  1495 solver.cpp:337] Iteration 8000, Testing net (#0)\n",
      "I0427 17:42:43.307467  1495 solver.cpp:404]     Test net output #0: accuracy = 0.9902\n",
      "I0427 17:42:43.307520  1495 solver.cpp:404]     Test net output #1: loss = 0.0308728 (* 1 = 0.0308728 loss)\n",
      "I0427 17:42:43.373483  1495 solver.cpp:228] Iteration 8000, loss = 0.00597149\n",
      "I0427 17:42:43.373528  1495 solver.cpp:244]     Train net output #0: loss = 0.0059714 (* 1 = 0.0059714 loss)\n",
      "I0427 17:42:43.373540  1495 sgd_solver.cpp:106] Iteration 8000, lr = 0.00643496\n",
      "I0427 17:42:50.057698  1495 solver.cpp:228] Iteration 8100, loss = 0.0106927\n",
      "I0427 17:42:50.057746  1495 solver.cpp:244]     Train net output #0: loss = 0.0106926 (* 1 = 0.0106926 loss)\n",
      "I0427 17:42:50.057757  1495 sgd_solver.cpp:106] Iteration 8100, lr = 0.00640827\n",
      "I0427 17:42:56.700911  1495 solver.cpp:228] Iteration 8200, loss = 0.00669345\n",
      "I0427 17:42:56.700958  1495 solver.cpp:244]     Train net output #0: loss = 0.00669336 (* 1 = 0.00669336 loss)\n",
      "I0427 17:42:56.700968  1495 sgd_solver.cpp:106] Iteration 8200, lr = 0.00638185\n",
      "I0427 17:43:03.346541  1495 solver.cpp:228] Iteration 8300, loss = 0.0319145\n",
      "I0427 17:43:03.346586  1495 solver.cpp:244]     Train net output #0: loss = 0.0319144 (* 1 = 0.0319144 loss)\n",
      "I0427 17:43:03.346596  1495 sgd_solver.cpp:106] Iteration 8300, lr = 0.00635567\n",
      "I0427 17:43:10.004933  1495 solver.cpp:228] Iteration 8400, loss = 0.00570121\n",
      "I0427 17:43:10.005130  1495 solver.cpp:244]     Train net output #0: loss = 0.00570112 (* 1 = 0.00570112 loss)\n",
      "I0427 17:43:10.005147  1495 sgd_solver.cpp:106] Iteration 8400, lr = 0.00632975\n",
      "I0427 17:43:16.839690  1495 solver.cpp:337] Iteration 8500, Testing net (#0)\n",
      "I0427 17:43:20.995076  1495 solver.cpp:404]     Test net output #0: accuracy = 0.9898\n",
      "I0427 17:43:20.995134  1495 solver.cpp:404]     Test net output #1: loss = 0.0299842 (* 1 = 0.0299842 loss)\n",
      "I0427 17:43:21.061585  1495 solver.cpp:228] Iteration 8500, loss = 0.0063924\n",
      "I0427 17:43:21.061637  1495 solver.cpp:244]     Train net output #0: loss = 0.0063923 (* 1 = 0.0063923 loss)\n",
      "I0427 17:43:21.061651  1495 sgd_solver.cpp:106] Iteration 8500, lr = 0.00630407\n",
      "I0427 17:43:27.730695  1495 solver.cpp:228] Iteration 8600, loss = 0.000780259\n",
      "I0427 17:43:27.730737  1495 solver.cpp:244]     Train net output #0: loss = 0.000780167 (* 1 = 0.000780167 loss)\n",
      "I0427 17:43:27.730748  1495 sgd_solver.cpp:106] Iteration 8600, lr = 0.00627864\n",
      "I0427 17:43:34.351380  1495 solver.cpp:228] Iteration 8700, loss = 0.00195479\n",
      "I0427 17:43:34.351445  1495 solver.cpp:244]     Train net output #0: loss = 0.0019547 (* 1 = 0.0019547 loss)\n",
      "I0427 17:43:34.351456  1495 sgd_solver.cpp:106] Iteration 8700, lr = 0.00625344\n",
      "I0427 17:43:40.970232  1495 solver.cpp:228] Iteration 8800, loss = 0.00206069\n",
      "I0427 17:43:40.970481  1495 solver.cpp:244]     Train net output #0: loss = 0.0020606 (* 1 = 0.0020606 loss)\n",
      "I0427 17:43:40.970496  1495 sgd_solver.cpp:106] Iteration 8800, lr = 0.00622847\n",
      "I0427 17:43:47.619493  1495 solver.cpp:228] Iteration 8900, loss = 0.00115835\n",
      "I0427 17:43:47.619529  1495 solver.cpp:244]     Train net output #0: loss = 0.00115825 (* 1 = 0.00115825 loss)\n",
      "I0427 17:43:47.619540  1495 sgd_solver.cpp:106] Iteration 8900, lr = 0.00620374\n",
      "I0427 17:43:54.160326  1495 solver.cpp:337] Iteration 9000, Testing net (#0)\n",
      "I0427 17:43:58.238796  1495 solver.cpp:404]     Test net output #0: accuracy = 0.9906\n",
      "I0427 17:43:58.238837  1495 solver.cpp:404]     Test net output #1: loss = 0.0290934 (* 1 = 0.0290934 loss)\n",
      "I0427 17:43:58.304533  1495 solver.cpp:228] Iteration 9000, loss = 0.0202301\n",
      "I0427 17:43:58.304579  1495 solver.cpp:244]     Train net output #0: loss = 0.02023 (* 1 = 0.02023 loss)\n",
      "I0427 17:43:58.304590  1495 sgd_solver.cpp:106] Iteration 9000, lr = 0.00617924\n",
      "I0427 17:44:04.909853  1495 solver.cpp:228] Iteration 9100, loss = 0.00615897\n",
      "I0427 17:44:04.909888  1495 solver.cpp:244]     Train net output #0: loss = 0.00615888 (* 1 = 0.00615888 loss)\n",
      "I0427 17:44:04.909898  1495 sgd_solver.cpp:106] Iteration 9100, lr = 0.00615496\n",
      "I0427 17:44:11.496551  1495 solver.cpp:228] Iteration 9200, loss = 0.00250854\n",
      "I0427 17:44:11.496691  1495 solver.cpp:244]     Train net output #0: loss = 0.00250845 (* 1 = 0.00250845 loss)\n",
      "I0427 17:44:11.496707  1495 sgd_solver.cpp:106] Iteration 9200, lr = 0.0061309\n",
      "I0427 17:44:18.508708  1495 solver.cpp:228] Iteration 9300, loss = 0.00537642\n",
      "I0427 17:44:18.508749  1495 solver.cpp:244]     Train net output #0: loss = 0.00537634 (* 1 = 0.00537634 loss)\n",
      "I0427 17:44:18.508760  1495 sgd_solver.cpp:106] Iteration 9300, lr = 0.00610706\n",
      "I0427 17:44:25.101447  1495 solver.cpp:228] Iteration 9400, loss = 0.0172117\n",
      "I0427 17:44:25.101500  1495 solver.cpp:244]     Train net output #0: loss = 0.0172116 (* 1 = 0.0172116 loss)\n",
      "I0427 17:44:25.101511  1495 sgd_solver.cpp:106] Iteration 9400, lr = 0.00608343\n",
      "I0427 17:44:31.642563  1495 solver.cpp:337] Iteration 9500, Testing net (#0)\n",
      "I0427 17:44:35.724877  1495 solver.cpp:404]     Test net output #0: accuracy = 0.9892\n",
      "I0427 17:44:35.724925  1495 solver.cpp:404]     Test net output #1: loss = 0.0342517 (* 1 = 0.0342517 loss)\n",
      "I0427 17:44:35.790988  1495 solver.cpp:228] Iteration 9500, loss = 0.00576358\n",
      "I0427 17:44:35.791033  1495 solver.cpp:244]     Train net output #0: loss = 0.00576348 (* 1 = 0.00576348 loss)\n",
      "I0427 17:44:35.791045  1495 sgd_solver.cpp:106] Iteration 9500, lr = 0.00606002\n",
      "I0427 17:44:42.396962  1495 solver.cpp:228] Iteration 9600, loss = 0.00355147\n",
      "I0427 17:44:42.397121  1495 solver.cpp:244]     Train net output #0: loss = 0.00355137 (* 1 = 0.00355137 loss)\n",
      "I0427 17:44:42.397143  1495 sgd_solver.cpp:106] Iteration 9600, lr = 0.00603682\n",
      "I0427 17:44:49.701345  1495 solver.cpp:228] Iteration 9700, loss = 0.00268725\n",
      "I0427 17:44:49.701398  1495 solver.cpp:244]     Train net output #0: loss = 0.00268715 (* 1 = 0.00268715 loss)\n",
      "I0427 17:44:49.701411  1495 sgd_solver.cpp:106] Iteration 9700, lr = 0.00601382\n",
      "I0427 17:44:56.298887  1495 solver.cpp:228] Iteration 9800, loss = 0.013594\n",
      "I0427 17:44:56.298943  1495 solver.cpp:244]     Train net output #0: loss = 0.0135939 (* 1 = 0.0135939 loss)\n",
      "I0427 17:44:56.298954  1495 sgd_solver.cpp:106] Iteration 9800, lr = 0.00599102\n",
      "I0427 17:45:02.886514  1495 solver.cpp:228] Iteration 9900, loss = 0.0052505\n",
      "I0427 17:45:02.886550  1495 solver.cpp:244]     Train net output #0: loss = 0.0052504 (* 1 = 0.0052504 loss)\n",
      "I0427 17:45:02.886560  1495 sgd_solver.cpp:106] Iteration 9900, lr = 0.00596843\n",
      "I0427 17:45:09.427040  1495 solver.cpp:454] Snapshotting to binary proto file ./lenet/lenet_iter_10000.caffemodel\n",
      "I0427 17:45:09.432242  1495 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./lenet/lenet_iter_10000.solverstate\n",
      "I0427 17:45:09.461935  1495 solver.cpp:317] Iteration 10000, loss = 0.00333327\n",
      "I0427 17:45:09.461974  1495 solver.cpp:337] Iteration 10000, Testing net (#0)\n",
      "I0427 17:45:13.530589  1495 solver.cpp:404]     Test net output #0: accuracy = 0.991\n",
      "I0427 17:45:13.530675  1495 solver.cpp:404]     Test net output #1: loss = 0.0284359 (* 1 = 0.0284359 loss)\n",
      "I0427 17:45:13.530685  1495 solver.cpp:322] Optimization Done.\n",
      "I0427 17:45:13.530691  1495 caffe.cpp:222] Optimization Done.\n"
     ]
    }
   ],
   "source": [
    "!./caffe/build/tools/caffe train --solver=./lenet/lenet_solver.prototxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caffe에서는 예기치 않은 이유로 학습이 중단되었을 경우 저장된 지점(.solverstate 파일)이 있다면 이어서 학습이 가능합니다. (Snapshot resume 기능)\n",
    "\n",
    "이미 5000번까지 학습된 Solver state가 저장되어 있으므로 아래의 명령어를 통해 iteration 5000번부터 이어서 학습이 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libdc1394 error: Failed to initialize libdc1394\n",
      "I0427 17:45:13.703312  1530 caffe.cpp:178] Use CPU.\n",
      "I0427 17:45:13.703660  1530 solver.cpp:48] Initializing solver from parameters: \n",
      "test_iter: 100\n",
      "test_interval: 500\n",
      "base_lr: 0.01\n",
      "display: 100\n",
      "max_iter: 10000\n",
      "lr_policy: \"inv\"\n",
      "gamma: 0.0001\n",
      "power: 0.75\n",
      "momentum: 0.9\n",
      "weight_decay: 0.0005\n",
      "snapshot: 5000\n",
      "snapshot_prefix: \"./lenet/lenet\"\n",
      "solver_mode: CPU\n",
      "net: \"./lenet/lenet_train_test.prototxt\"\n",
      "I0427 17:45:13.703784  1530 solver.cpp:91] Creating training net from net file: ./lenet/lenet_train_test.prototxt\n",
      "I0427 17:45:13.703986  1530 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist\n",
      "I0427 17:45:13.704006  1530 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy\n",
      "I0427 17:45:13.704066  1530 net.cpp:49] Initializing net from parameters: \n",
      "name: \"LeNet\"\n",
      "state {\n",
      "  phase: TRAIN\n",
      "}\n",
      "layer {\n",
      "  name: \"mnist\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TRAIN\n",
      "  }\n",
      "  transform_param {\n",
      "    scale: 0.00390625\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"./lenet/mnist_train_lmdb\"\n",
      "    batch_size: 64\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 20\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 50\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"ip1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"ip1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 500\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"ip2\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 10\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"ip2\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I0427 17:45:13.704401  1530 layer_factory.hpp:77] Creating layer mnist\n",
      "I0427 17:45:13.704767  1530 net.cpp:91] Creating Layer mnist\n",
      "I0427 17:45:13.704785  1530 net.cpp:399] mnist -> data\n",
      "I0427 17:45:13.704805  1530 net.cpp:399] mnist -> label\n",
      "I0427 17:45:13.704874  1531 db_lmdb.cpp:38] Opened lmdb ./lenet/mnist_train_lmdb\n",
      "I0427 17:45:13.704919  1530 data_layer.cpp:41] output data size: 64,1,28,28\n",
      "I0427 17:45:13.705263  1530 net.cpp:141] Setting up mnist\n",
      "I0427 17:45:13.705279  1530 net.cpp:148] Top shape: 64 1 28 28 (50176)\n",
      "I0427 17:45:13.705288  1530 net.cpp:148] Top shape: 64 (64)\n",
      "I0427 17:45:13.705296  1530 net.cpp:156] Memory required for data: 200960\n",
      "I0427 17:45:13.705305  1530 layer_factory.hpp:77] Creating layer conv1\n",
      "I0427 17:45:13.705322  1530 net.cpp:91] Creating Layer conv1\n",
      "I0427 17:45:13.705330  1530 net.cpp:425] conv1 <- data\n",
      "I0427 17:45:13.705343  1530 net.cpp:399] conv1 -> conv1\n",
      "I0427 17:45:13.705385  1530 net.cpp:141] Setting up conv1\n",
      "I0427 17:45:13.705396  1530 net.cpp:148] Top shape: 64 20 24 24 (737280)\n",
      "I0427 17:45:13.705404  1530 net.cpp:156] Memory required for data: 3150080\n",
      "I0427 17:45:13.705420  1530 layer_factory.hpp:77] Creating layer pool1\n",
      "I0427 17:45:13.705430  1530 net.cpp:91] Creating Layer pool1\n",
      "I0427 17:45:13.705437  1530 net.cpp:425] pool1 <- conv1\n",
      "I0427 17:45:13.705446  1530 net.cpp:399] pool1 -> pool1\n",
      "I0427 17:45:13.705464  1530 net.cpp:141] Setting up pool1\n",
      "I0427 17:45:13.705488  1530 net.cpp:148] Top shape: 64 20 12 12 (184320)\n",
      "I0427 17:45:13.705495  1530 net.cpp:156] Memory required for data: 3887360\n",
      "I0427 17:45:13.705502  1530 layer_factory.hpp:77] Creating layer conv2\n",
      "I0427 17:45:13.705513  1530 net.cpp:91] Creating Layer conv2\n",
      "I0427 17:45:13.705521  1530 net.cpp:425] conv2 <- pool1\n",
      "I0427 17:45:13.705530  1530 net.cpp:399] conv2 -> conv2\n",
      "I0427 17:45:13.705709  1530 net.cpp:141] Setting up conv2\n",
      "I0427 17:45:13.705724  1530 net.cpp:148] Top shape: 64 50 8 8 (204800)\n",
      "I0427 17:45:13.705730  1530 net.cpp:156] Memory required for data: 4706560\n",
      "I0427 17:45:13.705741  1530 layer_factory.hpp:77] Creating layer pool2\n",
      "I0427 17:45:13.705751  1530 net.cpp:91] Creating Layer pool2\n",
      "I0427 17:45:13.705759  1530 net.cpp:425] pool2 <- conv2\n",
      "I0427 17:45:13.705767  1530 net.cpp:399] pool2 -> pool2\n",
      "I0427 17:45:13.705778  1530 net.cpp:141] Setting up pool2\n",
      "I0427 17:45:13.705787  1530 net.cpp:148] Top shape: 64 50 4 4 (51200)\n",
      "I0427 17:45:13.705795  1530 net.cpp:156] Memory required for data: 4911360\n",
      "I0427 17:45:13.705801  1530 layer_factory.hpp:77] Creating layer ip1\n",
      "I0427 17:45:13.705811  1530 net.cpp:91] Creating Layer ip1\n",
      "I0427 17:45:13.705819  1530 net.cpp:425] ip1 <- pool2\n",
      "I0427 17:45:13.705827  1530 net.cpp:399] ip1 -> ip1\n",
      "I0427 17:45:13.708197  1530 net.cpp:141] Setting up ip1\n",
      "I0427 17:45:13.708214  1530 net.cpp:148] Top shape: 64 500 (32000)\n",
      "I0427 17:45:13.708220  1530 net.cpp:156] Memory required for data: 5039360\n",
      "I0427 17:45:13.708230  1530 layer_factory.hpp:77] Creating layer relu1\n",
      "I0427 17:45:13.708240  1530 net.cpp:91] Creating Layer relu1\n",
      "I0427 17:45:13.708246  1530 net.cpp:425] relu1 <- ip1\n",
      "I0427 17:45:13.708255  1530 net.cpp:386] relu1 -> ip1 (in-place)\n",
      "I0427 17:45:13.708264  1530 net.cpp:141] Setting up relu1\n",
      "I0427 17:45:13.708272  1530 net.cpp:148] Top shape: 64 500 (32000)\n",
      "I0427 17:45:13.708278  1530 net.cpp:156] Memory required for data: 5167360\n",
      "I0427 17:45:13.708286  1530 layer_factory.hpp:77] Creating layer ip2\n",
      "I0427 17:45:13.708294  1530 net.cpp:91] Creating Layer ip2\n",
      "I0427 17:45:13.708300  1530 net.cpp:425] ip2 <- ip1\n",
      "I0427 17:45:13.708309  1530 net.cpp:399] ip2 -> ip2\n",
      "I0427 17:45:13.708354  1530 net.cpp:141] Setting up ip2\n",
      "I0427 17:45:13.708364  1530 net.cpp:148] Top shape: 64 10 (640)\n",
      "I0427 17:45:13.708370  1530 net.cpp:156] Memory required for data: 5169920\n",
      "I0427 17:45:13.708379  1530 layer_factory.hpp:77] Creating layer loss\n",
      "I0427 17:45:13.708389  1530 net.cpp:91] Creating Layer loss\n",
      "I0427 17:45:13.708395  1530 net.cpp:425] loss <- ip2\n",
      "I0427 17:45:13.708402  1530 net.cpp:425] loss <- label\n",
      "I0427 17:45:13.708411  1530 net.cpp:399] loss -> loss\n",
      "I0427 17:45:13.708423  1530 layer_factory.hpp:77] Creating layer loss\n",
      "I0427 17:45:13.708439  1530 net.cpp:141] Setting up loss\n",
      "I0427 17:45:13.708448  1530 net.cpp:148] Top shape: (1)\n",
      "I0427 17:45:13.708454  1530 net.cpp:151]     with loss weight 1\n",
      "I0427 17:45:13.708468  1530 net.cpp:156] Memory required for data: 5169924\n",
      "I0427 17:45:13.708475  1530 net.cpp:217] loss needs backward computation.\n",
      "I0427 17:45:13.708482  1530 net.cpp:217] ip2 needs backward computation.\n",
      "I0427 17:45:13.708489  1530 net.cpp:217] relu1 needs backward computation.\n",
      "I0427 17:45:13.708497  1530 net.cpp:217] ip1 needs backward computation.\n",
      "I0427 17:45:13.708503  1530 net.cpp:217] pool2 needs backward computation.\n",
      "I0427 17:45:13.708509  1530 net.cpp:217] conv2 needs backward computation.\n",
      "I0427 17:45:13.708516  1530 net.cpp:217] pool1 needs backward computation.\n",
      "I0427 17:45:13.708523  1530 net.cpp:217] conv1 needs backward computation.\n",
      "I0427 17:45:13.708530  1530 net.cpp:219] mnist does not need backward computation.\n",
      "I0427 17:45:13.708537  1530 net.cpp:261] This network produces output loss\n",
      "I0427 17:45:13.708547  1530 net.cpp:274] Network initialization done.\n",
      "I0427 17:45:13.708752  1530 solver.cpp:181] Creating test net (#0) specified by net file: ./lenet/lenet_train_test.prototxt\n",
      "I0427 17:45:13.708780  1530 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist\n",
      "I0427 17:45:13.708844  1530 net.cpp:49] Initializing net from parameters: \n",
      "name: \"LeNet\"\n",
      "state {\n",
      "  phase: TEST\n",
      "}\n",
      "layer {\n",
      "  name: \"mnist\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "  transform_param {\n",
      "    scale: 0.00390625\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"./lenet/mnist_test_lmdb\"\n",
      "    batch_size: 100\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 20\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 50\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"ip1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"ip1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 500\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"ip2\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 10\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"accuracy\"\n",
      "  type: \"Accuracy\"\n",
      "  bottom: \"ip2\"\n",
      "  bottom: \"label\"\n",
      "  top: \"accuracy\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"ip2\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I0427 17:45:13.709223  1530 layer_factory.hpp:77] Creating layer mnist\n",
      "I0427 17:45:13.709319  1530 net.cpp:91] Creating Layer mnist\n",
      "I0427 17:45:13.709342  1530 net.cpp:399] mnist -> data\n",
      "I0427 17:45:13.709354  1530 net.cpp:399] mnist -> label\n",
      "I0427 17:45:13.709404  1533 db_lmdb.cpp:38] Opened lmdb ./lenet/mnist_test_lmdb\n",
      "I0427 17:45:13.709447  1530 data_layer.cpp:41] output data size: 100,1,28,28\n",
      "I0427 17:45:13.709949  1530 net.cpp:141] Setting up mnist\n",
      "I0427 17:45:13.709964  1530 net.cpp:148] Top shape: 100 1 28 28 (78400)\n",
      "I0427 17:45:13.709974  1530 net.cpp:148] Top shape: 100 (100)\n",
      "I0427 17:45:13.709980  1530 net.cpp:156] Memory required for data: 314000\n",
      "I0427 17:45:13.709987  1530 layer_factory.hpp:77] Creating layer label_mnist_1_split\n",
      "I0427 17:45:13.710000  1530 net.cpp:91] Creating Layer label_mnist_1_split\n",
      "I0427 17:45:13.710008  1530 net.cpp:425] label_mnist_1_split <- label\n",
      "I0427 17:45:13.710017  1530 net.cpp:399] label_mnist_1_split -> label_mnist_1_split_0\n",
      "I0427 17:45:13.710027  1530 net.cpp:399] label_mnist_1_split -> label_mnist_1_split_1\n",
      "I0427 17:45:13.710038  1530 net.cpp:141] Setting up label_mnist_1_split\n",
      "I0427 17:45:13.710049  1530 net.cpp:148] Top shape: 100 (100)\n",
      "I0427 17:45:13.710057  1530 net.cpp:148] Top shape: 100 (100)\n",
      "I0427 17:45:13.710063  1530 net.cpp:156] Memory required for data: 314800\n",
      "I0427 17:45:13.710070  1530 layer_factory.hpp:77] Creating layer conv1\n",
      "I0427 17:45:13.710083  1530 net.cpp:91] Creating Layer conv1\n",
      "I0427 17:45:13.710090  1530 net.cpp:425] conv1 <- data\n",
      "I0427 17:45:13.710101  1530 net.cpp:399] conv1 -> conv1\n",
      "I0427 17:45:13.710129  1530 net.cpp:141] Setting up conv1\n",
      "I0427 17:45:13.710141  1530 net.cpp:148] Top shape: 100 20 24 24 (1152000)\n",
      "I0427 17:45:13.710147  1530 net.cpp:156] Memory required for data: 4922800\n",
      "I0427 17:45:13.710160  1530 layer_factory.hpp:77] Creating layer pool1\n",
      "I0427 17:45:13.710175  1530 net.cpp:91] Creating Layer pool1\n",
      "I0427 17:45:13.710181  1530 net.cpp:425] pool1 <- conv1\n",
      "I0427 17:45:13.710191  1530 net.cpp:399] pool1 -> pool1\n",
      "I0427 17:45:13.710214  1530 net.cpp:141] Setting up pool1\n",
      "I0427 17:45:13.710227  1530 net.cpp:148] Top shape: 100 20 12 12 (288000)\n",
      "I0427 17:45:13.710233  1530 net.cpp:156] Memory required for data: 6074800\n",
      "I0427 17:45:13.710240  1530 layer_factory.hpp:77] Creating layer conv2\n",
      "I0427 17:45:13.710253  1530 net.cpp:91] Creating Layer conv2\n",
      "I0427 17:45:13.710260  1530 net.cpp:425] conv2 <- pool1\n",
      "I0427 17:45:13.710271  1530 net.cpp:399] conv2 -> conv2\n",
      "I0427 17:45:13.710438  1530 net.cpp:141] Setting up conv2\n",
      "I0427 17:45:13.710449  1530 net.cpp:148] Top shape: 100 50 8 8 (320000)\n",
      "I0427 17:45:13.710456  1530 net.cpp:156] Memory required for data: 7354800\n",
      "I0427 17:45:13.710467  1530 layer_factory.hpp:77] Creating layer pool2\n",
      "I0427 17:45:13.710476  1530 net.cpp:91] Creating Layer pool2\n",
      "I0427 17:45:13.710484  1530 net.cpp:425] pool2 <- conv2\n",
      "I0427 17:45:13.710492  1530 net.cpp:399] pool2 -> pool2\n",
      "I0427 17:45:13.710505  1530 net.cpp:141] Setting up pool2\n",
      "I0427 17:45:13.710515  1530 net.cpp:148] Top shape: 100 50 4 4 (80000)\n",
      "I0427 17:45:13.710520  1530 net.cpp:156] Memory required for data: 7674800\n",
      "I0427 17:45:13.710527  1530 layer_factory.hpp:77] Creating layer ip1\n",
      "I0427 17:45:13.710536  1530 net.cpp:91] Creating Layer ip1\n",
      "I0427 17:45:13.710544  1530 net.cpp:425] ip1 <- pool2\n",
      "I0427 17:45:13.710554  1530 net.cpp:399] ip1 -> ip1\n",
      "I0427 17:45:13.712980  1530 net.cpp:141] Setting up ip1\n",
      "I0427 17:45:13.712996  1530 net.cpp:148] Top shape: 100 500 (50000)\n",
      "I0427 17:45:13.713002  1530 net.cpp:156] Memory required for data: 7874800\n",
      "I0427 17:45:13.713013  1530 layer_factory.hpp:77] Creating layer relu1\n",
      "I0427 17:45:13.713021  1530 net.cpp:91] Creating Layer relu1\n",
      "I0427 17:45:13.713029  1530 net.cpp:425] relu1 <- ip1\n",
      "I0427 17:45:13.713039  1530 net.cpp:386] relu1 -> ip1 (in-place)\n",
      "I0427 17:45:13.713049  1530 net.cpp:141] Setting up relu1\n",
      "I0427 17:45:13.713057  1530 net.cpp:148] Top shape: 100 500 (50000)\n",
      "I0427 17:45:13.713063  1530 net.cpp:156] Memory required for data: 8074800\n",
      "I0427 17:45:13.713069  1530 layer_factory.hpp:77] Creating layer ip2\n",
      "I0427 17:45:13.713079  1530 net.cpp:91] Creating Layer ip2\n",
      "I0427 17:45:13.713086  1530 net.cpp:425] ip2 <- ip1\n",
      "I0427 17:45:13.713096  1530 net.cpp:399] ip2 -> ip2\n",
      "I0427 17:45:13.713140  1530 net.cpp:141] Setting up ip2\n",
      "I0427 17:45:13.713150  1530 net.cpp:148] Top shape: 100 10 (1000)\n",
      "I0427 17:45:13.713156  1530 net.cpp:156] Memory required for data: 8078800\n",
      "I0427 17:45:13.713165  1530 layer_factory.hpp:77] Creating layer ip2_ip2_0_split\n",
      "I0427 17:45:13.713174  1530 net.cpp:91] Creating Layer ip2_ip2_0_split\n",
      "I0427 17:45:13.713182  1530 net.cpp:425] ip2_ip2_0_split <- ip2\n",
      "I0427 17:45:13.713191  1530 net.cpp:399] ip2_ip2_0_split -> ip2_ip2_0_split_0\n",
      "I0427 17:45:13.713199  1530 net.cpp:399] ip2_ip2_0_split -> ip2_ip2_0_split_1\n",
      "I0427 17:45:13.713208  1530 net.cpp:141] Setting up ip2_ip2_0_split\n",
      "I0427 17:45:13.713217  1530 net.cpp:148] Top shape: 100 10 (1000)\n",
      "I0427 17:45:13.713223  1530 net.cpp:148] Top shape: 100 10 (1000)\n",
      "I0427 17:45:13.713230  1530 net.cpp:156] Memory required for data: 8086800\n",
      "I0427 17:45:13.713237  1530 layer_factory.hpp:77] Creating layer accuracy\n",
      "I0427 17:45:13.713245  1530 net.cpp:91] Creating Layer accuracy\n",
      "I0427 17:45:13.713251  1530 net.cpp:425] accuracy <- ip2_ip2_0_split_0\n",
      "I0427 17:45:13.713259  1530 net.cpp:425] accuracy <- label_mnist_1_split_0\n",
      "I0427 17:45:13.713268  1530 net.cpp:399] accuracy -> accuracy\n",
      "I0427 17:45:13.713277  1530 net.cpp:141] Setting up accuracy\n",
      "I0427 17:45:13.713284  1530 net.cpp:148] Top shape: (1)\n",
      "I0427 17:45:13.713291  1530 net.cpp:156] Memory required for data: 8086804\n",
      "I0427 17:45:13.713299  1530 layer_factory.hpp:77] Creating layer loss\n",
      "I0427 17:45:13.713307  1530 net.cpp:91] Creating Layer loss\n",
      "I0427 17:45:13.713315  1530 net.cpp:425] loss <- ip2_ip2_0_split_1\n",
      "I0427 17:45:13.713322  1530 net.cpp:425] loss <- label_mnist_1_split_1\n",
      "I0427 17:45:13.713330  1530 net.cpp:399] loss -> loss\n",
      "I0427 17:45:13.713340  1530 layer_factory.hpp:77] Creating layer loss\n",
      "I0427 17:45:13.713356  1530 net.cpp:141] Setting up loss\n",
      "I0427 17:45:13.713366  1530 net.cpp:148] Top shape: (1)\n",
      "I0427 17:45:13.713383  1530 net.cpp:151]     with loss weight 1\n",
      "I0427 17:45:13.713393  1530 net.cpp:156] Memory required for data: 8086808\n",
      "I0427 17:45:13.713400  1530 net.cpp:217] loss needs backward computation.\n",
      "I0427 17:45:13.713407  1530 net.cpp:219] accuracy does not need backward computation.\n",
      "I0427 17:45:13.713414  1530 net.cpp:217] ip2_ip2_0_split needs backward computation.\n",
      "I0427 17:45:13.713421  1530 net.cpp:217] ip2 needs backward computation.\n",
      "I0427 17:45:13.713428  1530 net.cpp:217] relu1 needs backward computation.\n",
      "I0427 17:45:13.713434  1530 net.cpp:217] ip1 needs backward computation.\n",
      "I0427 17:45:13.713441  1530 net.cpp:217] pool2 needs backward computation.\n",
      "I0427 17:45:13.713448  1530 net.cpp:217] conv2 needs backward computation.\n",
      "I0427 17:45:13.713454  1530 net.cpp:217] pool1 needs backward computation.\n",
      "I0427 17:45:13.713461  1530 net.cpp:217] conv1 needs backward computation.\n",
      "I0427 17:45:13.713470  1530 net.cpp:219] label_mnist_1_split does not need backward computation.\n",
      "I0427 17:45:13.713479  1530 net.cpp:219] mnist does not need backward computation.\n",
      "I0427 17:45:13.713485  1530 net.cpp:261] This network produces output accuracy\n",
      "I0427 17:45:13.713491  1530 net.cpp:261] This network produces output loss\n",
      "I0427 17:45:13.713505  1530 net.cpp:274] Network initialization done.\n",
      "I0427 17:45:13.713539  1530 solver.cpp:60] Solver scaffolding done.\n",
      "I0427 17:45:13.713567  1530 caffe.cpp:209] Resuming from ./lenet/lenet_iter_5000.solverstate\n",
      "I0427 17:45:13.719728  1530 sgd_solver.cpp:318] SGDSolver: restoring history\n",
      "I0427 17:45:13.720230  1530 caffe.cpp:219] Starting Optimization\n",
      "I0427 17:45:13.720244  1530 solver.cpp:279] Solving LeNet\n",
      "I0427 17:45:13.720252  1530 solver.cpp:280] Learning Rate Policy: inv\n",
      "I0427 17:45:13.720624  1530 solver.cpp:337] Iteration 5000, Testing net (#0)\n",
      "I0427 17:45:17.819998  1530 solver.cpp:404]     Test net output #0: accuracy = 0.9902\n",
      "I0427 17:45:17.820058  1530 solver.cpp:404]     Test net output #1: loss = 0.0304041 (* 1 = 0.0304041 loss)\n",
      "I0427 17:45:17.890264  1530 solver.cpp:228] Iteration 5000, loss = 0.0076677\n",
      "I0427 17:45:17.890321  1530 solver.cpp:244]     Train net output #0: loss = 0.0076677 (* 1 = 0.0076677 loss)\n",
      "I0427 17:45:17.890338  1530 sgd_solver.cpp:106] Iteration 5000, lr = 0.00737788\n",
      "I0427 17:45:24.651957  1530 solver.cpp:228] Iteration 5100, loss = 0.0147798\n",
      "I0427 17:45:24.652020  1530 solver.cpp:244]     Train net output #0: loss = 0.0147798 (* 1 = 0.0147798 loss)\n",
      "I0427 17:45:24.652031  1530 sgd_solver.cpp:106] Iteration 5100, lr = 0.0073412\n",
      "I0427 17:45:31.391093  1530 solver.cpp:228] Iteration 5200, loss = 0.0194539\n",
      "I0427 17:45:31.391155  1530 solver.cpp:244]     Train net output #0: loss = 0.0194539 (* 1 = 0.0194539 loss)\n",
      "I0427 17:45:31.391166  1530 sgd_solver.cpp:106] Iteration 5200, lr = 0.00730495\n",
      "I0427 17:45:38.114811  1530 solver.cpp:228] Iteration 5300, loss = 0.00602482\n",
      "I0427 17:45:38.114873  1530 solver.cpp:244]     Train net output #0: loss = 0.00602481 (* 1 = 0.00602481 loss)\n",
      "I0427 17:45:38.114884  1530 sgd_solver.cpp:106] Iteration 5300, lr = 0.00726911\n",
      "I0427 17:45:44.839052  1530 solver.cpp:228] Iteration 5400, loss = 0.00876122\n",
      "I0427 17:45:44.839242  1530 solver.cpp:244]     Train net output #0: loss = 0.0087612 (* 1 = 0.0087612 loss)\n",
      "I0427 17:45:44.839257  1530 sgd_solver.cpp:106] Iteration 5400, lr = 0.00723368\n",
      "I0427 17:45:51.497380  1530 solver.cpp:337] Iteration 5500, Testing net (#0)\n",
      "I0427 17:45:55.568485  1530 solver.cpp:404]     Test net output #0: accuracy = 0.99\n",
      "I0427 17:45:55.568547  1530 solver.cpp:404]     Test net output #1: loss = 0.0310209 (* 1 = 0.0310209 loss)\n",
      "I0427 17:45:55.638567  1530 solver.cpp:228] Iteration 5500, loss = 0.00890913\n",
      "I0427 17:45:55.638609  1530 solver.cpp:244]     Train net output #0: loss = 0.00890913 (* 1 = 0.00890913 loss)\n",
      "I0427 17:45:55.638623  1530 sgd_solver.cpp:106] Iteration 5500, lr = 0.00719865\n",
      "I0427 17:46:02.371999  1530 solver.cpp:228] Iteration 5600, loss = 0.0274219\n",
      "I0427 17:46:02.372051  1530 solver.cpp:244]     Train net output #0: loss = 0.0274219 (* 1 = 0.0274219 loss)\n",
      "I0427 17:46:02.372061  1530 sgd_solver.cpp:106] Iteration 5600, lr = 0.00716402\n",
      "I0427 17:46:09.101927  1530 solver.cpp:228] Iteration 5700, loss = 0.015893\n",
      "I0427 17:46:09.101989  1530 solver.cpp:244]     Train net output #0: loss = 0.015893 (* 1 = 0.015893 loss)\n",
      "I0427 17:46:09.101999  1530 sgd_solver.cpp:106] Iteration 5700, lr = 0.00712977\n",
      "I0427 17:46:16.009802  1530 solver.cpp:228] Iteration 5800, loss = 0.0487397\n",
      "I0427 17:46:16.009999  1530 solver.cpp:244]     Train net output #0: loss = 0.0487397 (* 1 = 0.0487397 loss)\n",
      "I0427 17:46:16.010018  1530 sgd_solver.cpp:106] Iteration 5800, lr = 0.0070959\n",
      "I0427 17:46:23.230331  1530 solver.cpp:228] Iteration 5900, loss = 0.0174426\n",
      "I0427 17:46:23.230398  1530 solver.cpp:244]     Train net output #0: loss = 0.0174426 (* 1 = 0.0174426 loss)\n",
      "I0427 17:46:23.230411  1530 sgd_solver.cpp:106] Iteration 5900, lr = 0.0070624\n",
      "I0427 17:46:30.094846  1530 solver.cpp:337] Iteration 6000, Testing net (#0)\n",
      "I0427 17:46:34.321378  1530 solver.cpp:404]     Test net output #0: accuracy = 0.9893\n",
      "I0427 17:46:34.321435  1530 solver.cpp:404]     Test net output #1: loss = 0.031535 (* 1 = 0.031535 loss)\n",
      "I0427 17:46:34.389155  1530 solver.cpp:228] Iteration 6000, loss = 0.0108712\n",
      "I0427 17:46:34.389196  1530 solver.cpp:244]     Train net output #0: loss = 0.0108712 (* 1 = 0.0108712 loss)\n",
      "I0427 17:46:34.389209  1530 sgd_solver.cpp:106] Iteration 6000, lr = 0.00702927\n",
      "I0427 17:46:41.350255  1530 solver.cpp:228] Iteration 6100, loss = 0.000772152\n",
      "I0427 17:46:41.350312  1530 solver.cpp:244]     Train net output #0: loss = 0.000772159 (* 1 = 0.000772159 loss)\n",
      "I0427 17:46:41.350323  1530 sgd_solver.cpp:106] Iteration 6100, lr = 0.0069965\n",
      "I0427 17:46:48.326719  1530 solver.cpp:228] Iteration 6200, loss = 0.00169094\n",
      "I0427 17:46:48.326843  1530 solver.cpp:244]     Train net output #0: loss = 0.00169094 (* 1 = 0.00169094 loss)\n",
      "I0427 17:46:48.326855  1530 sgd_solver.cpp:106] Iteration 6200, lr = 0.00696408\n",
      "I0427 17:46:55.294082  1530 solver.cpp:228] Iteration 6300, loss = 0.0034143\n",
      "I0427 17:46:55.294145  1530 solver.cpp:244]     Train net output #0: loss = 0.0034143 (* 1 = 0.0034143 loss)\n",
      "I0427 17:46:55.294157  1530 sgd_solver.cpp:106] Iteration 6300, lr = 0.00693201\n",
      "I0427 17:47:02.259126  1530 solver.cpp:228] Iteration 6400, loss = 0.00146135\n",
      "I0427 17:47:02.259186  1530 solver.cpp:244]     Train net output #0: loss = 0.00146135 (* 1 = 0.00146135 loss)\n",
      "I0427 17:47:02.259197  1530 sgd_solver.cpp:106] Iteration 6400, lr = 0.00690029\n",
      "I0427 17:47:09.174983  1530 solver.cpp:337] Iteration 6500, Testing net (#0)\n",
      "I0427 17:47:13.350828  1530 solver.cpp:404]     Test net output #0: accuracy = 0.9902\n",
      "I0427 17:47:13.350891  1530 solver.cpp:404]     Test net output #1: loss = 0.0299224 (* 1 = 0.0299224 loss)\n",
      "I0427 17:47:13.420075  1530 solver.cpp:228] Iteration 6500, loss = 0.02349\n",
      "I0427 17:47:13.420120  1530 solver.cpp:244]     Train net output #0: loss = 0.02349 (* 1 = 0.02349 loss)\n",
      "I0427 17:47:13.420132  1530 sgd_solver.cpp:106] Iteration 6500, lr = 0.0068689\n",
      "I0427 17:47:20.331462  1530 solver.cpp:228] Iteration 6600, loss = 0.0115372\n",
      "I0427 17:47:20.331652  1530 solver.cpp:244]     Train net output #0: loss = 0.0115373 (* 1 = 0.0115373 loss)\n",
      "I0427 17:47:20.331665  1530 sgd_solver.cpp:106] Iteration 6600, lr = 0.00683784\n",
      "I0427 17:47:27.231168  1530 solver.cpp:228] Iteration 6700, loss = 0.00513985\n",
      "I0427 17:47:27.231227  1530 solver.cpp:244]     Train net output #0: loss = 0.00513987 (* 1 = 0.00513987 loss)\n",
      "I0427 17:47:27.231238  1530 sgd_solver.cpp:106] Iteration 6700, lr = 0.00680711\n",
      "I0427 17:47:34.170649  1530 solver.cpp:228] Iteration 6800, loss = 0.00764375\n",
      "I0427 17:47:34.170709  1530 solver.cpp:244]     Train net output #0: loss = 0.00764377 (* 1 = 0.00764377 loss)\n",
      "I0427 17:47:34.170720  1530 sgd_solver.cpp:106] Iteration 6800, lr = 0.0067767\n",
      "I0427 17:47:41.152437  1530 solver.cpp:228] Iteration 6900, loss = 0.0370393\n",
      "I0427 17:47:41.152498  1530 solver.cpp:244]     Train net output #0: loss = 0.0370394 (* 1 = 0.0370394 loss)\n",
      "I0427 17:47:41.152509  1530 sgd_solver.cpp:106] Iteration 6900, lr = 0.0067466\n",
      "I0427 17:47:48.076433  1530 solver.cpp:337] Iteration 7000, Testing net (#0)\n",
      "I0427 17:47:52.233048  1530 solver.cpp:404]     Test net output #0: accuracy = 0.9892\n",
      "I0427 17:47:52.233141  1530 solver.cpp:404]     Test net output #1: loss = 0.0346584 (* 1 = 0.0346584 loss)\n",
      "I0427 17:47:52.301165  1530 solver.cpp:228] Iteration 7000, loss = 0.00639962\n",
      "I0427 17:47:52.301213  1530 solver.cpp:244]     Train net output #0: loss = 0.00639966 (* 1 = 0.00639966 loss)\n",
      "I0427 17:47:52.301224  1530 sgd_solver.cpp:106] Iteration 7000, lr = 0.00671681\n",
      "I0427 17:47:59.248271  1530 solver.cpp:228] Iteration 7100, loss = 0.00334766\n",
      "I0427 17:47:59.248332  1530 solver.cpp:244]     Train net output #0: loss = 0.00334769 (* 1 = 0.00334769 loss)\n",
      "I0427 17:47:59.248342  1530 sgd_solver.cpp:106] Iteration 7100, lr = 0.00668733\n",
      "I0427 17:48:06.218840  1530 solver.cpp:228] Iteration 7200, loss = 0.00327869\n",
      "I0427 17:48:06.218900  1530 solver.cpp:244]     Train net output #0: loss = 0.00327871 (* 1 = 0.00327871 loss)\n",
      "I0427 17:48:06.218912  1530 sgd_solver.cpp:106] Iteration 7200, lr = 0.00665815\n",
      "I0427 17:48:13.175864  1530 solver.cpp:228] Iteration 7300, loss = 0.0273182\n",
      "I0427 17:48:13.175925  1530 solver.cpp:244]     Train net output #0: loss = 0.0273183 (* 1 = 0.0273183 loss)\n",
      "I0427 17:48:13.175936  1530 sgd_solver.cpp:106] Iteration 7300, lr = 0.00662927\n",
      "I0427 17:48:20.156206  1530 solver.cpp:228] Iteration 7400, loss = 0.00688362\n",
      "I0427 17:48:20.156260  1530 solver.cpp:244]     Train net output #0: loss = 0.00688363 (* 1 = 0.00688363 loss)\n",
      "I0427 17:48:20.156270  1530 sgd_solver.cpp:106] Iteration 7400, lr = 0.00660067\n",
      "I0427 17:48:27.032243  1530 solver.cpp:337] Iteration 7500, Testing net (#0)\n",
      "I0427 17:48:31.219727  1530 solver.cpp:404]     Test net output #0: accuracy = 0.99\n",
      "I0427 17:48:31.219784  1530 solver.cpp:404]     Test net output #1: loss = 0.0301615 (* 1 = 0.0301615 loss)\n",
      "I0427 17:48:31.289167  1530 solver.cpp:228] Iteration 7500, loss = 0.00466027\n",
      "I0427 17:48:31.289212  1530 solver.cpp:244]     Train net output #0: loss = 0.00466028 (* 1 = 0.00466028 loss)\n",
      "I0427 17:48:31.289225  1530 sgd_solver.cpp:106] Iteration 7500, lr = 0.00657236\n",
      "I0427 17:48:38.214725  1530 solver.cpp:228] Iteration 7600, loss = 0.0113153\n",
      "I0427 17:48:38.214781  1530 solver.cpp:244]     Train net output #0: loss = 0.0113153 (* 1 = 0.0113153 loss)\n",
      "I0427 17:48:38.214792  1530 sgd_solver.cpp:106] Iteration 7600, lr = 0.00654433\n",
      "I0427 17:48:45.200309  1530 solver.cpp:228] Iteration 7700, loss = 0.0176158\n",
      "I0427 17:48:45.200361  1530 solver.cpp:244]     Train net output #0: loss = 0.0176158 (* 1 = 0.0176158 loss)\n",
      "I0427 17:48:45.200374  1530 sgd_solver.cpp:106] Iteration 7700, lr = 0.00651658\n",
      "I0427 17:48:52.146598  1530 solver.cpp:228] Iteration 7800, loss = 0.000491032\n",
      "I0427 17:48:52.146651  1530 solver.cpp:244]     Train net output #0: loss = 0.000491064 (* 1 = 0.000491064 loss)\n",
      "I0427 17:48:52.146664  1530 sgd_solver.cpp:106] Iteration 7800, lr = 0.00648911\n",
      "I0427 17:48:59.164958  1530 solver.cpp:228] Iteration 7900, loss = 0.00385922\n",
      "I0427 17:48:59.165169  1530 solver.cpp:244]     Train net output #0: loss = 0.00385924 (* 1 = 0.00385924 loss)\n",
      "I0427 17:48:59.165191  1530 sgd_solver.cpp:106] Iteration 7900, lr = 0.0064619\n",
      "I0427 17:49:06.279656  1530 solver.cpp:337] Iteration 8000, Testing net (#0)\n",
      "I0427 17:49:10.730276  1530 solver.cpp:404]     Test net output #0: accuracy = 0.9894\n",
      "I0427 17:49:10.730335  1530 solver.cpp:404]     Test net output #1: loss = 0.0323762 (* 1 = 0.0323762 loss)\n",
      "I0427 17:49:10.799837  1530 solver.cpp:228] Iteration 8000, loss = 0.0084533\n",
      "I0427 17:49:10.799886  1530 solver.cpp:244]     Train net output #0: loss = 0.00845332 (* 1 = 0.00845332 loss)\n",
      "I0427 17:49:10.799899  1530 sgd_solver.cpp:106] Iteration 8000, lr = 0.00643496\n",
      "I0427 17:49:17.771237  1530 solver.cpp:228] Iteration 8100, loss = 0.00636842\n",
      "I0427 17:49:17.771291  1530 solver.cpp:244]     Train net output #0: loss = 0.00636843 (* 1 = 0.00636843 loss)\n",
      "I0427 17:49:17.771303  1530 sgd_solver.cpp:106] Iteration 8100, lr = 0.00640827\n",
      "I0427 17:49:24.717530  1530 solver.cpp:228] Iteration 8200, loss = 0.00551724\n",
      "I0427 17:49:24.717622  1530 solver.cpp:244]     Train net output #0: loss = 0.00551725 (* 1 = 0.00551725 loss)\n",
      "I0427 17:49:24.717633  1530 sgd_solver.cpp:106] Iteration 8200, lr = 0.00638185\n",
      "I0427 17:49:31.696172  1530 solver.cpp:228] Iteration 8300, loss = 0.00239607\n",
      "I0427 17:49:31.696308  1530 solver.cpp:244]     Train net output #0: loss = 0.00239608 (* 1 = 0.00239608 loss)\n",
      "I0427 17:49:31.696321  1530 sgd_solver.cpp:106] Iteration 8300, lr = 0.00635567\n",
      "I0427 17:49:38.708418  1530 solver.cpp:228] Iteration 8400, loss = 0.00345158\n",
      "I0427 17:49:38.708483  1530 solver.cpp:244]     Train net output #0: loss = 0.0034516 (* 1 = 0.0034516 loss)\n",
      "I0427 17:49:38.708494  1530 sgd_solver.cpp:106] Iteration 8400, lr = 0.00632975\n",
      "I0427 17:49:45.529392  1530 solver.cpp:337] Iteration 8500, Testing net (#0)\n",
      "I0427 17:49:49.739779  1530 solver.cpp:404]     Test net output #0: accuracy = 0.9885\n",
      "I0427 17:49:49.739832  1530 solver.cpp:404]     Test net output #1: loss = 0.0345999 (* 1 = 0.0345999 loss)\n",
      "I0427 17:49:49.811391  1530 solver.cpp:228] Iteration 8500, loss = 0.00442299\n",
      "I0427 17:49:49.811447  1530 solver.cpp:244]     Train net output #0: loss = 0.00442301 (* 1 = 0.00442301 loss)\n",
      "I0427 17:49:49.811461  1530 sgd_solver.cpp:106] Iteration 8500, lr = 0.00630407\n",
      "I0427 17:49:56.770063  1530 solver.cpp:228] Iteration 8600, loss = 0.0120373\n",
      "I0427 17:49:56.770117  1530 solver.cpp:244]     Train net output #0: loss = 0.0120373 (* 1 = 0.0120373 loss)\n",
      "I0427 17:49:56.770128  1530 sgd_solver.cpp:106] Iteration 8600, lr = 0.00627864\n",
      "I0427 17:50:03.713593  1530 solver.cpp:228] Iteration 8700, loss = 0.0118002\n",
      "I0427 17:50:03.713724  1530 solver.cpp:244]     Train net output #0: loss = 0.0118002 (* 1 = 0.0118002 loss)\n",
      "I0427 17:50:03.713737  1530 sgd_solver.cpp:106] Iteration 8700, lr = 0.00625344\n",
      "I0427 17:50:10.587086  1530 solver.cpp:228] Iteration 8800, loss = 0.00378763\n",
      "I0427 17:50:10.587146  1530 solver.cpp:244]     Train net output #0: loss = 0.00378766 (* 1 = 0.00378766 loss)\n",
      "I0427 17:50:10.587155  1530 sgd_solver.cpp:106] Iteration 8800, lr = 0.00622847\n",
      "I0427 17:50:17.468952  1530 solver.cpp:228] Iteration 8900, loss = 0.00538051\n",
      "I0427 17:50:17.469017  1530 solver.cpp:244]     Train net output #0: loss = 0.00538055 (* 1 = 0.00538055 loss)\n",
      "I0427 17:50:17.469028  1530 sgd_solver.cpp:106] Iteration 8900, lr = 0.00620374\n",
      "I0427 17:50:24.290794  1530 solver.cpp:337] Iteration 9000, Testing net (#0)\n",
      "I0427 17:50:28.493734  1530 solver.cpp:404]     Test net output #0: accuracy = 0.9907\n",
      "I0427 17:50:28.493798  1530 solver.cpp:404]     Test net output #1: loss = 0.0295613 (* 1 = 0.0295613 loss)\n",
      "I0427 17:50:28.564144  1530 solver.cpp:228] Iteration 9000, loss = 0.00604891\n",
      "I0427 17:50:28.564188  1530 solver.cpp:244]     Train net output #0: loss = 0.00604894 (* 1 = 0.00604894 loss)\n",
      "I0427 17:50:28.564201  1530 sgd_solver.cpp:106] Iteration 9000, lr = 0.00617924\n",
      "I0427 17:50:35.452587  1530 solver.cpp:228] Iteration 9100, loss = 0.00554662\n",
      "I0427 17:50:35.452677  1530 solver.cpp:244]     Train net output #0: loss = 0.00554665 (* 1 = 0.00554665 loss)\n",
      "I0427 17:50:35.452688  1530 sgd_solver.cpp:106] Iteration 9100, lr = 0.00615496\n",
      "I0427 17:50:42.366819  1530 solver.cpp:228] Iteration 9200, loss = 0.00446554\n",
      "I0427 17:50:42.366885  1530 solver.cpp:244]     Train net output #0: loss = 0.00446557 (* 1 = 0.00446557 loss)\n",
      "I0427 17:50:42.366896  1530 sgd_solver.cpp:106] Iteration 9200, lr = 0.0061309\n",
      "I0427 17:50:49.319290  1530 solver.cpp:228] Iteration 9300, loss = 0.0115586\n",
      "I0427 17:50:49.319352  1530 solver.cpp:244]     Train net output #0: loss = 0.0115586 (* 1 = 0.0115586 loss)\n",
      "I0427 17:50:49.319363  1530 sgd_solver.cpp:106] Iteration 9300, lr = 0.00610706\n",
      "I0427 17:50:56.287519  1530 solver.cpp:228] Iteration 9400, loss = 0.0064912\n",
      "I0427 17:50:56.287578  1530 solver.cpp:244]     Train net output #0: loss = 0.00649124 (* 1 = 0.00649124 loss)\n",
      "I0427 17:50:56.287590  1530 sgd_solver.cpp:106] Iteration 9400, lr = 0.00608343\n",
      "I0427 17:51:03.232846  1530 solver.cpp:337] Iteration 9500, Testing net (#0)\n",
      "I0427 17:51:07.428465  1530 solver.cpp:404]     Test net output #0: accuracy = 0.9898\n",
      "I0427 17:51:07.428695  1530 solver.cpp:404]     Test net output #1: loss = 0.0319065 (* 1 = 0.0319065 loss)\n",
      "I0427 17:51:07.499553  1530 solver.cpp:228] Iteration 9500, loss = 0.00327267\n",
      "I0427 17:51:07.499610  1530 solver.cpp:244]     Train net output #0: loss = 0.0032727 (* 1 = 0.0032727 loss)\n",
      "I0427 17:51:07.499625  1530 sgd_solver.cpp:106] Iteration 9500, lr = 0.00606002\n",
      "I0427 17:51:14.734372  1530 solver.cpp:228] Iteration 9600, loss = 0.00859552\n",
      "I0427 17:51:14.734449  1530 solver.cpp:244]     Train net output #0: loss = 0.00859555 (* 1 = 0.00859555 loss)\n",
      "I0427 17:51:14.734462  1530 sgd_solver.cpp:106] Iteration 9600, lr = 0.00603682\n",
      "I0427 17:51:22.021895  1530 solver.cpp:228] Iteration 9700, loss = 0.00121225\n",
      "I0427 17:51:22.021971  1530 solver.cpp:244]     Train net output #0: loss = 0.0012123 (* 1 = 0.0012123 loss)\n",
      "I0427 17:51:22.021983  1530 sgd_solver.cpp:106] Iteration 9700, lr = 0.00601382\n",
      "I0427 17:51:29.276418  1530 solver.cpp:228] Iteration 9800, loss = 0.00641194\n",
      "I0427 17:51:29.276494  1530 solver.cpp:244]     Train net output #0: loss = 0.00641199 (* 1 = 0.00641199 loss)\n",
      "I0427 17:51:29.276509  1530 sgd_solver.cpp:106] Iteration 9800, lr = 0.00599102\n",
      "I0427 17:51:36.710583  1530 solver.cpp:228] Iteration 9900, loss = 0.0014144\n",
      "I0427 17:51:36.710655  1530 solver.cpp:244]     Train net output #0: loss = 0.00141444 (* 1 = 0.00141444 loss)\n",
      "I0427 17:51:36.710665  1530 sgd_solver.cpp:106] Iteration 9900, lr = 0.00596843\n",
      "I0427 17:51:43.567476  1530 solver.cpp:454] Snapshotting to binary proto file ./lenet/lenet_iter_10000.caffemodel\n",
      "I0427 17:51:43.572794  1530 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./lenet/lenet_iter_10000.solverstate\n",
      "I0427 17:51:43.603096  1530 solver.cpp:317] Iteration 10000, loss = 0.011584\n",
      "I0427 17:51:43.603126  1530 solver.cpp:337] Iteration 10000, Testing net (#0)\n",
      "I0427 17:51:47.755620  1530 solver.cpp:404]     Test net output #0: accuracy = 0.9913\n",
      "I0427 17:51:47.755667  1530 solver.cpp:404]     Test net output #1: loss = 0.0286597 (* 1 = 0.0286597 loss)\n",
      "I0427 17:51:47.755678  1530 solver.cpp:322] Optimization Done.\n",
      "I0427 17:51:47.755684  1530 caffe.cpp:222] Optimization Done.\n"
     ]
    }
   ],
   "source": [
    "!./caffe/build/tools/caffe train --solver=./lenet/lenet_solver.prototxt --snapshot=./lenet/lenet_iter_5000.solverstate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본 과정이 모두 완료되면 10000 minibatch의 학습 후 Test dataset에서 약 99.1%의 인식 성능을 갖는 모델(./lenet/lenet_iter_10000.caffemodel)을 얻을 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_mnist.sh\t\t      lenet_iter_5000.caffemodel   mnist.pyc\r\n",
      "lenet.prototxt\t\t      lenet_iter_5000.solverstate  mnist_test_lmdb\r\n",
      "lenet5.png\t\t      lenet_solver.prototxt\t   mnist_train_lmdb\r\n",
      "lenet_iter_10000.caffemodel   lenet_train_test.prototxt\r\n",
      "lenet_iter_10000.solverstate  mnist.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./lenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Testing LeNet 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "학습된 모델으로부터 Test set에 대한 성능을 측정해보겠습니다.\n",
    "MNIST dataset의 test sample 개수가 10000개이며, lenet_train_test.prototxt의 test batchsize가 100개로 설정되어 있으므로 총 100번의 Iteration을 수행하면 됩니다.\n",
    "\n",
    "즉 ./caffe/build/tools/caffe를 실행시킬때,\n",
    "- 학습된 모델을 사용하여 inference를 하기에 __test__의 인자를 넣어야 하며,\n",
    "- 학습된 모델의 definition prototxt __./lenet/lenet_train_test.prototxt__\n",
    "- 학습된 모델의 weights __./lenet/lenet_iter_10000.caffemodel__\n",
    "을 차례로 적용하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libdc1394 error: Failed to initialize libdc1394\n",
      "I0427 17:51:48.071506  1538 caffe.cpp:246] Use CPU.\n",
      "I0427 17:51:48.072994  1538 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist\n",
      "I0427 17:51:48.073102  1538 net.cpp:49] Initializing net from parameters: \n",
      "name: \"LeNet\"\n",
      "state {\n",
      "  phase: TEST\n",
      "}\n",
      "layer {\n",
      "  name: \"mnist\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "  transform_param {\n",
      "    scale: 0.00390625\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"./lenet/mnist_test_lmdb\"\n",
      "    batch_size: 100\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 20\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 50\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"ip1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"ip1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 500\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"ip2\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 10\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"accuracy\"\n",
      "  type: \"Accuracy\"\n",
      "  bottom: \"ip2\"\n",
      "  bottom: \"label\"\n",
      "  top: \"accuracy\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"ip2\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I0427 17:51:48.073504  1538 layer_factory.hpp:77] Creating layer mnist\n",
      "I0427 17:51:48.073963  1538 net.cpp:91] Creating Layer mnist\n",
      "I0427 17:51:48.073983  1538 net.cpp:399] mnist -> data\n",
      "I0427 17:51:48.074018  1538 net.cpp:399] mnist -> label\n",
      "I0427 17:51:48.074075  1539 db_lmdb.cpp:38] Opened lmdb ./lenet/mnist_test_lmdb\n",
      "I0427 17:51:48.074131  1538 data_layer.cpp:41] output data size: 100,1,28,28\n",
      "I0427 17:51:48.074625  1538 net.cpp:141] Setting up mnist\n",
      "I0427 17:51:48.074648  1538 net.cpp:148] Top shape: 100 1 28 28 (78400)\n",
      "I0427 17:51:48.074658  1538 net.cpp:148] Top shape: 100 (100)\n",
      "I0427 17:51:48.074666  1538 net.cpp:156] Memory required for data: 314000\n",
      "I0427 17:51:48.074676  1538 layer_factory.hpp:77] Creating layer label_mnist_1_split\n",
      "I0427 17:51:48.074687  1538 net.cpp:91] Creating Layer label_mnist_1_split\n",
      "I0427 17:51:48.074694  1538 net.cpp:425] label_mnist_1_split <- label\n",
      "I0427 17:51:48.074705  1538 net.cpp:399] label_mnist_1_split -> label_mnist_1_split_0\n",
      "I0427 17:51:48.074717  1538 net.cpp:399] label_mnist_1_split -> label_mnist_1_split_1\n",
      "I0427 17:51:48.074733  1538 net.cpp:141] Setting up label_mnist_1_split\n",
      "I0427 17:51:48.074741  1538 net.cpp:148] Top shape: 100 (100)\n",
      "I0427 17:51:48.074749  1538 net.cpp:148] Top shape: 100 (100)\n",
      "I0427 17:51:48.074756  1538 net.cpp:156] Memory required for data: 314800\n",
      "I0427 17:51:48.074764  1538 layer_factory.hpp:77] Creating layer conv1\n",
      "I0427 17:51:48.074779  1538 net.cpp:91] Creating Layer conv1\n",
      "I0427 17:51:48.074787  1538 net.cpp:425] conv1 <- data\n",
      "I0427 17:51:48.074796  1538 net.cpp:399] conv1 -> conv1\n",
      "I0427 17:51:48.074846  1538 net.cpp:141] Setting up conv1\n",
      "I0427 17:51:48.074859  1538 net.cpp:148] Top shape: 100 20 24 24 (1152000)\n",
      "I0427 17:51:48.074869  1538 net.cpp:156] Memory required for data: 4922800\n",
      "I0427 17:51:48.074885  1538 layer_factory.hpp:77] Creating layer pool1\n",
      "I0427 17:51:48.074910  1538 net.cpp:91] Creating Layer pool1\n",
      "I0427 17:51:48.074918  1538 net.cpp:425] pool1 <- conv1\n",
      "I0427 17:51:48.074928  1538 net.cpp:399] pool1 -> pool1\n",
      "I0427 17:51:48.074947  1538 net.cpp:141] Setting up pool1\n",
      "I0427 17:51:48.074957  1538 net.cpp:148] Top shape: 100 20 12 12 (288000)\n",
      "I0427 17:51:48.074965  1538 net.cpp:156] Memory required for data: 6074800\n",
      "I0427 17:51:48.074971  1538 layer_factory.hpp:77] Creating layer conv2\n",
      "I0427 17:51:48.074982  1538 net.cpp:91] Creating Layer conv2\n",
      "I0427 17:51:48.074990  1538 net.cpp:425] conv2 <- pool1\n",
      "I0427 17:51:48.074998  1538 net.cpp:399] conv2 -> conv2\n",
      "I0427 17:51:48.075172  1538 net.cpp:141] Setting up conv2\n",
      "I0427 17:51:48.075184  1538 net.cpp:148] Top shape: 100 50 8 8 (320000)\n",
      "I0427 17:51:48.075191  1538 net.cpp:156] Memory required for data: 7354800\n",
      "I0427 17:51:48.075202  1538 layer_factory.hpp:77] Creating layer pool2\n",
      "I0427 17:51:48.075212  1538 net.cpp:91] Creating Layer pool2\n",
      "I0427 17:51:48.075219  1538 net.cpp:425] pool2 <- conv2\n",
      "I0427 17:51:48.075227  1538 net.cpp:399] pool2 -> pool2\n",
      "I0427 17:51:48.075239  1538 net.cpp:141] Setting up pool2\n",
      "I0427 17:51:48.075248  1538 net.cpp:148] Top shape: 100 50 4 4 (80000)\n",
      "I0427 17:51:48.075254  1538 net.cpp:156] Memory required for data: 7674800\n",
      "I0427 17:51:48.075261  1538 layer_factory.hpp:77] Creating layer ip1\n",
      "I0427 17:51:48.075271  1538 net.cpp:91] Creating Layer ip1\n",
      "I0427 17:51:48.075278  1538 net.cpp:425] ip1 <- pool2\n",
      "I0427 17:51:48.075287  1538 net.cpp:399] ip1 -> ip1\n",
      "I0427 17:51:48.077690  1538 net.cpp:141] Setting up ip1\n",
      "I0427 17:51:48.077708  1538 net.cpp:148] Top shape: 100 500 (50000)\n",
      "I0427 17:51:48.077714  1538 net.cpp:156] Memory required for data: 7874800\n",
      "I0427 17:51:48.077726  1538 layer_factory.hpp:77] Creating layer relu1\n",
      "I0427 17:51:48.077735  1538 net.cpp:91] Creating Layer relu1\n",
      "I0427 17:51:48.077742  1538 net.cpp:425] relu1 <- ip1\n",
      "I0427 17:51:48.077751  1538 net.cpp:386] relu1 -> ip1 (in-place)\n",
      "I0427 17:51:48.077761  1538 net.cpp:141] Setting up relu1\n",
      "I0427 17:51:48.077769  1538 net.cpp:148] Top shape: 100 500 (50000)\n",
      "I0427 17:51:48.077775  1538 net.cpp:156] Memory required for data: 8074800\n",
      "I0427 17:51:48.077782  1538 layer_factory.hpp:77] Creating layer ip2\n",
      "I0427 17:51:48.077791  1538 net.cpp:91] Creating Layer ip2\n",
      "I0427 17:51:48.077798  1538 net.cpp:425] ip2 <- ip1\n",
      "I0427 17:51:48.077807  1538 net.cpp:399] ip2 -> ip2\n",
      "I0427 17:51:48.077855  1538 net.cpp:141] Setting up ip2\n",
      "I0427 17:51:48.077865  1538 net.cpp:148] Top shape: 100 10 (1000)\n",
      "I0427 17:51:48.077872  1538 net.cpp:156] Memory required for data: 8078800\n",
      "I0427 17:51:48.077882  1538 layer_factory.hpp:77] Creating layer ip2_ip2_0_split\n",
      "I0427 17:51:48.077889  1538 net.cpp:91] Creating Layer ip2_ip2_0_split\n",
      "I0427 17:51:48.077903  1538 net.cpp:425] ip2_ip2_0_split <- ip2\n",
      "I0427 17:51:48.077910  1538 net.cpp:399] ip2_ip2_0_split -> ip2_ip2_0_split_0\n",
      "I0427 17:51:48.077919  1538 net.cpp:399] ip2_ip2_0_split -> ip2_ip2_0_split_1\n",
      "I0427 17:51:48.077930  1538 net.cpp:141] Setting up ip2_ip2_0_split\n",
      "I0427 17:51:48.077939  1538 net.cpp:148] Top shape: 100 10 (1000)\n",
      "I0427 17:51:48.077945  1538 net.cpp:148] Top shape: 100 10 (1000)\n",
      "I0427 17:51:48.077952  1538 net.cpp:156] Memory required for data: 8086800\n",
      "I0427 17:51:48.077960  1538 layer_factory.hpp:77] Creating layer accuracy\n",
      "I0427 17:51:48.077971  1538 net.cpp:91] Creating Layer accuracy\n",
      "I0427 17:51:48.077978  1538 net.cpp:425] accuracy <- ip2_ip2_0_split_0\n",
      "I0427 17:51:48.077986  1538 net.cpp:425] accuracy <- label_mnist_1_split_0\n",
      "I0427 17:51:48.077994  1538 net.cpp:399] accuracy -> accuracy\n",
      "I0427 17:51:48.078006  1538 net.cpp:141] Setting up accuracy\n",
      "I0427 17:51:48.078013  1538 net.cpp:148] Top shape: (1)\n",
      "I0427 17:51:48.078021  1538 net.cpp:156] Memory required for data: 8086804\n",
      "I0427 17:51:48.078027  1538 layer_factory.hpp:77] Creating layer loss\n",
      "I0427 17:51:48.078038  1538 net.cpp:91] Creating Layer loss\n",
      "I0427 17:51:48.078045  1538 net.cpp:425] loss <- ip2_ip2_0_split_1\n",
      "I0427 17:51:48.078053  1538 net.cpp:425] loss <- label_mnist_1_split_1\n",
      "I0427 17:51:48.078061  1538 net.cpp:399] loss -> loss\n",
      "I0427 17:51:48.078086  1538 layer_factory.hpp:77] Creating layer loss\n",
      "I0427 17:51:48.078106  1538 net.cpp:141] Setting up loss\n",
      "I0427 17:51:48.078115  1538 net.cpp:148] Top shape: (1)\n",
      "I0427 17:51:48.078121  1538 net.cpp:151]     with loss weight 1\n",
      "I0427 17:51:48.078135  1538 net.cpp:156] Memory required for data: 8086808\n",
      "I0427 17:51:48.078142  1538 net.cpp:217] loss needs backward computation.\n",
      "I0427 17:51:48.078150  1538 net.cpp:219] accuracy does not need backward computation.\n",
      "I0427 17:51:48.078157  1538 net.cpp:217] ip2_ip2_0_split needs backward computation.\n",
      "I0427 17:51:48.078163  1538 net.cpp:217] ip2 needs backward computation.\n",
      "I0427 17:51:48.078171  1538 net.cpp:217] relu1 needs backward computation.\n",
      "I0427 17:51:48.078177  1538 net.cpp:217] ip1 needs backward computation.\n",
      "I0427 17:51:48.078183  1538 net.cpp:217] pool2 needs backward computation.\n",
      "I0427 17:51:48.078191  1538 net.cpp:217] conv2 needs backward computation.\n",
      "I0427 17:51:48.078197  1538 net.cpp:217] pool1 needs backward computation.\n",
      "I0427 17:51:48.078204  1538 net.cpp:217] conv1 needs backward computation.\n",
      "I0427 17:51:48.078212  1538 net.cpp:219] label_mnist_1_split does not need backward computation.\n",
      "I0427 17:51:48.078218  1538 net.cpp:219] mnist does not need backward computation.\n",
      "I0427 17:51:48.078227  1538 net.cpp:261] This network produces output accuracy\n",
      "I0427 17:51:48.078233  1538 net.cpp:261] This network produces output loss\n",
      "I0427 17:51:48.078246  1538 net.cpp:274] Network initialization done.\n",
      "I0427 17:51:48.081311  1538 caffe.cpp:252] Running for 100 iterations.\n",
      "I0427 17:51:48.128150  1538 caffe.cpp:275] Batch 0, accuracy = 1\n",
      "I0427 17:51:48.128206  1538 caffe.cpp:275] Batch 0, loss = 0.002174\n",
      "I0427 17:51:48.173277  1538 caffe.cpp:275] Batch 1, accuracy = 0.99\n",
      "I0427 17:51:48.173306  1538 caffe.cpp:275] Batch 1, loss = 0.0190438\n",
      "I0427 17:51:48.218138  1538 caffe.cpp:275] Batch 2, accuracy = 0.99\n",
      "I0427 17:51:48.218183  1538 caffe.cpp:275] Batch 2, loss = 0.0503563\n",
      "I0427 17:51:48.262850  1538 caffe.cpp:275] Batch 3, accuracy = 1\n",
      "I0427 17:51:48.262876  1538 caffe.cpp:275] Batch 3, loss = 0.0126848\n",
      "I0427 17:51:48.307344  1538 caffe.cpp:275] Batch 4, accuracy = 0.98\n",
      "I0427 17:51:48.307364  1538 caffe.cpp:275] Batch 4, loss = 0.122827\n",
      "I0427 17:51:48.351491  1538 caffe.cpp:275] Batch 5, accuracy = 0.99\n",
      "I0427 17:51:48.351524  1538 caffe.cpp:275] Batch 5, loss = 0.0423915\n",
      "I0427 17:51:48.404127  1538 caffe.cpp:275] Batch 6, accuracy = 0.99\n",
      "I0427 17:51:48.404194  1538 caffe.cpp:275] Batch 6, loss = 0.0448084\n",
      "I0427 17:51:48.468825  1538 caffe.cpp:275] Batch 7, accuracy = 0.99\n",
      "I0427 17:51:48.468849  1538 caffe.cpp:275] Batch 7, loss = 0.020353\n",
      "I0427 17:51:48.521733  1538 caffe.cpp:275] Batch 8, accuracy = 1\n",
      "I0427 17:51:48.521754  1538 caffe.cpp:275] Batch 8, loss = 0.00725952\n",
      "I0427 17:51:48.568174  1538 caffe.cpp:275] Batch 9, accuracy = 0.99\n",
      "I0427 17:51:48.568193  1538 caffe.cpp:275] Batch 9, loss = 0.0487802\n",
      "I0427 17:51:48.613574  1538 caffe.cpp:275] Batch 10, accuracy = 0.98\n",
      "I0427 17:51:48.613592  1538 caffe.cpp:275] Batch 10, loss = 0.0641907\n",
      "I0427 17:51:48.658617  1538 caffe.cpp:275] Batch 11, accuracy = 0.99\n",
      "I0427 17:51:48.658634  1538 caffe.cpp:275] Batch 11, loss = 0.0305862\n",
      "I0427 17:51:48.703655  1538 caffe.cpp:275] Batch 12, accuracy = 0.94\n",
      "I0427 17:51:48.703671  1538 caffe.cpp:275] Batch 12, loss = 0.193584\n",
      "I0427 17:51:48.748651  1538 caffe.cpp:275] Batch 13, accuracy = 0.99\n",
      "I0427 17:51:48.748669  1538 caffe.cpp:275] Batch 13, loss = 0.0514671\n",
      "I0427 17:51:48.793722  1538 caffe.cpp:275] Batch 14, accuracy = 0.99\n",
      "I0427 17:51:48.793740  1538 caffe.cpp:275] Batch 14, loss = 0.0178111\n",
      "I0427 17:51:48.838739  1538 caffe.cpp:275] Batch 15, accuracy = 0.99\n",
      "I0427 17:51:48.838757  1538 caffe.cpp:275] Batch 15, loss = 0.0507264\n",
      "I0427 17:51:48.883733  1538 caffe.cpp:275] Batch 16, accuracy = 0.98\n",
      "I0427 17:51:48.883750  1538 caffe.cpp:275] Batch 16, loss = 0.035195\n",
      "I0427 17:51:48.928736  1538 caffe.cpp:275] Batch 17, accuracy = 0.99\n",
      "I0427 17:51:48.928755  1538 caffe.cpp:275] Batch 17, loss = 0.039896\n",
      "I0427 17:51:48.971815  1538 caffe.cpp:275] Batch 18, accuracy = 0.98\n",
      "I0427 17:51:48.971915  1538 caffe.cpp:275] Batch 18, loss = 0.0223742\n",
      "I0427 17:51:49.015235  1538 caffe.cpp:275] Batch 19, accuracy = 0.99\n",
      "I0427 17:51:49.015255  1538 caffe.cpp:275] Batch 19, loss = 0.0422851\n",
      "I0427 17:51:49.057296  1538 caffe.cpp:275] Batch 20, accuracy = 0.98\n",
      "I0427 17:51:49.057317  1538 caffe.cpp:275] Batch 20, loss = 0.0618467\n",
      "I0427 17:51:49.099375  1538 caffe.cpp:275] Batch 21, accuracy = 0.96\n",
      "I0427 17:51:49.099395  1538 caffe.cpp:275] Batch 21, loss = 0.0912608\n",
      "I0427 17:51:49.141693  1538 caffe.cpp:275] Batch 22, accuracy = 1\n",
      "I0427 17:51:49.141715  1538 caffe.cpp:275] Batch 22, loss = 0.0140799\n",
      "I0427 17:51:49.184036  1538 caffe.cpp:275] Batch 23, accuracy = 0.99\n",
      "I0427 17:51:49.184056  1538 caffe.cpp:275] Batch 23, loss = 0.0151361\n",
      "I0427 17:51:49.226604  1538 caffe.cpp:275] Batch 24, accuracy = 0.98\n",
      "I0427 17:51:49.226627  1538 caffe.cpp:275] Batch 24, loss = 0.0408164\n",
      "I0427 17:51:49.269170  1538 caffe.cpp:275] Batch 25, accuracy = 0.99\n",
      "I0427 17:51:49.269189  1538 caffe.cpp:275] Batch 25, loss = 0.0706237\n",
      "I0427 17:51:49.312031  1538 caffe.cpp:275] Batch 26, accuracy = 0.99\n",
      "I0427 17:51:49.312052  1538 caffe.cpp:275] Batch 26, loss = 0.0678972\n",
      "I0427 17:51:49.354909  1538 caffe.cpp:275] Batch 27, accuracy = 1\n",
      "I0427 17:51:49.354929  1538 caffe.cpp:275] Batch 27, loss = 0.0163519\n",
      "I0427 17:51:49.397802  1538 caffe.cpp:275] Batch 28, accuracy = 0.99\n",
      "I0427 17:51:49.397821  1538 caffe.cpp:275] Batch 28, loss = 0.0614227\n",
      "I0427 17:51:49.440553  1538 caffe.cpp:275] Batch 29, accuracy = 0.96\n",
      "I0427 17:51:49.440574  1538 caffe.cpp:275] Batch 29, loss = 0.14449\n",
      "I0427 17:51:49.483415  1538 caffe.cpp:275] Batch 30, accuracy = 0.99\n",
      "I0427 17:51:49.483436  1538 caffe.cpp:275] Batch 30, loss = 0.0278846\n",
      "I0427 17:51:49.526278  1538 caffe.cpp:275] Batch 31, accuracy = 1\n",
      "I0427 17:51:49.526298  1538 caffe.cpp:275] Batch 31, loss = 0.00188993\n",
      "I0427 17:51:49.569039  1538 caffe.cpp:275] Batch 32, accuracy = 0.99\n",
      "I0427 17:51:49.569067  1538 caffe.cpp:275] Batch 32, loss = 0.0181356\n",
      "I0427 17:51:49.612457  1538 caffe.cpp:275] Batch 33, accuracy = 1\n",
      "I0427 17:51:49.612484  1538 caffe.cpp:275] Batch 33, loss = 0.0036697\n",
      "I0427 17:51:49.655238  1538 caffe.cpp:275] Batch 34, accuracy = 0.99\n",
      "I0427 17:51:49.655261  1538 caffe.cpp:275] Batch 34, loss = 0.0437479\n",
      "I0427 17:51:49.698246  1538 caffe.cpp:275] Batch 35, accuracy = 0.96\n",
      "I0427 17:51:49.698274  1538 caffe.cpp:275] Batch 35, loss = 0.144056\n",
      "I0427 17:51:49.741016  1538 caffe.cpp:275] Batch 36, accuracy = 1\n",
      "I0427 17:51:49.741035  1538 caffe.cpp:275] Batch 36, loss = 0.00373965\n",
      "I0427 17:51:49.783179  1538 caffe.cpp:275] Batch 37, accuracy = 0.98\n",
      "I0427 17:51:49.783197  1538 caffe.cpp:275] Batch 37, loss = 0.0371831\n",
      "I0427 17:51:49.825355  1538 caffe.cpp:275] Batch 38, accuracy = 0.99\n",
      "I0427 17:51:49.825373  1538 caffe.cpp:275] Batch 38, loss = 0.0229032\n",
      "I0427 17:51:49.867331  1538 caffe.cpp:275] Batch 39, accuracy = 0.97\n",
      "I0427 17:51:49.867352  1538 caffe.cpp:275] Batch 39, loss = 0.0520606\n",
      "I0427 17:51:49.909394  1538 caffe.cpp:275] Batch 40, accuracy = 0.99\n",
      "I0427 17:51:49.909415  1538 caffe.cpp:275] Batch 40, loss = 0.0190114\n",
      "I0427 17:51:49.951553  1538 caffe.cpp:275] Batch 41, accuracy = 0.99\n",
      "I0427 17:51:49.951586  1538 caffe.cpp:275] Batch 41, loss = 0.0529089\n",
      "I0427 17:51:49.994065  1538 caffe.cpp:275] Batch 42, accuracy = 0.97\n",
      "I0427 17:51:49.994086  1538 caffe.cpp:275] Batch 42, loss = 0.0407268\n",
      "I0427 17:51:50.036806  1538 caffe.cpp:275] Batch 43, accuracy = 1\n",
      "I0427 17:51:50.036826  1538 caffe.cpp:275] Batch 43, loss = 0.00784205\n",
      "I0427 17:51:50.080648  1538 caffe.cpp:275] Batch 44, accuracy = 0.99\n",
      "I0427 17:51:50.080673  1538 caffe.cpp:275] Batch 44, loss = 0.0722231\n",
      "I0427 17:51:50.123613  1538 caffe.cpp:275] Batch 45, accuracy = 0.99\n",
      "I0427 17:51:50.123636  1538 caffe.cpp:275] Batch 45, loss = 0.0313876\n",
      "I0427 17:51:50.166456  1538 caffe.cpp:275] Batch 46, accuracy = 1\n",
      "I0427 17:51:50.166477  1538 caffe.cpp:275] Batch 46, loss = 0.00611202\n",
      "I0427 17:51:50.209316  1538 caffe.cpp:275] Batch 47, accuracy = 0.99\n",
      "I0427 17:51:50.209336  1538 caffe.cpp:275] Batch 47, loss = 0.0504559\n",
      "I0427 17:51:50.252203  1538 caffe.cpp:275] Batch 48, accuracy = 0.97\n",
      "I0427 17:51:50.252225  1538 caffe.cpp:275] Batch 48, loss = 0.0813489\n",
      "I0427 17:51:50.294977  1538 caffe.cpp:275] Batch 49, accuracy = 1\n",
      "I0427 17:51:50.294998  1538 caffe.cpp:275] Batch 49, loss = 0.00420377\n",
      "I0427 17:51:50.337941  1538 caffe.cpp:275] Batch 50, accuracy = 1\n",
      "I0427 17:51:50.337972  1538 caffe.cpp:275] Batch 50, loss = 0.000257611\n",
      "I0427 17:51:50.381108  1538 caffe.cpp:275] Batch 51, accuracy = 1\n",
      "I0427 17:51:50.381127  1538 caffe.cpp:275] Batch 51, loss = 0.00180851\n",
      "I0427 17:51:50.423941  1538 caffe.cpp:275] Batch 52, accuracy = 0.99\n",
      "I0427 17:51:50.423967  1538 caffe.cpp:275] Batch 52, loss = 0.0108628\n",
      "I0427 17:51:50.466665  1538 caffe.cpp:275] Batch 53, accuracy = 1\n",
      "I0427 17:51:50.466701  1538 caffe.cpp:275] Batch 53, loss = 0.00288654\n",
      "I0427 17:51:50.509130  1538 caffe.cpp:275] Batch 54, accuracy = 1\n",
      "I0427 17:51:50.509150  1538 caffe.cpp:275] Batch 54, loss = 0.00525014\n",
      "I0427 17:51:50.551188  1538 caffe.cpp:275] Batch 55, accuracy = 1\n",
      "I0427 17:51:50.551208  1538 caffe.cpp:275] Batch 55, loss = 0.000623081\n",
      "I0427 17:51:50.593169  1538 caffe.cpp:275] Batch 56, accuracy = 1\n",
      "I0427 17:51:50.593189  1538 caffe.cpp:275] Batch 56, loss = 0.00819538\n",
      "I0427 17:51:50.635372  1538 caffe.cpp:275] Batch 57, accuracy = 0.99\n",
      "I0427 17:51:50.635395  1538 caffe.cpp:275] Batch 57, loss = 0.0106424\n",
      "I0427 17:51:50.677714  1538 caffe.cpp:275] Batch 58, accuracy = 1\n",
      "I0427 17:51:50.677733  1538 caffe.cpp:275] Batch 58, loss = 0.00173683\n",
      "I0427 17:51:50.720278  1538 caffe.cpp:275] Batch 59, accuracy = 0.98\n",
      "I0427 17:51:50.720299  1538 caffe.cpp:275] Batch 59, loss = 0.0531448\n",
      "I0427 17:51:50.762691  1538 caffe.cpp:275] Batch 60, accuracy = 1\n",
      "I0427 17:51:50.762711  1538 caffe.cpp:275] Batch 60, loss = 0.00906233\n",
      "I0427 17:51:50.804858  1538 caffe.cpp:275] Batch 61, accuracy = 1\n",
      "I0427 17:51:50.804890  1538 caffe.cpp:275] Batch 61, loss = 0.00182192\n",
      "I0427 17:51:50.847164  1538 caffe.cpp:275] Batch 62, accuracy = 1\n",
      "I0427 17:51:50.847184  1538 caffe.cpp:275] Batch 62, loss = 8.41762e-06\n",
      "I0427 17:51:50.889293  1538 caffe.cpp:275] Batch 63, accuracy = 1\n",
      "I0427 17:51:50.889312  1538 caffe.cpp:275] Batch 63, loss = 0.000101747\n",
      "I0427 17:51:50.931262  1538 caffe.cpp:275] Batch 64, accuracy = 1\n",
      "I0427 17:51:50.931282  1538 caffe.cpp:275] Batch 64, loss = 0.000151115\n",
      "I0427 17:51:50.973399  1538 caffe.cpp:275] Batch 65, accuracy = 0.96\n",
      "I0427 17:51:50.973420  1538 caffe.cpp:275] Batch 65, loss = 0.119593\n",
      "I0427 17:51:51.015336  1538 caffe.cpp:275] Batch 66, accuracy = 0.98\n",
      "I0427 17:51:51.015355  1538 caffe.cpp:275] Batch 66, loss = 0.0502177\n",
      "I0427 17:51:51.057410  1538 caffe.cpp:275] Batch 67, accuracy = 0.99\n",
      "I0427 17:51:51.057449  1538 caffe.cpp:275] Batch 67, loss = 0.0201148\n",
      "I0427 17:51:51.099843  1538 caffe.cpp:275] Batch 68, accuracy = 1\n",
      "I0427 17:51:51.099880  1538 caffe.cpp:275] Batch 68, loss = 0.00208406\n",
      "I0427 17:51:51.141759  1538 caffe.cpp:275] Batch 69, accuracy = 1\n",
      "I0427 17:51:51.141779  1538 caffe.cpp:275] Batch 69, loss = 0.00463459\n",
      "I0427 17:51:51.183346  1538 caffe.cpp:275] Batch 70, accuracy = 1\n",
      "I0427 17:51:51.183374  1538 caffe.cpp:275] Batch 70, loss = 0.000249587\n",
      "I0427 17:51:51.225214  1538 caffe.cpp:275] Batch 71, accuracy = 1\n",
      "I0427 17:51:51.225246  1538 caffe.cpp:275] Batch 71, loss = 0.000476616\n",
      "I0427 17:51:51.266576  1538 caffe.cpp:275] Batch 72, accuracy = 1\n",
      "I0427 17:51:51.266597  1538 caffe.cpp:275] Batch 72, loss = 0.0066501\n",
      "I0427 17:51:51.308545  1538 caffe.cpp:275] Batch 73, accuracy = 1\n",
      "I0427 17:51:51.308578  1538 caffe.cpp:275] Batch 73, loss = 7.84527e-05\n",
      "I0427 17:51:51.350849  1538 caffe.cpp:275] Batch 74, accuracy = 1\n",
      "I0427 17:51:51.350870  1538 caffe.cpp:275] Batch 74, loss = 0.00168813\n",
      "I0427 17:51:51.393183  1538 caffe.cpp:275] Batch 75, accuracy = 1\n",
      "I0427 17:51:51.393203  1538 caffe.cpp:275] Batch 75, loss = 0.000960828\n",
      "I0427 17:51:51.436102  1538 caffe.cpp:275] Batch 76, accuracy = 1\n",
      "I0427 17:51:51.436122  1538 caffe.cpp:275] Batch 76, loss = 0.000120348\n",
      "I0427 17:51:51.479046  1538 caffe.cpp:275] Batch 77, accuracy = 1\n",
      "I0427 17:51:51.479068  1538 caffe.cpp:275] Batch 77, loss = 0.000149271\n",
      "I0427 17:51:51.522023  1538 caffe.cpp:275] Batch 78, accuracy = 1\n",
      "I0427 17:51:51.522043  1538 caffe.cpp:275] Batch 78, loss = 0.00267475\n",
      "I0427 17:51:51.564537  1538 caffe.cpp:275] Batch 79, accuracy = 1\n",
      "I0427 17:51:51.564558  1538 caffe.cpp:275] Batch 79, loss = 0.00406783\n",
      "I0427 17:51:51.606618  1538 caffe.cpp:275] Batch 80, accuracy = 1\n",
      "I0427 17:51:51.606650  1538 caffe.cpp:275] Batch 80, loss = 0.00426423\n",
      "I0427 17:51:51.648802  1538 caffe.cpp:275] Batch 81, accuracy = 1\n",
      "I0427 17:51:51.648824  1538 caffe.cpp:275] Batch 81, loss = 0.00106487\n",
      "I0427 17:51:51.691642  1538 caffe.cpp:275] Batch 82, accuracy = 1\n",
      "I0427 17:51:51.691675  1538 caffe.cpp:275] Batch 82, loss = 0.00517984\n",
      "I0427 17:51:51.733898  1538 caffe.cpp:275] Batch 83, accuracy = 1\n",
      "I0427 17:51:51.733918  1538 caffe.cpp:275] Batch 83, loss = 0.00621952\n",
      "I0427 17:51:51.775960  1538 caffe.cpp:275] Batch 84, accuracy = 0.99\n",
      "I0427 17:51:51.775981  1538 caffe.cpp:275] Batch 84, loss = 0.0228145\n",
      "I0427 17:51:51.818619  1538 caffe.cpp:275] Batch 85, accuracy = 0.99\n",
      "I0427 17:51:51.818660  1538 caffe.cpp:275] Batch 85, loss = 0.0162328\n",
      "I0427 17:51:51.861269  1538 caffe.cpp:275] Batch 86, accuracy = 1\n",
      "I0427 17:51:51.861290  1538 caffe.cpp:275] Batch 86, loss = 6.37265e-05\n",
      "I0427 17:51:51.903689  1538 caffe.cpp:275] Batch 87, accuracy = 1\n",
      "I0427 17:51:51.903708  1538 caffe.cpp:275] Batch 87, loss = 1.01156e-05\n",
      "I0427 17:51:51.946077  1538 caffe.cpp:275] Batch 88, accuracy = 1\n",
      "I0427 17:51:51.946104  1538 caffe.cpp:275] Batch 88, loss = 2.40566e-05\n",
      "I0427 17:51:51.988649  1538 caffe.cpp:275] Batch 89, accuracy = 1\n",
      "I0427 17:51:51.988667  1538 caffe.cpp:275] Batch 89, loss = 3.15153e-05\n",
      "I0427 17:51:52.030292  1538 caffe.cpp:275] Batch 90, accuracy = 0.97\n",
      "I0427 17:51:52.030313  1538 caffe.cpp:275] Batch 90, loss = 0.0898077\n",
      "I0427 17:51:52.072111  1538 caffe.cpp:275] Batch 91, accuracy = 1\n",
      "I0427 17:51:52.072131  1538 caffe.cpp:275] Batch 91, loss = 5.87488e-05\n",
      "I0427 17:51:52.115501  1538 caffe.cpp:275] Batch 92, accuracy = 1\n",
      "I0427 17:51:52.115532  1538 caffe.cpp:275] Batch 92, loss = 0.000293934\n",
      "I0427 17:51:52.157140  1538 caffe.cpp:275] Batch 93, accuracy = 1\n",
      "I0427 17:51:52.157161  1538 caffe.cpp:275] Batch 93, loss = 0.00119341\n",
      "I0427 17:51:52.199360  1538 caffe.cpp:275] Batch 94, accuracy = 1\n",
      "I0427 17:51:52.199383  1538 caffe.cpp:275] Batch 94, loss = 0.000214964\n",
      "I0427 17:51:52.241441  1538 caffe.cpp:275] Batch 95, accuracy = 1\n",
      "I0427 17:51:52.241462  1538 caffe.cpp:275] Batch 95, loss = 0.0017093\n",
      "I0427 17:51:52.283442  1538 caffe.cpp:275] Batch 96, accuracy = 0.98\n",
      "I0427 17:51:52.283459  1538 caffe.cpp:275] Batch 96, loss = 0.0641182\n",
      "I0427 17:51:52.325137  1538 caffe.cpp:275] Batch 97, accuracy = 0.98\n",
      "I0427 17:51:52.325170  1538 caffe.cpp:275] Batch 97, loss = 0.0891785\n",
      "I0427 17:51:52.365902  1538 caffe.cpp:275] Batch 98, accuracy = 1\n",
      "I0427 17:51:52.365919  1538 caffe.cpp:275] Batch 98, loss = 0.00507255\n",
      "I0427 17:51:52.406788  1538 caffe.cpp:275] Batch 99, accuracy = 0.99\n",
      "I0427 17:51:52.406806  1538 caffe.cpp:275] Batch 99, loss = 0.013897\n",
      "I0427 17:51:52.406827  1538 caffe.cpp:280] Loss: 0.0286597\n",
      "I0427 17:51:52.406849  1538 caffe.cpp:292] accuracy = 0.9913\n",
      "I0427 17:51:52.406865  1538 caffe.cpp:292] loss = 0.0286597 (* 1 = 0.0286597 loss)\n"
     ]
    }
   ],
   "source": [
    "!./caffe/build/tools/caffe test --model=./lenet/lenet_train_test.prototxt --weights=./lenet/lenet_iter_10000.caffemodel --iterations=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "위의 코드의 수행 결과 가장 아래쪽에 100개의 minibatch상에서의 평균 성능이 나오며, 정상적으로 본 tutorial을 수행한다면 99.1%의 accuracy를 보실 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "description": "Define, train, and test the classic LeNet with the Python interface.",
  "example_name": "Learning LeNet",
  "include_in_docs": true,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "priority": 2
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
