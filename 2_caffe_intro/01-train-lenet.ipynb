{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training  and testing LeNet using caffe\n",
    "Jupyter notebook을 사용하여 Linux Shell 명령어로 caffe를 사용하는 방법에 대해서 배웁니다.\n",
    "\n",
    "본 예제는 크게 두 Part로 나누어져 있습니다.\n",
    "1. Caffe 명령어를 통해 모델을 학습하는 방법\n",
    "2. 학습된 모델의 test data에 대한 Classification 성능을 측정하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Basic Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Jupyter Notebook에서 Linux Shell 명령어를 사용하기 위해서는 명령앞에 !를 붙여서 실행하시면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/dlworkshop/2_caffe_intro\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_mnist.sh\t\t      lenet_iter_5000.caffemodel   mnist.pyc\r\n",
      "lenet.prototxt\t\t      lenet_iter_5000.solverstate  mnist_test_lmdb\r\n",
      "lenet5.png\t\t      lenet_solver.prototxt\t   mnist_train_lmdb\r\n",
      "lenet_iter_10000.caffemodel   lenet_train_test.prototxt\r\n",
      "lenet_iter_10000.solverstate  mnist.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./lenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training LeNet 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=./lenet/lenet5.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. Convert MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본 예제에서 사용할 데이터셋은 MNIST로 0~9까지의 숫자 이미지와 이미지에 해당하는 Label을 포함합니다.\n",
    "MNIST 데이터셋에 대해 자세한 내용은 http://yann.lecun.com/exdb/mnist 를 참고해 주세요.\n",
    "\n",
    "Raw MNIST dataset은 제공해드린 docker 이미지상에 이미 다운로드 되어 있으며, 해당 위치는 ./caffe/data/mnist 입니다.\n",
    "하지만 Raw MNIST dataset의 형식은 caffe에서 지원되지 않기에 caffe에서 가장 많이 쓰는 LMDB형식으로 변환되어야 합니다.\n",
    "변환에 필요한 코드는 이미 작성되어 있으며 ./lenet/create_mnist.sh 을 실행하여 변환합니다.\n",
    "\n",
    "스크립트 실행시 발생하는 libdc1394 error는 docker의 driver상의 오류로 무시하셔도 됩니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating lmdb...\n",
      "libdc1394 error: Failed to initialize libdc1394\n",
      "libdc1394 error: Failed to initialize libdc1394\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "!sh ./lenet/create_mnist.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "변환이 완료된 후, !ls ./lenet 명령어를 통해 mnist_test_lmdb와 mnist_train_lmdb가 정상적으로 생성되었음을 확인하실 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_mnist.sh\t\t      lenet_iter_5000.caffemodel   mnist.pyc\r\n",
      "lenet.prototxt\t\t      lenet_iter_5000.solverstate  mnist_test_lmdb\r\n",
      "lenet5.png\t\t      lenet_solver.prototxt\t   mnist_train_lmdb\r\n",
      "lenet_iter_10000.caffemodel   lenet_train_test.prototxt\r\n",
      "lenet_iter_10000.solverstate  mnist.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./lenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2. Define model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet5의 모델은 ./lenet/lenet_train_test.prototxt에 정의되어 있습니다. \n",
    "\n",
    "cat 명령어는 Linux에서 text파일을 모니터에 출력해주는 함수로 prototxt 파일을 읽어올 수 있습니다.\n",
    "명령어 실행 결과를 참고하여 각 layer가 요구하는 parameter를 참고하세요.\n",
    "\n",
    "prototxt는 jupyter notebook 상에서 해당 파일을 클릭하는 것으로 수정하실 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"LeNet\"\r\n",
      "layer {\r\n",
      "  name: \"mnist\"\r\n",
      "  type: \"Data\"\r\n",
      "  top: \"data\"\r\n",
      "  top: \"label\"\r\n",
      "  include {\r\n",
      "    phase: TRAIN\r\n",
      "  }\r\n",
      "  transform_param {\r\n",
      "    scale: 0.00390625\r\n",
      "  }\r\n",
      "  data_param {\r\n",
      "    source: \"./lenet/mnist_train_lmdb\"\r\n",
      "    batch_size: 64\r\n",
      "    backend: LMDB\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"mnist\"\r\n",
      "  type: \"Data\"\r\n",
      "  top: \"data\"\r\n",
      "  top: \"label\"\r\n",
      "  include {\r\n",
      "    phase: TEST\r\n",
      "  }\r\n",
      "  transform_param {\r\n",
      "    scale: 0.00390625\r\n",
      "  }\r\n",
      "  data_param {\r\n",
      "    source: \"./lenet/mnist_test_lmdb\"\r\n",
      "    batch_size: 100\r\n",
      "    backend: LMDB\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"conv1\"\r\n",
      "  type: \"Convolution\"\r\n",
      "  bottom: \"data\"\r\n",
      "  top: \"conv1\"\r\n",
      "  param {\r\n",
      "    lr_mult: 1\r\n",
      "  }\r\n",
      "  param {\r\n",
      "    lr_mult: 2\r\n",
      "  }\r\n",
      "  convolution_param {\r\n",
      "    num_output: 20\r\n",
      "    kernel_size: 5\r\n",
      "    stride: 1\r\n",
      "    weight_filler {\r\n",
      "      type: \"xavier\"\r\n",
      "    }\r\n",
      "    bias_filler {\r\n",
      "      type: \"constant\"\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"pool1\"\r\n",
      "  type: \"Pooling\"\r\n",
      "  bottom: \"conv1\"\r\n",
      "  top: \"pool1\"\r\n",
      "  pooling_param {\r\n",
      "    pool: MAX\r\n",
      "    kernel_size: 2\r\n",
      "    stride: 2\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"conv2\"\r\n",
      "  type: \"Convolution\"\r\n",
      "  bottom: \"pool1\"\r\n",
      "  top: \"conv2\"\r\n",
      "  param {\r\n",
      "    lr_mult: 1\r\n",
      "  }\r\n",
      "  param {\r\n",
      "    lr_mult: 2\r\n",
      "  }\r\n",
      "  convolution_param {\r\n",
      "    num_output: 50\r\n",
      "    kernel_size: 5\r\n",
      "    stride: 1\r\n",
      "    weight_filler {\r\n",
      "      type: \"xavier\"\r\n",
      "    }\r\n",
      "    bias_filler {\r\n",
      "      type: \"constant\"\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"pool2\"\r\n",
      "  type: \"Pooling\"\r\n",
      "  bottom: \"conv2\"\r\n",
      "  top: \"pool2\"\r\n",
      "  pooling_param {\r\n",
      "    pool: MAX\r\n",
      "    kernel_size: 2\r\n",
      "    stride: 2\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"ip1\"\r\n",
      "  type: \"InnerProduct\"\r\n",
      "  bottom: \"pool2\"\r\n",
      "  top: \"ip1\"\r\n",
      "  param {\r\n",
      "    lr_mult: 1\r\n",
      "  }\r\n",
      "  param {\r\n",
      "    lr_mult: 2\r\n",
      "  }\r\n",
      "  inner_product_param {\r\n",
      "    num_output: 500\r\n",
      "    weight_filler {\r\n",
      "      type: \"xavier\"\r\n",
      "    }\r\n",
      "    bias_filler {\r\n",
      "      type: \"constant\"\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"relu1\"\r\n",
      "  type: \"ReLU\"\r\n",
      "  bottom: \"ip1\"\r\n",
      "  top: \"ip1\"\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"ip2\"\r\n",
      "  type: \"InnerProduct\"\r\n",
      "  bottom: \"ip1\"\r\n",
      "  top: \"ip2\"\r\n",
      "  param {\r\n",
      "    lr_mult: 1\r\n",
      "  }\r\n",
      "  param {\r\n",
      "    lr_mult: 2\r\n",
      "  }\r\n",
      "  inner_product_param {\r\n",
      "    num_output: 10\r\n",
      "    weight_filler {\r\n",
      "      type: \"xavier\"\r\n",
      "    }\r\n",
      "    bias_filler {\r\n",
      "      type: \"constant\"\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"accuracy\"\r\n",
      "  type: \"Accuracy\"\r\n",
      "  bottom: \"ip2\"\r\n",
      "  bottom: \"label\"\r\n",
      "  top: \"accuracy\"\r\n",
      "  include {\r\n",
      "    phase: TEST\r\n",
      "  }\r\n",
      "}\r\n",
      "layer {\r\n",
      "  name: \"loss\"\r\n",
      "  type: \"SoftmaxWithLoss\"\r\n",
      "  bottom: \"ip2\"\r\n",
      "  bottom: \"label\"\r\n",
      "  top: \"loss\"\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./lenet/lenet_train_test.prototxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-3. Define solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정의된 Network를 학습하기 위해서 Solver를 prototxt형식으로 정의합니다. \n",
    "\n",
    "Solver hyperparameters\n",
    "- Solver: SGD (Stochastic gradient descent)\n",
    "- Base Learning Rate: 0.01 (Staring learning rate)\n",
    "- Momemtum: 0.9 (SGD parameter)\n",
    "- weight_decay: 0.005\n",
    "- lr_policy: inv \n",
    "- Gamma: 0.0001\n",
    "- Power: 0.75\n",
    "- solver_mode: CPU  \n",
    "\n",
    "Display param\n",
    "- display: 100 (training 100 batch 마다 Loss 출력)\n",
    "- test_iter: 100 (validation data시 100개의 batch를 테스트)\n",
    "- test_interval: 500 (training 500 batch 마다 validation data test)\n",
    "- max_iter: 10000 (10000 batch training 도달시 종료)\n",
    "\n",
    "Snapshot param\n",
    "- snapshot: 5000 (5000 batch training 마다 모델 저장)\n",
    "- snapshot_prefix: ./lenet/lenet (모델 저장 위치와 이름에 대한 prefix)\n",
    "\n",
    "위의 내용을 caffe의 prototxt 형식으로 나타내면 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# The train/test net protocol buffer definition\r\n",
      "net: \"./lenet/lenet_train_test.prototxt\"\r\n",
      "# test_iter specifies how many forward passes the test should carry out.\r\n",
      "# In the case of MNIST, we have test batch size 100 and 100 test iterations,\r\n",
      "# covering the full 10,000 testing images.\r\n",
      "test_iter: 100\r\n",
      "# Carry out testing every 500 training iterations.\r\n",
      "test_interval: 500\r\n",
      "# The base learning rate, momentum and the weight decay of the network.\r\n",
      "base_lr: 0.01\r\n",
      "momentum: 0.9\r\n",
      "weight_decay: 0.0005\r\n",
      "# The learning rate policy\r\n",
      "lr_policy: \"inv\"\r\n",
      "gamma: 0.0001\r\n",
      "power: 0.75\r\n",
      "# Display every 100 iterations\r\n",
      "display: 100\r\n",
      "# The maximum number of iterations\r\n",
      "max_iter: 10000\r\n",
      "# snapshot intermediate results\r\n",
      "snapshot: 5000\r\n",
      "snapshot_prefix: \"./lenet/lenet\"\r\n",
      "# solver mode: CPU or GPU\r\n",
      "solver_mode: CPU\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./lenet/lenet_solver.prototxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-4. Train LeNet5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "빌드된 Caffe의 실행파일은 ./caffe/build/tools/caffe에 존재합니다.\n",
    "\n",
    "caffe 실행파일의 사용법(commands & arguments)을 보기 위해서 아래의 명령어를 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libdc1394 error: Failed to initialize libdc1394\r\n",
      "caffe: command line brew\r\n",
      "usage: caffe <command> <args>\r\n",
      "\r\n",
      "commands:\r\n",
      "  train           train or finetune a model\r\n",
      "  test            score a model\r\n",
      "  device_query    show GPU diagnostic information\r\n",
      "  time            benchmark model execution time\r\n",
      "\r\n",
      "  Flags from /root/caffe/tools/caffe.cpp:\r\n",
      "    -gpu (Optional; run in GPU mode on given device IDs separated by ','.Use\r\n",
      "      '-gpu all' to run on all available GPUs. The effective training batch\r\n",
      "      size is multiplied by the number of devices.) type: string default: \"\"\r\n",
      "    -iterations (The number of iterations to run.) type: int32 default: 50\r\n",
      "    -model (The model definition protocol buffer text file.) type: string\r\n",
      "      default: \"\"\r\n",
      "    -sighup_effect (Optional; action to take when a SIGHUP signal is received:\r\n",
      "      snapshot, stop or none.) type: string default: \"snapshot\"\r\n",
      "    -sigint_effect (Optional; action to take when a SIGINT signal is received:\r\n",
      "      snapshot, stop or none.) type: string default: \"stop\"\r\n",
      "    -snapshot (Optional; the snapshot solver state to resume training.)\r\n",
      "      type: string default: \"\"\r\n",
      "    -solver (The solver definition protocol buffer text file.) type: string\r\n",
      "      default: \"\"\r\n",
      "    -weights (Optional; the pretrained weights to initialize finetuning,\r\n",
      "      separated by ','. Cannot be set simultaneously with snapshot.)\r\n",
      "      type: string default: \"\"\r\n"
     ]
    }
   ],
   "source": [
    "!./caffe/build/tools/caffe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 간단하게는 ./caffe/build/tools/caffe train --solver=/path/to/solver/file.prototxt로 정의된 모델을 정의된 solver로 학습시킬 수 있습니다.\n",
    "\n",
    "Desktop에서 모델을 전부 학습하기 위해서는 약 10여분이 소요됩니다. (노트북의 경우 15분 이상이 소요될 수 있습니다.)\n",
    "\n",
    "#### 중단을 원하시면 Jupyter notebook 상단의 Kernel 탭 클릭 후 Interrupt를 클릭해주세요.\n",
    "#### Shell 상에서 중단 (Jupyter notebook 상의 kernel interrupt)이 될 경우 중단 시점에서의 snapshot이 자동저장됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter notebook 상의 ln [\\*] 표시는 현재 처리중임을 의미하며, 완료가 되면 \\* 기호는 숫자로 변경됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libdc1394 error: Failed to initialize libdc1394\n",
      "I0427 14:47:38.377068  1084 caffe.cpp:178] Use CPU.\n",
      "I0427 14:47:38.377388  1084 solver.cpp:48] Initializing solver from parameters: \n",
      "test_iter: 100\n",
      "test_interval: 500\n",
      "base_lr: 0.01\n",
      "display: 100\n",
      "max_iter: 10000\n",
      "lr_policy: \"inv\"\n",
      "gamma: 0.0001\n",
      "power: 0.75\n",
      "momentum: 0.9\n",
      "weight_decay: 0.0005\n",
      "snapshot: 5000\n",
      "snapshot_prefix: \"./lenet/lenet\"\n",
      "solver_mode: CPU\n",
      "net: \"./lenet/lenet_train_test.prototxt\"\n",
      "I0427 14:47:38.377507  1084 solver.cpp:91] Creating training net from net file: ./lenet/lenet_train_test.prototxt\n",
      "I0427 14:47:38.377833  1084 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist\n",
      "I0427 14:47:38.377861  1084 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy\n",
      "I0427 14:47:38.377943  1084 net.cpp:49] Initializing net from parameters: \n",
      "name: \"LeNet\"\n",
      "state {\n",
      "  phase: TRAIN\n",
      "}\n",
      "layer {\n",
      "  name: \"mnist\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TRAIN\n",
      "  }\n",
      "  transform_param {\n",
      "    scale: 0.00390625\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"./lenet/mnist_train_lmdb\"\n",
      "    batch_size: 64\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 20\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 50\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"ip1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"ip1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 500\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"ip2\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 10\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"ip2\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I0427 14:47:38.378356  1084 layer_factory.hpp:77] Creating layer mnist\n",
      "I0427 14:47:38.378739  1084 net.cpp:91] Creating Layer mnist\n",
      "I0427 14:47:38.378762  1084 net.cpp:399] mnist -> data\n",
      "I0427 14:47:38.378795  1084 net.cpp:399] mnist -> label\n",
      "I0427 14:47:38.378866  1085 db_lmdb.cpp:38] Opened lmdb ./lenet/mnist_train_lmdb\n",
      "I0427 14:47:38.378958  1084 data_layer.cpp:41] output data size: 64,1,28,28\n",
      "I0427 14:47:38.379371  1084 net.cpp:141] Setting up mnist\n",
      "I0427 14:47:38.379393  1084 net.cpp:148] Top shape: 64 1 28 28 (50176)\n",
      "I0427 14:47:38.379407  1084 net.cpp:148] Top shape: 64 (64)\n",
      "I0427 14:47:38.379415  1084 net.cpp:156] Memory required for data: 200960\n",
      "I0427 14:47:38.379432  1084 layer_factory.hpp:77] Creating layer conv1\n",
      "I0427 14:47:38.379449  1084 net.cpp:91] Creating Layer conv1\n",
      "I0427 14:47:38.379461  1084 net.cpp:425] conv1 <- data\n",
      "I0427 14:47:38.379472  1084 net.cpp:399] conv1 -> conv1\n",
      "I0427 14:47:38.379529  1084 net.cpp:141] Setting up conv1\n",
      "I0427 14:47:38.379540  1084 net.cpp:148] Top shape: 64 20 24 24 (737280)\n",
      "I0427 14:47:38.379555  1084 net.cpp:156] Memory required for data: 3150080\n",
      "I0427 14:47:38.379585  1084 layer_factory.hpp:77] Creating layer pool1\n",
      "I0427 14:47:38.379601  1084 net.cpp:91] Creating Layer pool1\n",
      "I0427 14:47:38.379609  1084 net.cpp:425] pool1 <- conv1\n",
      "I0427 14:47:38.379618  1084 net.cpp:399] pool1 -> pool1\n",
      "I0427 14:47:38.379639  1084 net.cpp:141] Setting up pool1\n",
      "I0427 14:47:38.379667  1084 net.cpp:148] Top shape: 64 20 12 12 (184320)\n",
      "I0427 14:47:38.379674  1084 net.cpp:156] Memory required for data: 3887360\n",
      "I0427 14:47:38.379681  1084 layer_factory.hpp:77] Creating layer conv2\n",
      "I0427 14:47:38.379694  1084 net.cpp:91] Creating Layer conv2\n",
      "I0427 14:47:38.379701  1084 net.cpp:425] conv2 <- pool1\n",
      "I0427 14:47:38.379714  1084 net.cpp:399] conv2 -> conv2\n",
      "I0427 14:47:38.379901  1084 net.cpp:141] Setting up conv2\n",
      "I0427 14:47:38.379917  1084 net.cpp:148] Top shape: 64 50 8 8 (204800)\n",
      "I0427 14:47:38.379923  1084 net.cpp:156] Memory required for data: 4706560\n",
      "I0427 14:47:38.379943  1084 layer_factory.hpp:77] Creating layer pool2\n",
      "I0427 14:47:38.379963  1084 net.cpp:91] Creating Layer pool2\n",
      "I0427 14:47:38.379977  1084 net.cpp:425] pool2 <- conv2\n",
      "I0427 14:47:38.379987  1084 net.cpp:399] pool2 -> pool2\n",
      "I0427 14:47:38.380000  1084 net.cpp:141] Setting up pool2\n",
      "I0427 14:47:38.380010  1084 net.cpp:148] Top shape: 64 50 4 4 (51200)\n",
      "I0427 14:47:38.380017  1084 net.cpp:156] Memory required for data: 4911360\n",
      "I0427 14:47:38.380023  1084 layer_factory.hpp:77] Creating layer ip1\n",
      "I0427 14:47:38.380033  1084 net.cpp:91] Creating Layer ip1\n",
      "I0427 14:47:38.380040  1084 net.cpp:425] ip1 <- pool2\n",
      "I0427 14:47:38.380050  1084 net.cpp:399] ip1 -> ip1\n",
      "I0427 14:47:38.382319  1084 net.cpp:141] Setting up ip1\n",
      "I0427 14:47:38.382339  1084 net.cpp:148] Top shape: 64 500 (32000)\n",
      "I0427 14:47:38.382351  1084 net.cpp:156] Memory required for data: 5039360\n",
      "I0427 14:47:38.382364  1084 layer_factory.hpp:77] Creating layer relu1\n",
      "I0427 14:47:38.382374  1084 net.cpp:91] Creating Layer relu1\n",
      "I0427 14:47:38.382380  1084 net.cpp:425] relu1 <- ip1\n",
      "I0427 14:47:38.382388  1084 net.cpp:386] relu1 -> ip1 (in-place)\n",
      "I0427 14:47:38.382397  1084 net.cpp:141] Setting up relu1\n",
      "I0427 14:47:38.382405  1084 net.cpp:148] Top shape: 64 500 (32000)\n",
      "I0427 14:47:38.382411  1084 net.cpp:156] Memory required for data: 5167360\n",
      "I0427 14:47:38.382417  1084 layer_factory.hpp:77] Creating layer ip2\n",
      "I0427 14:47:38.382426  1084 net.cpp:91] Creating Layer ip2\n",
      "I0427 14:47:38.382432  1084 net.cpp:425] ip2 <- ip1\n",
      "I0427 14:47:38.382441  1084 net.cpp:399] ip2 -> ip2\n",
      "I0427 14:47:38.382485  1084 net.cpp:141] Setting up ip2\n",
      "I0427 14:47:38.382494  1084 net.cpp:148] Top shape: 64 10 (640)\n",
      "I0427 14:47:38.382504  1084 net.cpp:156] Memory required for data: 5169920\n",
      "I0427 14:47:38.382520  1084 layer_factory.hpp:77] Creating layer loss\n",
      "I0427 14:47:38.382539  1084 net.cpp:91] Creating Layer loss\n",
      "I0427 14:47:38.382549  1084 net.cpp:425] loss <- ip2\n",
      "I0427 14:47:38.382555  1084 net.cpp:425] loss <- label\n",
      "I0427 14:47:38.382563  1084 net.cpp:399] loss -> loss\n",
      "I0427 14:47:38.382576  1084 layer_factory.hpp:77] Creating layer loss\n",
      "I0427 14:47:38.382592  1084 net.cpp:141] Setting up loss\n",
      "I0427 14:47:38.382601  1084 net.cpp:148] Top shape: (1)\n",
      "I0427 14:47:38.382607  1084 net.cpp:151]     with loss weight 1\n",
      "I0427 14:47:38.382621  1084 net.cpp:156] Memory required for data: 5169924\n",
      "I0427 14:47:38.382627  1084 net.cpp:217] loss needs backward computation.\n",
      "I0427 14:47:38.382634  1084 net.cpp:217] ip2 needs backward computation.\n",
      "I0427 14:47:38.382642  1084 net.cpp:217] relu1 needs backward computation.\n",
      "I0427 14:47:38.382647  1084 net.cpp:217] ip1 needs backward computation.\n",
      "I0427 14:47:38.382653  1084 net.cpp:217] pool2 needs backward computation.\n",
      "I0427 14:47:38.382659  1084 net.cpp:217] conv2 needs backward computation.\n",
      "I0427 14:47:38.382666  1084 net.cpp:217] pool1 needs backward computation.\n",
      "I0427 14:47:38.382673  1084 net.cpp:217] conv1 needs backward computation.\n",
      "I0427 14:47:38.382679  1084 net.cpp:219] mnist does not need backward computation.\n",
      "I0427 14:47:38.382688  1084 net.cpp:261] This network produces output loss\n",
      "I0427 14:47:38.382704  1084 net.cpp:274] Network initialization done.\n",
      "I0427 14:47:38.382936  1084 solver.cpp:181] Creating test net (#0) specified by net file: ./lenet/lenet_train_test.prototxt\n",
      "I0427 14:47:38.382969  1084 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist\n",
      "I0427 14:47:38.383035  1084 net.cpp:49] Initializing net from parameters: \n",
      "name: \"LeNet\"\n",
      "state {\n",
      "  phase: TEST\n",
      "}\n",
      "layer {\n",
      "  name: \"mnist\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "  transform_param {\n",
      "    scale: 0.00390625\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"./lenet/mnist_test_lmdb\"\n",
      "    batch_size: 100\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 20\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 50\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"ip1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"ip1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 500\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"ip2\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 10\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"accuracy\"\n",
      "  type: \"Accuracy\"\n",
      "  bottom: \"ip2\"\n",
      "  bottom: \"label\"\n",
      "  top: \"accuracy\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"ip2\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I0427 14:47:38.383441  1084 layer_factory.hpp:77] Creating layer mnist\n",
      "I0427 14:47:38.383543  1084 net.cpp:91] Creating Layer mnist\n",
      "I0427 14:47:38.383558  1084 net.cpp:399] mnist -> data\n",
      "I0427 14:47:38.383569  1084 net.cpp:399] mnist -> label\n",
      "I0427 14:47:38.383596  1087 db_lmdb.cpp:38] Opened lmdb ./lenet/mnist_test_lmdb\n",
      "I0427 14:47:38.383635  1084 data_layer.cpp:41] output data size: 100,1,28,28\n",
      "I0427 14:47:38.384332  1084 net.cpp:141] Setting up mnist\n",
      "I0427 14:47:38.384351  1084 net.cpp:148] Top shape: 100 1 28 28 (78400)\n",
      "I0427 14:47:38.384374  1084 net.cpp:148] Top shape: 100 (100)\n",
      "I0427 14:47:38.384382  1084 net.cpp:156] Memory required for data: 314000\n",
      "I0427 14:47:38.384389  1084 layer_factory.hpp:77] Creating layer label_mnist_1_split\n",
      "I0427 14:47:38.384405  1084 net.cpp:91] Creating Layer label_mnist_1_split\n",
      "I0427 14:47:38.384413  1084 net.cpp:425] label_mnist_1_split <- label\n",
      "I0427 14:47:38.384425  1084 net.cpp:399] label_mnist_1_split -> label_mnist_1_split_0\n",
      "I0427 14:47:38.384438  1084 net.cpp:399] label_mnist_1_split -> label_mnist_1_split_1\n",
      "I0427 14:47:38.384449  1084 net.cpp:141] Setting up label_mnist_1_split\n",
      "I0427 14:47:38.384456  1084 net.cpp:148] Top shape: 100 (100)\n",
      "I0427 14:47:38.384464  1084 net.cpp:148] Top shape: 100 (100)\n",
      "I0427 14:47:38.384471  1084 net.cpp:156] Memory required for data: 314800\n",
      "I0427 14:47:38.384477  1084 layer_factory.hpp:77] Creating layer conv1\n",
      "I0427 14:47:38.384492  1084 net.cpp:91] Creating Layer conv1\n",
      "I0427 14:47:38.384500  1084 net.cpp:425] conv1 <- data\n",
      "I0427 14:47:38.384516  1084 net.cpp:399] conv1 -> conv1\n",
      "I0427 14:47:38.384567  1084 net.cpp:141] Setting up conv1\n",
      "I0427 14:47:38.384580  1084 net.cpp:148] Top shape: 100 20 24 24 (1152000)\n",
      "I0427 14:47:38.384588  1084 net.cpp:156] Memory required for data: 4922800\n",
      "I0427 14:47:38.384599  1084 layer_factory.hpp:77] Creating layer pool1\n",
      "I0427 14:47:38.384609  1084 net.cpp:91] Creating Layer pool1\n",
      "I0427 14:47:38.384618  1084 net.cpp:425] pool1 <- conv1\n",
      "I0427 14:47:38.384627  1084 net.cpp:399] pool1 -> pool1\n",
      "I0427 14:47:38.384651  1084 net.cpp:141] Setting up pool1\n",
      "I0427 14:47:38.384660  1084 net.cpp:148] Top shape: 100 20 12 12 (288000)\n",
      "I0427 14:47:38.384666  1084 net.cpp:156] Memory required for data: 6074800\n",
      "I0427 14:47:38.384673  1084 layer_factory.hpp:77] Creating layer conv2\n",
      "I0427 14:47:38.384686  1084 net.cpp:91] Creating Layer conv2\n",
      "I0427 14:47:38.384698  1084 net.cpp:425] conv2 <- pool1\n",
      "I0427 14:47:38.384718  1084 net.cpp:399] conv2 -> conv2\n",
      "I0427 14:47:38.384891  1084 net.cpp:141] Setting up conv2\n",
      "I0427 14:47:38.384917  1084 net.cpp:148] Top shape: 100 50 8 8 (320000)\n",
      "I0427 14:47:38.384929  1084 net.cpp:156] Memory required for data: 7354800\n",
      "I0427 14:47:38.384943  1084 layer_factory.hpp:77] Creating layer pool2\n",
      "I0427 14:47:38.384955  1084 net.cpp:91] Creating Layer pool2\n",
      "I0427 14:47:38.384963  1084 net.cpp:425] pool2 <- conv2\n",
      "I0427 14:47:38.384971  1084 net.cpp:399] pool2 -> pool2\n",
      "I0427 14:47:38.384982  1084 net.cpp:141] Setting up pool2\n",
      "I0427 14:47:38.384991  1084 net.cpp:148] Top shape: 100 50 4 4 (80000)\n",
      "I0427 14:47:38.384999  1084 net.cpp:156] Memory required for data: 7674800\n",
      "I0427 14:47:38.385004  1084 layer_factory.hpp:77] Creating layer ip1\n",
      "I0427 14:47:38.385015  1084 net.cpp:91] Creating Layer ip1\n",
      "I0427 14:47:38.385022  1084 net.cpp:425] ip1 <- pool2\n",
      "I0427 14:47:38.385033  1084 net.cpp:399] ip1 -> ip1\n",
      "I0427 14:47:38.387398  1084 net.cpp:141] Setting up ip1\n",
      "I0427 14:47:38.387418  1084 net.cpp:148] Top shape: 100 500 (50000)\n",
      "I0427 14:47:38.387429  1084 net.cpp:156] Memory required for data: 7874800\n",
      "I0427 14:47:38.387441  1084 layer_factory.hpp:77] Creating layer relu1\n",
      "I0427 14:47:38.387450  1084 net.cpp:91] Creating Layer relu1\n",
      "I0427 14:47:38.387457  1084 net.cpp:425] relu1 <- ip1\n",
      "I0427 14:47:38.387465  1084 net.cpp:386] relu1 -> ip1 (in-place)\n",
      "I0427 14:47:38.387473  1084 net.cpp:141] Setting up relu1\n",
      "I0427 14:47:38.387480  1084 net.cpp:148] Top shape: 100 500 (50000)\n",
      "I0427 14:47:38.387486  1084 net.cpp:156] Memory required for data: 8074800\n",
      "I0427 14:47:38.387492  1084 layer_factory.hpp:77] Creating layer ip2\n",
      "I0427 14:47:38.387503  1084 net.cpp:91] Creating Layer ip2\n",
      "I0427 14:47:38.387511  1084 net.cpp:425] ip2 <- ip1\n",
      "I0427 14:47:38.387521  1084 net.cpp:399] ip2 -> ip2\n",
      "I0427 14:47:38.387562  1084 net.cpp:141] Setting up ip2\n",
      "I0427 14:47:38.387570  1084 net.cpp:148] Top shape: 100 10 (1000)\n",
      "I0427 14:47:38.387579  1084 net.cpp:156] Memory required for data: 8078800\n",
      "I0427 14:47:38.387595  1084 layer_factory.hpp:77] Creating layer ip2_ip2_0_split\n",
      "I0427 14:47:38.387611  1084 net.cpp:91] Creating Layer ip2_ip2_0_split\n",
      "I0427 14:47:38.387622  1084 net.cpp:425] ip2_ip2_0_split <- ip2\n",
      "I0427 14:47:38.387634  1084 net.cpp:399] ip2_ip2_0_split -> ip2_ip2_0_split_0\n",
      "I0427 14:47:38.387642  1084 net.cpp:399] ip2_ip2_0_split -> ip2_ip2_0_split_1\n",
      "I0427 14:47:38.387652  1084 net.cpp:141] Setting up ip2_ip2_0_split\n",
      "I0427 14:47:38.387660  1084 net.cpp:148] Top shape: 100 10 (1000)\n",
      "I0427 14:47:38.387666  1084 net.cpp:148] Top shape: 100 10 (1000)\n",
      "I0427 14:47:38.387672  1084 net.cpp:156] Memory required for data: 8086800\n",
      "I0427 14:47:38.387678  1084 layer_factory.hpp:77] Creating layer accuracy\n",
      "I0427 14:47:38.387687  1084 net.cpp:91] Creating Layer accuracy\n",
      "I0427 14:47:38.387693  1084 net.cpp:425] accuracy <- ip2_ip2_0_split_0\n",
      "I0427 14:47:38.387701  1084 net.cpp:425] accuracy <- label_mnist_1_split_0\n",
      "I0427 14:47:38.387707  1084 net.cpp:399] accuracy -> accuracy\n",
      "I0427 14:47:38.387717  1084 net.cpp:141] Setting up accuracy\n",
      "I0427 14:47:38.387724  1084 net.cpp:148] Top shape: (1)\n",
      "I0427 14:47:38.387730  1084 net.cpp:156] Memory required for data: 8086804\n",
      "I0427 14:47:38.387737  1084 layer_factory.hpp:77] Creating layer loss\n",
      "I0427 14:47:38.387747  1084 net.cpp:91] Creating Layer loss\n",
      "I0427 14:47:38.387753  1084 net.cpp:425] loss <- ip2_ip2_0_split_1\n",
      "I0427 14:47:38.387759  1084 net.cpp:425] loss <- label_mnist_1_split_1\n",
      "I0427 14:47:38.387771  1084 net.cpp:399] loss -> loss\n",
      "I0427 14:47:38.387790  1084 layer_factory.hpp:77] Creating layer loss\n",
      "I0427 14:47:38.387814  1084 net.cpp:141] Setting up loss\n",
      "I0427 14:47:38.387825  1084 net.cpp:148] Top shape: (1)\n",
      "I0427 14:47:38.387841  1084 net.cpp:151]     with loss weight 1\n",
      "I0427 14:47:38.387851  1084 net.cpp:156] Memory required for data: 8086808\n",
      "I0427 14:47:38.387857  1084 net.cpp:217] loss needs backward computation.\n",
      "I0427 14:47:38.387864  1084 net.cpp:219] accuracy does not need backward computation.\n",
      "I0427 14:47:38.387871  1084 net.cpp:217] ip2_ip2_0_split needs backward computation.\n",
      "I0427 14:47:38.387877  1084 net.cpp:217] ip2 needs backward computation.\n",
      "I0427 14:47:38.387883  1084 net.cpp:217] relu1 needs backward computation.\n",
      "I0427 14:47:38.387889  1084 net.cpp:217] ip1 needs backward computation.\n",
      "I0427 14:47:38.387895  1084 net.cpp:217] pool2 needs backward computation.\n",
      "I0427 14:47:38.387902  1084 net.cpp:217] conv2 needs backward computation.\n",
      "I0427 14:47:38.387907  1084 net.cpp:217] pool1 needs backward computation.\n",
      "I0427 14:47:38.387913  1084 net.cpp:217] conv1 needs backward computation.\n",
      "I0427 14:47:38.387920  1084 net.cpp:219] label_mnist_1_split does not need backward computation.\n",
      "I0427 14:47:38.387928  1084 net.cpp:219] mnist does not need backward computation.\n",
      "I0427 14:47:38.387933  1084 net.cpp:261] This network produces output accuracy\n",
      "I0427 14:47:38.387939  1084 net.cpp:261] This network produces output loss\n",
      "I0427 14:47:38.387954  1084 net.cpp:274] Network initialization done.\n",
      "I0427 14:47:38.388005  1084 solver.cpp:60] Solver scaffolding done.\n",
      "I0427 14:47:38.388028  1084 caffe.cpp:219] Starting Optimization\n",
      "I0427 14:47:38.388036  1084 solver.cpp:279] Solving LeNet\n",
      "I0427 14:47:38.388042  1084 solver.cpp:280] Learning Rate Policy: inv\n",
      "I0427 14:47:38.388654  1084 solver.cpp:337] Iteration 0, Testing net (#0)\n",
      "I0427 14:47:42.472014  1084 solver.cpp:404]     Test net output #0: accuracy = 0.1169\n",
      "I0427 14:47:42.472067  1084 solver.cpp:404]     Test net output #1: loss = 2.36211 (* 1 = 2.36211 loss)\n",
      "I0427 14:47:42.542270  1084 solver.cpp:228] Iteration 0, loss = 2.3602\n",
      "I0427 14:47:42.542311  1084 solver.cpp:244]     Train net output #0: loss = 2.3602 (* 1 = 2.3602 loss)\n",
      "I0427 14:47:42.542336  1084 sgd_solver.cpp:106] Iteration 0, lr = 0.01\n",
      "I0427 14:47:49.246477  1084 solver.cpp:228] Iteration 100, loss = 0.206195\n",
      "I0427 14:47:49.246527  1084 solver.cpp:244]     Train net output #0: loss = 0.206195 (* 1 = 0.206195 loss)\n",
      "I0427 14:47:49.246546  1084 sgd_solver.cpp:106] Iteration 100, lr = 0.00992565\n",
      "I0427 14:47:55.952998  1084 solver.cpp:228] Iteration 200, loss = 0.13826\n",
      "I0427 14:47:55.953063  1084 solver.cpp:244]     Train net output #0: loss = 0.13826 (* 1 = 0.13826 loss)\n",
      "I0427 14:47:55.953080  1084 sgd_solver.cpp:106] Iteration 200, lr = 0.00985258\n",
      "I0427 14:48:02.670298  1084 solver.cpp:228] Iteration 300, loss = 0.18043\n",
      "I0427 14:48:02.670344  1084 solver.cpp:244]     Train net output #0: loss = 0.18043 (* 1 = 0.18043 loss)\n",
      "I0427 14:48:02.670361  1084 sgd_solver.cpp:106] Iteration 300, lr = 0.00978075\n",
      "I0427 14:48:09.381314  1084 solver.cpp:228] Iteration 400, loss = 0.0778412\n",
      "I0427 14:48:09.381534  1084 solver.cpp:244]     Train net output #0: loss = 0.0778412 (* 1 = 0.0778412 loss)\n",
      "I0427 14:48:09.381559  1084 sgd_solver.cpp:106] Iteration 400, lr = 0.00971013\n",
      "I0427 14:48:16.031733  1084 solver.cpp:337] Iteration 500, Testing net (#0)\n",
      "I0427 14:48:20.011384  1084 solver.cpp:404]     Test net output #0: accuracy = 0.9716\n",
      "I0427 14:48:20.011430  1084 solver.cpp:404]     Test net output #1: loss = 0.0900412 (* 1 = 0.0900412 loss)\n",
      "I0427 14:48:20.077278  1084 solver.cpp:228] Iteration 500, loss = 0.0913506\n",
      "I0427 14:48:20.077332  1084 solver.cpp:244]     Train net output #0: loss = 0.0913506 (* 1 = 0.0913506 loss)\n",
      "I0427 14:48:20.077353  1084 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069\n",
      "I0427 14:48:26.788036  1084 solver.cpp:228] Iteration 600, loss = 0.104088\n",
      "I0427 14:48:26.788082  1084 solver.cpp:244]     Train net output #0: loss = 0.104088 (* 1 = 0.104088 loss)\n",
      "I0427 14:48:26.788100  1084 sgd_solver.cpp:106] Iteration 600, lr = 0.0095724\n",
      "I0427 14:48:33.502512  1084 solver.cpp:228] Iteration 700, loss = 0.0960012\n",
      "I0427 14:48:33.502558  1084 solver.cpp:244]     Train net output #0: loss = 0.0960012 (* 1 = 0.0960012 loss)\n",
      "I0427 14:48:33.502576  1084 sgd_solver.cpp:106] Iteration 700, lr = 0.00950522\n",
      "I0427 14:48:40.205106  1084 solver.cpp:228] Iteration 800, loss = 0.208507\n",
      "I0427 14:48:40.205296  1084 solver.cpp:244]     Train net output #0: loss = 0.208507 (* 1 = 0.208507 loss)\n",
      "I0427 14:48:40.205315  1084 sgd_solver.cpp:106] Iteration 800, lr = 0.00943913\n",
      "I0427 14:48:46.886361  1084 solver.cpp:228] Iteration 900, loss = 0.13226\n",
      "I0427 14:48:46.886406  1084 solver.cpp:244]     Train net output #0: loss = 0.13226 (* 1 = 0.13226 loss)\n",
      "I0427 14:48:46.886423  1084 sgd_solver.cpp:106] Iteration 900, lr = 0.00937411\n",
      "I0427 14:48:53.513088  1084 solver.cpp:337] Iteration 1000, Testing net (#0)\n",
      "I0427 14:48:57.506093  1084 solver.cpp:404]     Test net output #0: accuracy = 0.9807\n",
      "I0427 14:48:57.506139  1084 solver.cpp:404]     Test net output #1: loss = 0.0605413 (* 1 = 0.0605413 loss)\n",
      "I0427 14:48:57.572262  1084 solver.cpp:228] Iteration 1000, loss = 0.0957608\n",
      "I0427 14:48:57.572298  1084 solver.cpp:244]     Train net output #0: loss = 0.0957608 (* 1 = 0.0957608 loss)\n",
      "I0427 14:48:57.572317  1084 sgd_solver.cpp:106] Iteration 1000, lr = 0.00931012\n",
      "I0427 14:49:04.248670  1084 solver.cpp:228] Iteration 1100, loss = 0.00487162\n",
      "I0427 14:49:04.248720  1084 solver.cpp:244]     Train net output #0: loss = 0.00487163 (* 1 = 0.00487163 loss)\n",
      "I0427 14:49:04.248739  1084 sgd_solver.cpp:106] Iteration 1100, lr = 0.00924715\n",
      "I0427 14:49:10.900952  1084 solver.cpp:228] Iteration 1200, loss = 0.0142693\n",
      "I0427 14:49:10.901156  1084 solver.cpp:244]     Train net output #0: loss = 0.0142693 (* 1 = 0.0142693 loss)\n",
      "I0427 14:49:10.901176  1084 sgd_solver.cpp:106] Iteration 1200, lr = 0.00918515\n",
      "I0427 14:49:17.566193  1084 solver.cpp:228] Iteration 1300, loss = 0.0194645\n",
      "I0427 14:49:17.566254  1084 solver.cpp:244]     Train net output #0: loss = 0.0194645 (* 1 = 0.0194645 loss)\n",
      "I0427 14:49:17.566265  1084 sgd_solver.cpp:106] Iteration 1300, lr = 0.00912412\n",
      "I0427 14:49:24.096998  1084 solver.cpp:228] Iteration 1400, loss = 0.0138808\n",
      "I0427 14:49:24.097054  1084 solver.cpp:244]     Train net output #0: loss = 0.0138809 (* 1 = 0.0138809 loss)\n",
      "I0427 14:49:24.097064  1084 sgd_solver.cpp:106] Iteration 1400, lr = 0.00906403\n",
      "I0427 14:49:30.714573  1084 solver.cpp:337] Iteration 1500, Testing net (#0)\n",
      "I0427 14:49:34.742280  1084 solver.cpp:404]     Test net output #0: accuracy = 0.9846\n",
      "I0427 14:49:34.742341  1084 solver.cpp:404]     Test net output #1: loss = 0.0500739 (* 1 = 0.0500739 loss)\n",
      "I0427 14:49:34.809615  1084 solver.cpp:228] Iteration 1500, loss = 0.074626\n",
      "I0427 14:49:34.809664  1084 solver.cpp:244]     Train net output #0: loss = 0.074626 (* 1 = 0.074626 loss)\n",
      "I0427 14:49:34.809676  1084 sgd_solver.cpp:106] Iteration 1500, lr = 0.00900485\n",
      "I0427 14:49:41.636524  1084 solver.cpp:228] Iteration 1600, loss = 0.0604374\n",
      "I0427 14:49:41.636706  1084 solver.cpp:244]     Train net output #0: loss = 0.0604375 (* 1 = 0.0604375 loss)\n",
      "I0427 14:49:41.636720  1084 sgd_solver.cpp:106] Iteration 1600, lr = 0.00894657\n",
      "I0427 14:49:48.463080  1084 solver.cpp:228] Iteration 1700, loss = 0.0267\n",
      "I0427 14:49:48.463124  1084 solver.cpp:244]     Train net output #0: loss = 0.0267001 (* 1 = 0.0267001 loss)\n",
      "I0427 14:49:48.463135  1084 sgd_solver.cpp:106] Iteration 1700, lr = 0.00888916\n",
      "I0427 14:49:55.282915  1084 solver.cpp:228] Iteration 1800, loss = 0.0249361\n",
      "I0427 14:49:55.282970  1084 solver.cpp:244]     Train net output #0: loss = 0.0249362 (* 1 = 0.0249362 loss)\n",
      "I0427 14:49:55.282980  1084 sgd_solver.cpp:106] Iteration 1800, lr = 0.0088326\n",
      "I0427 14:50:02.120139  1084 solver.cpp:228] Iteration 1900, loss = 0.12321\n",
      "I0427 14:50:02.120203  1084 solver.cpp:244]     Train net output #0: loss = 0.12321 (* 1 = 0.12321 loss)\n",
      "I0427 14:50:02.120213  1084 sgd_solver.cpp:106] Iteration 1900, lr = 0.00877687\n",
      "I0427 14:50:08.901063  1084 solver.cpp:337] Iteration 2000, Testing net (#0)\n",
      "I0427 14:50:13.176399  1084 solver.cpp:404]     Test net output #0: accuracy = 0.9866\n",
      "I0427 14:50:13.176591  1084 solver.cpp:404]     Test net output #1: loss = 0.0425273 (* 1 = 0.0425273 loss)\n",
      "I0427 14:50:13.246148  1084 solver.cpp:228] Iteration 2000, loss = 0.0154341\n",
      "I0427 14:50:13.246191  1084 solver.cpp:244]     Train net output #0: loss = 0.0154341 (* 1 = 0.0154341 loss)\n",
      "I0427 14:50:13.246204  1084 sgd_solver.cpp:106] Iteration 2000, lr = 0.00872196\n",
      "I0427 14:50:20.057240  1084 solver.cpp:228] Iteration 2100, loss = 0.00946417\n",
      "I0427 14:50:20.057283  1084 solver.cpp:244]     Train net output #0: loss = 0.00946423 (* 1 = 0.00946423 loss)\n",
      "I0427 14:50:20.057294  1084 sgd_solver.cpp:106] Iteration 2100, lr = 0.00866784\n",
      "I0427 14:50:26.868160  1084 solver.cpp:228] Iteration 2200, loss = 0.0145872\n",
      "I0427 14:50:26.868206  1084 solver.cpp:244]     Train net output #0: loss = 0.0145872 (* 1 = 0.0145872 loss)\n",
      "I0427 14:50:26.868216  1084 sgd_solver.cpp:106] Iteration 2200, lr = 0.0086145\n",
      "I0427 14:50:33.675540  1084 solver.cpp:228] Iteration 2300, loss = 0.119179\n",
      "I0427 14:50:33.675583  1084 solver.cpp:244]     Train net output #0: loss = 0.119179 (* 1 = 0.119179 loss)\n",
      "I0427 14:50:33.675593  1084 sgd_solver.cpp:106] Iteration 2300, lr = 0.00856192\n",
      "I0427 14:50:40.490201  1084 solver.cpp:228] Iteration 2400, loss = 0.0134924\n",
      "I0427 14:50:40.490257  1084 solver.cpp:244]     Train net output #0: loss = 0.0134924 (* 1 = 0.0134924 loss)\n",
      "I0427 14:50:40.490268  1084 sgd_solver.cpp:106] Iteration 2400, lr = 0.00851008\n",
      "I0427 14:50:47.238430  1084 solver.cpp:337] Iteration 2500, Testing net (#0)\n",
      "I0427 14:50:51.318701  1084 solver.cpp:404]     Test net output #0: accuracy = 0.9836\n",
      "I0427 14:50:51.318765  1084 solver.cpp:404]     Test net output #1: loss = 0.0511028 (* 1 = 0.0511028 loss)\n",
      "I0427 14:50:51.388098  1084 solver.cpp:228] Iteration 2500, loss = 0.0311495\n",
      "I0427 14:50:51.388160  1084 solver.cpp:244]     Train net output #0: loss = 0.0311496 (* 1 = 0.0311496 loss)\n",
      "I0427 14:50:51.388175  1084 sgd_solver.cpp:106] Iteration 2500, lr = 0.00845897\n",
      "I0427 14:50:58.244319  1084 solver.cpp:228] Iteration 2600, loss = 0.0367285\n",
      "I0427 14:50:58.244370  1084 solver.cpp:244]     Train net output #0: loss = 0.0367285 (* 1 = 0.0367285 loss)\n",
      "I0427 14:50:58.244381  1084 sgd_solver.cpp:106] Iteration 2600, lr = 0.00840857\n",
      "I0427 14:51:05.120021  1084 solver.cpp:228] Iteration 2700, loss = 0.0998714\n",
      "I0427 14:51:05.120074  1084 solver.cpp:244]     Train net output #0: loss = 0.0998715 (* 1 = 0.0998715 loss)\n",
      "I0427 14:51:05.120085  1084 sgd_solver.cpp:106] Iteration 2700, lr = 0.00835886\n",
      "I0427 14:51:12.006011  1084 solver.cpp:228] Iteration 2800, loss = 0.00402059\n",
      "I0427 14:51:12.006068  1084 solver.cpp:244]     Train net output #0: loss = 0.00402063 (* 1 = 0.00402063 loss)\n",
      "I0427 14:51:12.006080  1084 sgd_solver.cpp:106] Iteration 2800, lr = 0.00830984\n",
      "I0427 14:51:18.910115  1084 solver.cpp:228] Iteration 2900, loss = 0.0232503\n",
      "I0427 14:51:18.910346  1084 solver.cpp:244]     Train net output #0: loss = 0.0232503 (* 1 = 0.0232503 loss)\n",
      "I0427 14:51:18.910361  1084 sgd_solver.cpp:106] Iteration 2900, lr = 0.00826148\n",
      "I0427 14:51:25.848238  1084 solver.cpp:337] Iteration 3000, Testing net (#0)\n",
      "I0427 14:51:30.035923  1084 solver.cpp:404]     Test net output #0: accuracy = 0.9865\n",
      "I0427 14:51:30.035972  1084 solver.cpp:404]     Test net output #1: loss = 0.0386694 (* 1 = 0.0386694 loss)\n",
      "I0427 14:51:30.104941  1084 solver.cpp:228] Iteration 3000, loss = 0.029219\n",
      "I0427 14:51:30.104989  1084 solver.cpp:244]     Train net output #0: loss = 0.029219 (* 1 = 0.029219 loss)\n",
      "I0427 14:51:30.105002  1084 sgd_solver.cpp:106] Iteration 3000, lr = 0.00821377\n",
      "I0427 14:51:36.926067  1084 solver.cpp:228] Iteration 3100, loss = 0.0106949\n",
      "I0427 14:51:36.926127  1084 solver.cpp:244]     Train net output #0: loss = 0.0106949 (* 1 = 0.0106949 loss)\n",
      "I0427 14:51:36.926138  1084 sgd_solver.cpp:106] Iteration 3100, lr = 0.0081667\n",
      "I0427 14:51:43.771035  1084 solver.cpp:228] Iteration 3200, loss = 0.005835\n",
      "I0427 14:51:43.771095  1084 solver.cpp:244]     Train net output #0: loss = 0.00583503 (* 1 = 0.00583503 loss)\n",
      "I0427 14:51:43.771106  1084 sgd_solver.cpp:106] Iteration 3200, lr = 0.00812025\n",
      "I0427 14:51:50.590833  1084 solver.cpp:228] Iteration 3300, loss = 0.00693946\n",
      "I0427 14:51:50.591034  1084 solver.cpp:244]     Train net output #0: loss = 0.00693949 (* 1 = 0.00693949 loss)\n",
      "I0427 14:51:50.591048  1084 sgd_solver.cpp:106] Iteration 3300, lr = 0.00807442\n",
      "I0427 14:51:57.414422  1084 solver.cpp:228] Iteration 3400, loss = 0.00849651\n",
      "I0427 14:51:57.414484  1084 solver.cpp:244]     Train net output #0: loss = 0.00849656 (* 1 = 0.00849656 loss)\n",
      "I0427 14:51:57.414494  1084 sgd_solver.cpp:106] Iteration 3400, lr = 0.00802918\n",
      "I0427 14:52:04.222517  1084 solver.cpp:337] Iteration 3500, Testing net (#0)\n",
      "I0427 14:52:08.406395  1084 solver.cpp:404]     Test net output #0: accuracy = 0.987\n",
      "I0427 14:52:08.406461  1084 solver.cpp:404]     Test net output #1: loss = 0.0393552 (* 1 = 0.0393552 loss)\n",
      "I0427 14:52:08.473953  1084 solver.cpp:228] Iteration 3500, loss = 0.00500615\n",
      "I0427 14:52:08.474000  1084 solver.cpp:244]     Train net output #0: loss = 0.00500619 (* 1 = 0.00500619 loss)\n",
      "I0427 14:52:08.474014  1084 sgd_solver.cpp:106] Iteration 3500, lr = 0.00798454\n",
      "I0427 14:52:15.310782  1084 solver.cpp:228] Iteration 3600, loss = 0.0371368\n",
      "I0427 14:52:15.310840  1084 solver.cpp:244]     Train net output #0: loss = 0.0371369 (* 1 = 0.0371369 loss)\n",
      "I0427 14:52:15.310852  1084 sgd_solver.cpp:106] Iteration 3600, lr = 0.00794046\n",
      "^CI0427 14:52:17.299170  1084 solver.cpp:454] Snapshotting to binary proto file ./lenet/lenet_iter_3630.caffemodel\n",
      "I0427 14:52:17.303726  1084 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./lenet/lenet_iter_3630.solverstate\n",
      "I0427 14:52:17.306079  1084 solver.cpp:301] Optimization stopped early.\n",
      "I0427 14:52:17.306092  1084 caffe.cpp:222] Optimization Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!./caffe/build/tools/caffe train --solver=./lenet/lenet_solver.prototxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caffe에서는 예기치 않은 이유로 학습이 중단되었을 경우 저장된 지점(.solverstate 파일)이 있다면 이어서 학습이 가능합니다. (Snapshot resume 기능)\n",
    "\n",
    "이미 5000번까지 학습된 Solver state가 저장되어 있으므로 아래의 명령어를 통해 iteration 5000번부터 이어서 학습이 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libdc1394 error: Failed to initialize libdc1394\n",
      "I0427 14:52:17.499214  1090 caffe.cpp:178] Use CPU.\n",
      "I0427 14:52:17.499536  1090 solver.cpp:48] Initializing solver from parameters: \n",
      "test_iter: 100\n",
      "test_interval: 500\n",
      "base_lr: 0.01\n",
      "display: 100\n",
      "max_iter: 10000\n",
      "lr_policy: \"inv\"\n",
      "gamma: 0.0001\n",
      "power: 0.75\n",
      "momentum: 0.9\n",
      "weight_decay: 0.0005\n",
      "snapshot: 5000\n",
      "snapshot_prefix: \"./lenet/lenet\"\n",
      "solver_mode: CPU\n",
      "net: \"./lenet/lenet_train_test.prototxt\"\n",
      "I0427 14:52:17.499645  1090 solver.cpp:91] Creating training net from net file: ./lenet/lenet_train_test.prototxt\n",
      "I0427 14:52:17.499848  1090 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist\n",
      "I0427 14:52:17.499872  1090 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy\n",
      "I0427 14:52:17.499933  1090 net.cpp:49] Initializing net from parameters: \n",
      "name: \"LeNet\"\n",
      "state {\n",
      "  phase: TRAIN\n",
      "}\n",
      "layer {\n",
      "  name: \"mnist\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TRAIN\n",
      "  }\n",
      "  transform_param {\n",
      "    scale: 0.00390625\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"./lenet/mnist_train_lmdb\"\n",
      "    batch_size: 64\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 20\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 50\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"ip1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"ip1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 500\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"ip2\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 10\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"ip2\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I0427 14:52:17.500267  1090 layer_factory.hpp:77] Creating layer mnist\n",
      "I0427 14:52:17.500617  1090 net.cpp:91] Creating Layer mnist\n",
      "I0427 14:52:17.500646  1090 net.cpp:399] mnist -> data\n",
      "I0427 14:52:17.500669  1090 net.cpp:399] mnist -> label\n",
      "I0427 14:52:17.500732  1091 db_lmdb.cpp:38] Opened lmdb ./lenet/mnist_train_lmdb\n",
      "I0427 14:52:17.500787  1090 data_layer.cpp:41] output data size: 64,1,28,28\n",
      "I0427 14:52:17.501118  1090 net.cpp:141] Setting up mnist\n",
      "I0427 14:52:17.501135  1090 net.cpp:148] Top shape: 64 1 28 28 (50176)\n",
      "I0427 14:52:17.501145  1090 net.cpp:148] Top shape: 64 (64)\n",
      "I0427 14:52:17.501152  1090 net.cpp:156] Memory required for data: 200960\n",
      "I0427 14:52:17.501162  1090 layer_factory.hpp:77] Creating layer conv1\n",
      "I0427 14:52:17.501176  1090 net.cpp:91] Creating Layer conv1\n",
      "I0427 14:52:17.501185  1090 net.cpp:425] conv1 <- data\n",
      "I0427 14:52:17.501197  1090 net.cpp:399] conv1 -> conv1\n",
      "I0427 14:52:17.501237  1090 net.cpp:141] Setting up conv1\n",
      "I0427 14:52:17.501250  1090 net.cpp:148] Top shape: 64 20 24 24 (737280)\n",
      "I0427 14:52:17.501256  1090 net.cpp:156] Memory required for data: 3150080\n",
      "I0427 14:52:17.501272  1090 layer_factory.hpp:77] Creating layer pool1\n",
      "I0427 14:52:17.501283  1090 net.cpp:91] Creating Layer pool1\n",
      "I0427 14:52:17.501291  1090 net.cpp:425] pool1 <- conv1\n",
      "I0427 14:52:17.501298  1090 net.cpp:399] pool1 -> pool1\n",
      "I0427 14:52:17.501317  1090 net.cpp:141] Setting up pool1\n",
      "I0427 14:52:17.501340  1090 net.cpp:148] Top shape: 64 20 12 12 (184320)\n",
      "I0427 14:52:17.501348  1090 net.cpp:156] Memory required for data: 3887360\n",
      "I0427 14:52:17.501355  1090 layer_factory.hpp:77] Creating layer conv2\n",
      "I0427 14:52:17.501366  1090 net.cpp:91] Creating Layer conv2\n",
      "I0427 14:52:17.501374  1090 net.cpp:425] conv2 <- pool1\n",
      "I0427 14:52:17.501384  1090 net.cpp:399] conv2 -> conv2\n",
      "I0427 14:52:17.501564  1090 net.cpp:141] Setting up conv2\n",
      "I0427 14:52:17.501577  1090 net.cpp:148] Top shape: 64 50 8 8 (204800)\n",
      "I0427 14:52:17.501585  1090 net.cpp:156] Memory required for data: 4706560\n",
      "I0427 14:52:17.501601  1090 layer_factory.hpp:77] Creating layer pool2\n",
      "I0427 14:52:17.501610  1090 net.cpp:91] Creating Layer pool2\n",
      "I0427 14:52:17.501618  1090 net.cpp:425] pool2 <- conv2\n",
      "I0427 14:52:17.501626  1090 net.cpp:399] pool2 -> pool2\n",
      "I0427 14:52:17.501637  1090 net.cpp:141] Setting up pool2\n",
      "I0427 14:52:17.501646  1090 net.cpp:148] Top shape: 64 50 4 4 (51200)\n",
      "I0427 14:52:17.501659  1090 net.cpp:156] Memory required for data: 4911360\n",
      "I0427 14:52:17.501665  1090 layer_factory.hpp:77] Creating layer ip1\n",
      "I0427 14:52:17.501674  1090 net.cpp:91] Creating Layer ip1\n",
      "I0427 14:52:17.501682  1090 net.cpp:425] ip1 <- pool2\n",
      "I0427 14:52:17.501690  1090 net.cpp:399] ip1 -> ip1\n",
      "I0427 14:52:17.504030  1090 net.cpp:141] Setting up ip1\n",
      "I0427 14:52:17.504056  1090 net.cpp:148] Top shape: 64 500 (32000)\n",
      "I0427 14:52:17.504063  1090 net.cpp:156] Memory required for data: 5039360\n",
      "I0427 14:52:17.504075  1090 layer_factory.hpp:77] Creating layer relu1\n",
      "I0427 14:52:17.504083  1090 net.cpp:91] Creating Layer relu1\n",
      "I0427 14:52:17.504091  1090 net.cpp:425] relu1 <- ip1\n",
      "I0427 14:52:17.504098  1090 net.cpp:386] relu1 -> ip1 (in-place)\n",
      "I0427 14:52:17.504108  1090 net.cpp:141] Setting up relu1\n",
      "I0427 14:52:17.504117  1090 net.cpp:148] Top shape: 64 500 (32000)\n",
      "I0427 14:52:17.504122  1090 net.cpp:156] Memory required for data: 5167360\n",
      "I0427 14:52:17.504129  1090 layer_factory.hpp:77] Creating layer ip2\n",
      "I0427 14:52:17.504137  1090 net.cpp:91] Creating Layer ip2\n",
      "I0427 14:52:17.504144  1090 net.cpp:425] ip2 <- ip1\n",
      "I0427 14:52:17.504153  1090 net.cpp:399] ip2 -> ip2\n",
      "I0427 14:52:17.504199  1090 net.cpp:141] Setting up ip2\n",
      "I0427 14:52:17.504207  1090 net.cpp:148] Top shape: 64 10 (640)\n",
      "I0427 14:52:17.504215  1090 net.cpp:156] Memory required for data: 5169920\n",
      "I0427 14:52:17.504222  1090 layer_factory.hpp:77] Creating layer loss\n",
      "I0427 14:52:17.504235  1090 net.cpp:91] Creating Layer loss\n",
      "I0427 14:52:17.504242  1090 net.cpp:425] loss <- ip2\n",
      "I0427 14:52:17.504250  1090 net.cpp:425] loss <- label\n",
      "I0427 14:52:17.504257  1090 net.cpp:399] loss -> loss\n",
      "I0427 14:52:17.504271  1090 layer_factory.hpp:77] Creating layer loss\n",
      "I0427 14:52:17.504287  1090 net.cpp:141] Setting up loss\n",
      "I0427 14:52:17.504295  1090 net.cpp:148] Top shape: (1)\n",
      "I0427 14:52:17.504302  1090 net.cpp:151]     with loss weight 1\n",
      "I0427 14:52:17.504315  1090 net.cpp:156] Memory required for data: 5169924\n",
      "I0427 14:52:17.504323  1090 net.cpp:217] loss needs backward computation.\n",
      "I0427 14:52:17.504329  1090 net.cpp:217] ip2 needs backward computation.\n",
      "I0427 14:52:17.504336  1090 net.cpp:217] relu1 needs backward computation.\n",
      "I0427 14:52:17.504343  1090 net.cpp:217] ip1 needs backward computation.\n",
      "I0427 14:52:17.504349  1090 net.cpp:217] pool2 needs backward computation.\n",
      "I0427 14:52:17.504356  1090 net.cpp:217] conv2 needs backward computation.\n",
      "I0427 14:52:17.504362  1090 net.cpp:217] pool1 needs backward computation.\n",
      "I0427 14:52:17.504369  1090 net.cpp:217] conv1 needs backward computation.\n",
      "I0427 14:52:17.504376  1090 net.cpp:219] mnist does not need backward computation.\n",
      "I0427 14:52:17.504382  1090 net.cpp:261] This network produces output loss\n",
      "I0427 14:52:17.504392  1090 net.cpp:274] Network initialization done.\n",
      "I0427 14:52:17.504598  1090 solver.cpp:181] Creating test net (#0) specified by net file: ./lenet/lenet_train_test.prototxt\n",
      "I0427 14:52:17.504623  1090 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist\n",
      "I0427 14:52:17.504688  1090 net.cpp:49] Initializing net from parameters: \n",
      "name: \"LeNet\"\n",
      "state {\n",
      "  phase: TEST\n",
      "}\n",
      "layer {\n",
      "  name: \"mnist\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "  transform_param {\n",
      "    scale: 0.00390625\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"./lenet/mnist_test_lmdb\"\n",
      "    batch_size: 100\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 20\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 50\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"ip1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"ip1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 500\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"ip2\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 10\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"accuracy\"\n",
      "  type: \"Accuracy\"\n",
      "  bottom: \"ip2\"\n",
      "  bottom: \"label\"\n",
      "  top: \"accuracy\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"ip2\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I0427 14:52:17.505060  1090 layer_factory.hpp:77] Creating layer mnist\n",
      "I0427 14:52:17.505146  1090 net.cpp:91] Creating Layer mnist\n",
      "I0427 14:52:17.505159  1090 net.cpp:399] mnist -> data\n",
      "I0427 14:52:17.505170  1090 net.cpp:399] mnist -> label\n",
      "I0427 14:52:17.505203  1093 db_lmdb.cpp:38] Opened lmdb ./lenet/mnist_test_lmdb\n",
      "I0427 14:52:17.505233  1090 data_layer.cpp:41] output data size: 100,1,28,28\n",
      "I0427 14:52:17.505820  1090 net.cpp:141] Setting up mnist\n",
      "I0427 14:52:17.505836  1090 net.cpp:148] Top shape: 100 1 28 28 (78400)\n",
      "I0427 14:52:17.505843  1090 net.cpp:148] Top shape: 100 (100)\n",
      "I0427 14:52:17.505851  1090 net.cpp:156] Memory required for data: 314000\n",
      "I0427 14:52:17.505858  1090 layer_factory.hpp:77] Creating layer label_mnist_1_split\n",
      "I0427 14:52:17.505868  1090 net.cpp:91] Creating Layer label_mnist_1_split\n",
      "I0427 14:52:17.505877  1090 net.cpp:425] label_mnist_1_split <- label\n",
      "I0427 14:52:17.505887  1090 net.cpp:399] label_mnist_1_split -> label_mnist_1_split_0\n",
      "I0427 14:52:17.505898  1090 net.cpp:399] label_mnist_1_split -> label_mnist_1_split_1\n",
      "I0427 14:52:17.505908  1090 net.cpp:141] Setting up label_mnist_1_split\n",
      "I0427 14:52:17.505918  1090 net.cpp:148] Top shape: 100 (100)\n",
      "I0427 14:52:17.505924  1090 net.cpp:148] Top shape: 100 (100)\n",
      "I0427 14:52:17.505931  1090 net.cpp:156] Memory required for data: 314800\n",
      "I0427 14:52:17.505937  1090 layer_factory.hpp:77] Creating layer conv1\n",
      "I0427 14:52:17.505951  1090 net.cpp:91] Creating Layer conv1\n",
      "I0427 14:52:17.505959  1090 net.cpp:425] conv1 <- data\n",
      "I0427 14:52:17.505970  1090 net.cpp:399] conv1 -> conv1\n",
      "I0427 14:52:17.505997  1090 net.cpp:141] Setting up conv1\n",
      "I0427 14:52:17.506014  1090 net.cpp:148] Top shape: 100 20 24 24 (1152000)\n",
      "I0427 14:52:17.506021  1090 net.cpp:156] Memory required for data: 4922800\n",
      "I0427 14:52:17.506032  1090 layer_factory.hpp:77] Creating layer pool1\n",
      "I0427 14:52:17.506044  1090 net.cpp:91] Creating Layer pool1\n",
      "I0427 14:52:17.506052  1090 net.cpp:425] pool1 <- conv1\n",
      "I0427 14:52:17.506060  1090 net.cpp:399] pool1 -> pool1\n",
      "I0427 14:52:17.506084  1090 net.cpp:141] Setting up pool1\n",
      "I0427 14:52:17.506096  1090 net.cpp:148] Top shape: 100 20 12 12 (288000)\n",
      "I0427 14:52:17.506108  1090 net.cpp:156] Memory required for data: 6074800\n",
      "I0427 14:52:17.506114  1090 layer_factory.hpp:77] Creating layer conv2\n",
      "I0427 14:52:17.506125  1090 net.cpp:91] Creating Layer conv2\n",
      "I0427 14:52:17.506134  1090 net.cpp:425] conv2 <- pool1\n",
      "I0427 14:52:17.506145  1090 net.cpp:399] conv2 -> conv2\n",
      "I0427 14:52:17.506312  1090 net.cpp:141] Setting up conv2\n",
      "I0427 14:52:17.506325  1090 net.cpp:148] Top shape: 100 50 8 8 (320000)\n",
      "I0427 14:52:17.506331  1090 net.cpp:156] Memory required for data: 7354800\n",
      "I0427 14:52:17.506341  1090 layer_factory.hpp:77] Creating layer pool2\n",
      "I0427 14:52:17.506351  1090 net.cpp:91] Creating Layer pool2\n",
      "I0427 14:52:17.506358  1090 net.cpp:425] pool2 <- conv2\n",
      "I0427 14:52:17.506369  1090 net.cpp:399] pool2 -> pool2\n",
      "I0427 14:52:17.506381  1090 net.cpp:141] Setting up pool2\n",
      "I0427 14:52:17.506388  1090 net.cpp:148] Top shape: 100 50 4 4 (80000)\n",
      "I0427 14:52:17.506395  1090 net.cpp:156] Memory required for data: 7674800\n",
      "I0427 14:52:17.506402  1090 layer_factory.hpp:77] Creating layer ip1\n",
      "I0427 14:52:17.506410  1090 net.cpp:91] Creating Layer ip1\n",
      "I0427 14:52:17.506417  1090 net.cpp:425] ip1 <- pool2\n",
      "I0427 14:52:17.506428  1090 net.cpp:399] ip1 -> ip1\n",
      "I0427 14:52:17.508858  1090 net.cpp:141] Setting up ip1\n",
      "I0427 14:52:17.508873  1090 net.cpp:148] Top shape: 100 500 (50000)\n",
      "I0427 14:52:17.508880  1090 net.cpp:156] Memory required for data: 7874800\n",
      "I0427 14:52:17.508891  1090 layer_factory.hpp:77] Creating layer relu1\n",
      "I0427 14:52:17.508899  1090 net.cpp:91] Creating Layer relu1\n",
      "I0427 14:52:17.508908  1090 net.cpp:425] relu1 <- ip1\n",
      "I0427 14:52:17.508918  1090 net.cpp:386] relu1 -> ip1 (in-place)\n",
      "I0427 14:52:17.508926  1090 net.cpp:141] Setting up relu1\n",
      "I0427 14:52:17.508934  1090 net.cpp:148] Top shape: 100 500 (50000)\n",
      "I0427 14:52:17.508940  1090 net.cpp:156] Memory required for data: 8074800\n",
      "I0427 14:52:17.508946  1090 layer_factory.hpp:77] Creating layer ip2\n",
      "I0427 14:52:17.508956  1090 net.cpp:91] Creating Layer ip2\n",
      "I0427 14:52:17.508963  1090 net.cpp:425] ip2 <- ip1\n",
      "I0427 14:52:17.508973  1090 net.cpp:399] ip2 -> ip2\n",
      "I0427 14:52:17.509021  1090 net.cpp:141] Setting up ip2\n",
      "I0427 14:52:17.509030  1090 net.cpp:148] Top shape: 100 10 (1000)\n",
      "I0427 14:52:17.509037  1090 net.cpp:156] Memory required for data: 8078800\n",
      "I0427 14:52:17.509045  1090 layer_factory.hpp:77] Creating layer ip2_ip2_0_split\n",
      "I0427 14:52:17.509055  1090 net.cpp:91] Creating Layer ip2_ip2_0_split\n",
      "I0427 14:52:17.509063  1090 net.cpp:425] ip2_ip2_0_split <- ip2\n",
      "I0427 14:52:17.509070  1090 net.cpp:399] ip2_ip2_0_split -> ip2_ip2_0_split_0\n",
      "I0427 14:52:17.509080  1090 net.cpp:399] ip2_ip2_0_split -> ip2_ip2_0_split_1\n",
      "I0427 14:52:17.509089  1090 net.cpp:141] Setting up ip2_ip2_0_split\n",
      "I0427 14:52:17.509098  1090 net.cpp:148] Top shape: 100 10 (1000)\n",
      "I0427 14:52:17.509105  1090 net.cpp:148] Top shape: 100 10 (1000)\n",
      "I0427 14:52:17.509111  1090 net.cpp:156] Memory required for data: 8086800\n",
      "I0427 14:52:17.509117  1090 layer_factory.hpp:77] Creating layer accuracy\n",
      "I0427 14:52:17.509126  1090 net.cpp:91] Creating Layer accuracy\n",
      "I0427 14:52:17.509132  1090 net.cpp:425] accuracy <- ip2_ip2_0_split_0\n",
      "I0427 14:52:17.509140  1090 net.cpp:425] accuracy <- label_mnist_1_split_0\n",
      "I0427 14:52:17.509147  1090 net.cpp:399] accuracy -> accuracy\n",
      "I0427 14:52:17.509158  1090 net.cpp:141] Setting up accuracy\n",
      "I0427 14:52:17.509166  1090 net.cpp:148] Top shape: (1)\n",
      "I0427 14:52:17.509172  1090 net.cpp:156] Memory required for data: 8086804\n",
      "I0427 14:52:17.509179  1090 layer_factory.hpp:77] Creating layer loss\n",
      "I0427 14:52:17.509188  1090 net.cpp:91] Creating Layer loss\n",
      "I0427 14:52:17.509196  1090 net.cpp:425] loss <- ip2_ip2_0_split_1\n",
      "I0427 14:52:17.509202  1090 net.cpp:425] loss <- label_mnist_1_split_1\n",
      "I0427 14:52:17.509210  1090 net.cpp:399] loss -> loss\n",
      "I0427 14:52:17.509220  1090 layer_factory.hpp:77] Creating layer loss\n",
      "I0427 14:52:17.509237  1090 net.cpp:141] Setting up loss\n",
      "I0427 14:52:17.509245  1090 net.cpp:148] Top shape: (1)\n",
      "I0427 14:52:17.509263  1090 net.cpp:151]     with loss weight 1\n",
      "I0427 14:52:17.509273  1090 net.cpp:156] Memory required for data: 8086808\n",
      "I0427 14:52:17.509279  1090 net.cpp:217] loss needs backward computation.\n",
      "I0427 14:52:17.509286  1090 net.cpp:219] accuracy does not need backward computation.\n",
      "I0427 14:52:17.509294  1090 net.cpp:217] ip2_ip2_0_split needs backward computation.\n",
      "I0427 14:52:17.509300  1090 net.cpp:217] ip2 needs backward computation.\n",
      "I0427 14:52:17.509306  1090 net.cpp:217] relu1 needs backward computation.\n",
      "I0427 14:52:17.509313  1090 net.cpp:217] ip1 needs backward computation.\n",
      "I0427 14:52:17.509320  1090 net.cpp:217] pool2 needs backward computation.\n",
      "I0427 14:52:17.509326  1090 net.cpp:217] conv2 needs backward computation.\n",
      "I0427 14:52:17.509332  1090 net.cpp:217] pool1 needs backward computation.\n",
      "I0427 14:52:17.509341  1090 net.cpp:217] conv1 needs backward computation.\n",
      "I0427 14:52:17.509349  1090 net.cpp:219] label_mnist_1_split does not need backward computation.\n",
      "I0427 14:52:17.509356  1090 net.cpp:219] mnist does not need backward computation.\n",
      "I0427 14:52:17.509362  1090 net.cpp:261] This network produces output accuracy\n",
      "I0427 14:52:17.509369  1090 net.cpp:261] This network produces output loss\n",
      "I0427 14:52:17.509382  1090 net.cpp:274] Network initialization done.\n",
      "I0427 14:52:17.509418  1090 solver.cpp:60] Solver scaffolding done.\n",
      "I0427 14:52:17.509438  1090 caffe.cpp:209] Resuming from ./lenet/lenet_iter_5000.solverstate\n",
      "I0427 14:52:17.515432  1090 sgd_solver.cpp:318] SGDSolver: restoring history\n",
      "I0427 14:52:17.515918  1090 caffe.cpp:219] Starting Optimization\n",
      "I0427 14:52:17.515931  1090 solver.cpp:279] Solving LeNet\n",
      "I0427 14:52:17.515938  1090 solver.cpp:280] Learning Rate Policy: inv\n",
      "I0427 14:52:17.516309  1090 solver.cpp:337] Iteration 5000, Testing net (#0)\n",
      "I0427 14:52:21.659440  1090 solver.cpp:404]     Test net output #0: accuracy = 0.9907\n",
      "I0427 14:52:21.659512  1090 solver.cpp:404]     Test net output #1: loss = 0.0288877 (* 1 = 0.0288877 loss)\n",
      "I0427 14:52:21.729460  1090 solver.cpp:228] Iteration 5000, loss = 0.014341\n",
      "I0427 14:52:21.729516  1090 solver.cpp:244]     Train net output #0: loss = 0.014341 (* 1 = 0.014341 loss)\n",
      "I0427 14:52:21.729533  1090 sgd_solver.cpp:106] Iteration 5000, lr = 0.00737788\n",
      "^C\n",
      "I0427 14:52:28.020229  1090 solver.cpp:454] Snapshotting to binary proto file ./lenet/lenet_iter_5093.caffemodel\n",
      "I0427 14:52:28.025295  1090 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./lenet/lenet_iter_5093.solverstate\n",
      "I0427 14:52:28.027691  1090 solver.cpp:301] Optimization stopped early.\n",
      "I0427 14:52:28.027705  1090 caffe.cpp:222] Optimization Done.\n"
     ]
    }
   ],
   "source": [
    "!./caffe/build/tools/caffe train --solver=./lenet/lenet_solver.prototxt --snapshot=./lenet/lenet_iter_5000.solverstate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본 과정이 모두 완료되면 10000 minibatch의 학습 후 Test dataset에서 약 99.1%의 인식 성능을 갖는 모델(./lenet/lenet_iter_10000.caffemodel)을 얻을 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ls ./lenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Testing LeNet 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libdc1394 error: Failed to initialize libdc1394\n",
      "I0427 15:17:46.006711  1096 caffe.cpp:246] Use CPU.\n",
      "I0427 15:17:46.008276  1096 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist\n",
      "I0427 15:17:46.008383  1096 net.cpp:49] Initializing net from parameters: \n",
      "name: \"LeNet\"\n",
      "state {\n",
      "  phase: TEST\n",
      "}\n",
      "layer {\n",
      "  name: \"mnist\"\n",
      "  type: \"Data\"\n",
      "  top: \"data\"\n",
      "  top: \"label\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "  transform_param {\n",
      "    scale: 0.00390625\n",
      "  }\n",
      "  data_param {\n",
      "    source: \"./lenet/mnist_test_lmdb\"\n",
      "    batch_size: 100\n",
      "    backend: LMDB\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 20\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 50\n",
      "    kernel_size: 5\n",
      "    stride: 1\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 2\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"ip1\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"ip1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 500\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"ip2\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"ip1\"\n",
      "  top: \"ip2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 10\n",
      "    weight_filler {\n",
      "      type: \"xavier\"\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"accuracy\"\n",
      "  type: \"Accuracy\"\n",
      "  bottom: \"ip2\"\n",
      "  bottom: \"label\"\n",
      "  top: \"accuracy\"\n",
      "  include {\n",
      "    phase: TEST\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"loss\"\n",
      "  type: \"SoftmaxWithLoss\"\n",
      "  bottom: \"ip2\"\n",
      "  bottom: \"label\"\n",
      "  top: \"loss\"\n",
      "}\n",
      "I0427 15:17:46.008782  1096 layer_factory.hpp:77] Creating layer mnist\n",
      "I0427 15:17:46.009163  1096 net.cpp:91] Creating Layer mnist\n",
      "I0427 15:17:46.009186  1096 net.cpp:399] mnist -> data\n",
      "I0427 15:17:46.009232  1096 net.cpp:399] mnist -> label\n",
      "I0427 15:17:46.009333  1097 db_lmdb.cpp:38] Opened lmdb ./lenet/mnist_test_lmdb\n",
      "I0427 15:17:46.009421  1096 data_layer.cpp:41] output data size: 100,1,28,28\n",
      "I0427 15:17:46.010208  1096 net.cpp:141] Setting up mnist\n",
      "I0427 15:17:46.010241  1096 net.cpp:148] Top shape: 100 1 28 28 (78400)\n",
      "I0427 15:17:46.010249  1096 net.cpp:148] Top shape: 100 (100)\n",
      "I0427 15:17:46.010257  1096 net.cpp:156] Memory required for data: 314000\n",
      "I0427 15:17:46.010267  1096 layer_factory.hpp:77] Creating layer label_mnist_1_split\n",
      "I0427 15:17:46.010277  1096 net.cpp:91] Creating Layer label_mnist_1_split\n",
      "I0427 15:17:46.010299  1096 net.cpp:425] label_mnist_1_split <- label\n",
      "I0427 15:17:46.010313  1096 net.cpp:399] label_mnist_1_split -> label_mnist_1_split_0\n",
      "I0427 15:17:46.010326  1096 net.cpp:399] label_mnist_1_split -> label_mnist_1_split_1\n",
      "I0427 15:17:46.010339  1096 net.cpp:141] Setting up label_mnist_1_split\n",
      "I0427 15:17:46.010349  1096 net.cpp:148] Top shape: 100 (100)\n",
      "I0427 15:17:46.010357  1096 net.cpp:148] Top shape: 100 (100)\n",
      "I0427 15:17:46.010365  1096 net.cpp:156] Memory required for data: 314800\n",
      "I0427 15:17:46.010371  1096 layer_factory.hpp:77] Creating layer conv1\n",
      "I0427 15:17:46.010388  1096 net.cpp:91] Creating Layer conv1\n",
      "I0427 15:17:46.010396  1096 net.cpp:425] conv1 <- data\n",
      "I0427 15:17:46.010406  1096 net.cpp:399] conv1 -> conv1\n",
      "I0427 15:17:46.010468  1096 net.cpp:141] Setting up conv1\n",
      "I0427 15:17:46.010483  1096 net.cpp:148] Top shape: 100 20 24 24 (1152000)\n",
      "I0427 15:17:46.010489  1096 net.cpp:156] Memory required for data: 4922800\n",
      "I0427 15:17:46.010507  1096 layer_factory.hpp:77] Creating layer pool1\n",
      "I0427 15:17:46.010538  1096 net.cpp:91] Creating Layer pool1\n",
      "I0427 15:17:46.010547  1096 net.cpp:425] pool1 <- conv1\n",
      "I0427 15:17:46.010556  1096 net.cpp:399] pool1 -> pool1\n",
      "I0427 15:17:46.010591  1096 net.cpp:141] Setting up pool1\n",
      "I0427 15:17:46.010607  1096 net.cpp:148] Top shape: 100 20 12 12 (288000)\n",
      "I0427 15:17:46.010618  1096 net.cpp:156] Memory required for data: 6074800\n",
      "I0427 15:17:46.010630  1096 layer_factory.hpp:77] Creating layer conv2\n",
      "I0427 15:17:46.010643  1096 net.cpp:91] Creating Layer conv2\n",
      "I0427 15:17:46.010649  1096 net.cpp:425] conv2 <- pool1\n",
      "I0427 15:17:46.010658  1096 net.cpp:399] conv2 -> conv2\n",
      "I0427 15:17:46.010829  1096 net.cpp:141] Setting up conv2\n",
      "I0427 15:17:46.010841  1096 net.cpp:148] Top shape: 100 50 8 8 (320000)\n",
      "I0427 15:17:46.010848  1096 net.cpp:156] Memory required for data: 7354800\n",
      "I0427 15:17:46.010860  1096 layer_factory.hpp:77] Creating layer pool2\n",
      "I0427 15:17:46.010869  1096 net.cpp:91] Creating Layer pool2\n",
      "I0427 15:17:46.010877  1096 net.cpp:425] pool2 <- conv2\n",
      "I0427 15:17:46.010893  1096 net.cpp:399] pool2 -> pool2\n",
      "I0427 15:17:46.010905  1096 net.cpp:141] Setting up pool2\n",
      "I0427 15:17:46.010913  1096 net.cpp:148] Top shape: 100 50 4 4 (80000)\n",
      "I0427 15:17:46.010921  1096 net.cpp:156] Memory required for data: 7674800\n",
      "I0427 15:17:46.010927  1096 layer_factory.hpp:77] Creating layer ip1\n",
      "I0427 15:17:46.010938  1096 net.cpp:91] Creating Layer ip1\n",
      "I0427 15:17:46.010946  1096 net.cpp:425] ip1 <- pool2\n",
      "I0427 15:17:46.010957  1096 net.cpp:399] ip1 -> ip1\n",
      "I0427 15:17:46.013216  1096 net.cpp:141] Setting up ip1\n",
      "I0427 15:17:46.013231  1096 net.cpp:148] Top shape: 100 500 (50000)\n",
      "I0427 15:17:46.013250  1096 net.cpp:156] Memory required for data: 7874800\n",
      "I0427 15:17:46.013262  1096 layer_factory.hpp:77] Creating layer relu1\n",
      "I0427 15:17:46.013270  1096 net.cpp:91] Creating Layer relu1\n",
      "I0427 15:17:46.013278  1096 net.cpp:425] relu1 <- ip1\n",
      "I0427 15:17:46.013285  1096 net.cpp:386] relu1 -> ip1 (in-place)\n",
      "I0427 15:17:46.013294  1096 net.cpp:141] Setting up relu1\n",
      "I0427 15:17:46.013303  1096 net.cpp:148] Top shape: 100 500 (50000)\n",
      "I0427 15:17:46.013309  1096 net.cpp:156] Memory required for data: 8074800\n",
      "I0427 15:17:46.013314  1096 layer_factory.hpp:77] Creating layer ip2\n",
      "I0427 15:17:46.013324  1096 net.cpp:91] Creating Layer ip2\n",
      "I0427 15:17:46.013331  1096 net.cpp:425] ip2 <- ip1\n",
      "I0427 15:17:46.013339  1096 net.cpp:399] ip2 -> ip2\n",
      "I0427 15:17:46.013384  1096 net.cpp:141] Setting up ip2\n",
      "I0427 15:17:46.013393  1096 net.cpp:148] Top shape: 100 10 (1000)\n",
      "I0427 15:17:46.013401  1096 net.cpp:156] Memory required for data: 8078800\n",
      "I0427 15:17:46.013408  1096 layer_factory.hpp:77] Creating layer ip2_ip2_0_split\n",
      "I0427 15:17:46.013417  1096 net.cpp:91] Creating Layer ip2_ip2_0_split\n",
      "I0427 15:17:46.013423  1096 net.cpp:425] ip2_ip2_0_split <- ip2\n",
      "I0427 15:17:46.013432  1096 net.cpp:399] ip2_ip2_0_split -> ip2_ip2_0_split_0\n",
      "I0427 15:17:46.013440  1096 net.cpp:399] ip2_ip2_0_split -> ip2_ip2_0_split_1\n",
      "I0427 15:17:46.013450  1096 net.cpp:141] Setting up ip2_ip2_0_split\n",
      "I0427 15:17:46.013458  1096 net.cpp:148] Top shape: 100 10 (1000)\n",
      "I0427 15:17:46.013464  1096 net.cpp:148] Top shape: 100 10 (1000)\n",
      "I0427 15:17:46.013470  1096 net.cpp:156] Memory required for data: 8086800\n",
      "I0427 15:17:46.013478  1096 layer_factory.hpp:77] Creating layer accuracy\n",
      "I0427 15:17:46.013487  1096 net.cpp:91] Creating Layer accuracy\n",
      "I0427 15:17:46.013494  1096 net.cpp:425] accuracy <- ip2_ip2_0_split_0\n",
      "I0427 15:17:46.013502  1096 net.cpp:425] accuracy <- label_mnist_1_split_0\n",
      "I0427 15:17:46.013510  1096 net.cpp:399] accuracy -> accuracy\n",
      "I0427 15:17:46.013520  1096 net.cpp:141] Setting up accuracy\n",
      "I0427 15:17:46.013528  1096 net.cpp:148] Top shape: (1)\n",
      "I0427 15:17:46.013535  1096 net.cpp:156] Memory required for data: 8086804\n",
      "I0427 15:17:46.013541  1096 layer_factory.hpp:77] Creating layer loss\n",
      "I0427 15:17:46.013561  1096 net.cpp:91] Creating Layer loss\n",
      "I0427 15:17:46.013571  1096 net.cpp:425] loss <- ip2_ip2_0_split_1\n",
      "I0427 15:17:46.013578  1096 net.cpp:425] loss <- label_mnist_1_split_1\n",
      "I0427 15:17:46.013589  1096 net.cpp:399] loss -> loss\n",
      "I0427 15:17:46.013613  1096 layer_factory.hpp:77] Creating layer loss\n",
      "I0427 15:17:46.013633  1096 net.cpp:141] Setting up loss\n",
      "I0427 15:17:46.013643  1096 net.cpp:148] Top shape: (1)\n",
      "I0427 15:17:46.013648  1096 net.cpp:151]     with loss weight 1\n",
      "I0427 15:17:46.013661  1096 net.cpp:156] Memory required for data: 8086808\n",
      "I0427 15:17:46.013669  1096 net.cpp:217] loss needs backward computation.\n",
      "I0427 15:17:46.013675  1096 net.cpp:219] accuracy does not need backward computation.\n",
      "I0427 15:17:46.013682  1096 net.cpp:217] ip2_ip2_0_split needs backward computation.\n",
      "I0427 15:17:46.013689  1096 net.cpp:217] ip2 needs backward computation.\n",
      "I0427 15:17:46.013695  1096 net.cpp:217] relu1 needs backward computation.\n",
      "I0427 15:17:46.013702  1096 net.cpp:217] ip1 needs backward computation.\n",
      "I0427 15:17:46.013708  1096 net.cpp:217] pool2 needs backward computation.\n",
      "I0427 15:17:46.013715  1096 net.cpp:217] conv2 needs backward computation.\n",
      "I0427 15:17:46.013721  1096 net.cpp:217] pool1 needs backward computation.\n",
      "I0427 15:17:46.013728  1096 net.cpp:217] conv1 needs backward computation.\n",
      "I0427 15:17:46.013736  1096 net.cpp:219] label_mnist_1_split does not need backward computation.\n",
      "I0427 15:17:46.013742  1096 net.cpp:219] mnist does not need backward computation.\n",
      "I0427 15:17:46.013751  1096 net.cpp:261] This network produces output accuracy\n",
      "I0427 15:17:46.013757  1096 net.cpp:261] This network produces output loss\n",
      "I0427 15:17:46.013769  1096 net.cpp:274] Network initialization done.\n",
      "I0427 15:17:46.016618  1096 caffe.cpp:252] Running for 100 iterations.\n",
      "I0427 15:17:46.059595  1096 caffe.cpp:275] Batch 0, accuracy = 1\n",
      "I0427 15:17:46.059618  1096 caffe.cpp:275] Batch 0, loss = 0.00781204\n",
      "I0427 15:17:46.101263  1096 caffe.cpp:275] Batch 1, accuracy = 1\n",
      "I0427 15:17:46.101281  1096 caffe.cpp:275] Batch 1, loss = 0.00797504\n",
      "I0427 15:17:46.141954  1096 caffe.cpp:275] Batch 2, accuracy = 1\n",
      "I0427 15:17:46.141971  1096 caffe.cpp:275] Batch 2, loss = 0.0101783\n",
      "I0427 15:17:46.181592  1096 caffe.cpp:275] Batch 3, accuracy = 0.99\n",
      "I0427 15:17:46.181609  1096 caffe.cpp:275] Batch 3, loss = 0.031967\n",
      "I0427 15:17:46.221434  1096 caffe.cpp:275] Batch 4, accuracy = 0.99\n",
      "I0427 15:17:46.221451  1096 caffe.cpp:275] Batch 4, loss = 0.0241223\n",
      "I0427 15:17:46.261325  1096 caffe.cpp:275] Batch 5, accuracy = 0.99\n",
      "I0427 15:17:46.261343  1096 caffe.cpp:275] Batch 5, loss = 0.0561458\n",
      "I0427 15:17:46.301146  1096 caffe.cpp:275] Batch 6, accuracy = 0.97\n",
      "I0427 15:17:46.301162  1096 caffe.cpp:275] Batch 6, loss = 0.078677\n",
      "I0427 15:17:46.340926  1096 caffe.cpp:275] Batch 7, accuracy = 0.99\n",
      "I0427 15:17:46.340942  1096 caffe.cpp:275] Batch 7, loss = 0.0229096\n",
      "I0427 15:17:46.380913  1096 caffe.cpp:275] Batch 8, accuracy = 1\n",
      "I0427 15:17:46.380931  1096 caffe.cpp:275] Batch 8, loss = 0.0133228\n",
      "I0427 15:17:46.420931  1096 caffe.cpp:275] Batch 9, accuracy = 0.98\n",
      "I0427 15:17:46.420949  1096 caffe.cpp:275] Batch 9, loss = 0.0541435\n",
      "I0427 15:17:46.460856  1096 caffe.cpp:275] Batch 10, accuracy = 0.98\n",
      "I0427 15:17:46.460872  1096 caffe.cpp:275] Batch 10, loss = 0.053926\n",
      "I0427 15:17:46.501149  1096 caffe.cpp:275] Batch 11, accuracy = 0.98\n",
      "I0427 15:17:46.501179  1096 caffe.cpp:275] Batch 11, loss = 0.0425442\n",
      "I0427 15:17:46.540983  1096 caffe.cpp:275] Batch 12, accuracy = 0.97\n",
      "I0427 15:17:46.541002  1096 caffe.cpp:275] Batch 12, loss = 0.101579\n",
      "I0427 15:17:46.581017  1096 caffe.cpp:275] Batch 13, accuracy = 0.98\n",
      "I0427 15:17:46.581051  1096 caffe.cpp:275] Batch 13, loss = 0.040417\n",
      "I0427 15:17:46.621878  1096 caffe.cpp:275] Batch 14, accuracy = 1\n",
      "I0427 15:17:46.621896  1096 caffe.cpp:275] Batch 14, loss = 0.00989321\n",
      "I0427 15:17:46.662736  1096 caffe.cpp:275] Batch 15, accuracy = 0.99\n",
      "I0427 15:17:46.662755  1096 caffe.cpp:275] Batch 15, loss = 0.0318298\n",
      "I0427 15:17:46.703567  1096 caffe.cpp:275] Batch 16, accuracy = 0.99\n",
      "I0427 15:17:46.703584  1096 caffe.cpp:275] Batch 16, loss = 0.0421189\n",
      "I0427 15:17:46.744294  1096 caffe.cpp:275] Batch 17, accuracy = 1\n",
      "I0427 15:17:46.744312  1096 caffe.cpp:275] Batch 17, loss = 0.0196156\n",
      "I0427 15:17:46.785158  1096 caffe.cpp:275] Batch 18, accuracy = 1\n",
      "I0427 15:17:46.785198  1096 caffe.cpp:275] Batch 18, loss = 0.00936466\n",
      "I0427 15:17:46.825978  1096 caffe.cpp:275] Batch 19, accuracy = 0.99\n",
      "I0427 15:17:46.826009  1096 caffe.cpp:275] Batch 19, loss = 0.0820557\n",
      "I0427 15:17:46.866806  1096 caffe.cpp:275] Batch 20, accuracy = 0.97\n",
      "I0427 15:17:46.866824  1096 caffe.cpp:275] Batch 20, loss = 0.073514\n",
      "I0427 15:17:46.907748  1096 caffe.cpp:275] Batch 21, accuracy = 0.97\n",
      "I0427 15:17:46.907766  1096 caffe.cpp:275] Batch 21, loss = 0.0711975\n",
      "I0427 15:17:46.948546  1096 caffe.cpp:275] Batch 22, accuracy = 0.99\n",
      "I0427 15:17:46.948565  1096 caffe.cpp:275] Batch 22, loss = 0.0350986\n",
      "I0427 15:17:46.989362  1096 caffe.cpp:275] Batch 23, accuracy = 0.99\n",
      "I0427 15:17:46.989379  1096 caffe.cpp:275] Batch 23, loss = 0.0245004\n",
      "I0427 15:17:47.031023  1096 caffe.cpp:275] Batch 24, accuracy = 1\n",
      "I0427 15:17:47.031041  1096 caffe.cpp:275] Batch 24, loss = 0.0256251\n",
      "I0427 15:17:47.072392  1096 caffe.cpp:275] Batch 25, accuracy = 0.98\n",
      "I0427 15:17:47.072409  1096 caffe.cpp:275] Batch 25, loss = 0.0884702\n",
      "I0427 15:17:47.113288  1096 caffe.cpp:275] Batch 26, accuracy = 0.99\n",
      "I0427 15:17:47.113306  1096 caffe.cpp:275] Batch 26, loss = 0.1174\n",
      "I0427 15:17:47.154235  1096 caffe.cpp:275] Batch 27, accuracy = 0.99\n",
      "I0427 15:17:47.154253  1096 caffe.cpp:275] Batch 27, loss = 0.0157793\n",
      "I0427 15:17:47.195003  1096 caffe.cpp:275] Batch 28, accuracy = 0.99\n",
      "I0427 15:17:47.195034  1096 caffe.cpp:275] Batch 28, loss = 0.0469445\n",
      "I0427 15:17:47.236614  1096 caffe.cpp:275] Batch 29, accuracy = 0.97\n",
      "I0427 15:17:47.236632  1096 caffe.cpp:275] Batch 29, loss = 0.0733662\n",
      "I0427 15:17:47.278144  1096 caffe.cpp:275] Batch 30, accuracy = 0.98\n",
      "I0427 15:17:47.278162  1096 caffe.cpp:275] Batch 30, loss = 0.0275676\n",
      "I0427 15:17:47.319070  1096 caffe.cpp:275] Batch 31, accuracy = 1\n",
      "I0427 15:17:47.319102  1096 caffe.cpp:275] Batch 31, loss = 0.00288306\n",
      "I0427 15:17:47.359964  1096 caffe.cpp:275] Batch 32, accuracy = 0.99\n",
      "I0427 15:17:47.359982  1096 caffe.cpp:275] Batch 32, loss = 0.0166335\n",
      "I0427 15:17:47.400893  1096 caffe.cpp:275] Batch 33, accuracy = 1\n",
      "I0427 15:17:47.400914  1096 caffe.cpp:275] Batch 33, loss = 0.00690314\n",
      "I0427 15:17:47.441859  1096 caffe.cpp:275] Batch 34, accuracy = 0.99\n",
      "I0427 15:17:47.441885  1096 caffe.cpp:275] Batch 34, loss = 0.0536583\n",
      "I0427 15:17:47.483160  1096 caffe.cpp:275] Batch 35, accuracy = 0.97\n",
      "I0427 15:17:47.483178  1096 caffe.cpp:275] Batch 35, loss = 0.0887918\n",
      "I0427 15:17:47.523696  1096 caffe.cpp:275] Batch 36, accuracy = 1\n",
      "I0427 15:17:47.523713  1096 caffe.cpp:275] Batch 36, loss = 0.00769525\n",
      "I0427 15:17:47.564527  1096 caffe.cpp:275] Batch 37, accuracy = 0.98\n",
      "I0427 15:17:47.564544  1096 caffe.cpp:275] Batch 37, loss = 0.0387033\n",
      "I0427 15:17:47.605144  1096 caffe.cpp:275] Batch 38, accuracy = 1\n",
      "I0427 15:17:47.605159  1096 caffe.cpp:275] Batch 38, loss = 0.0114513\n",
      "I0427 15:17:47.645710  1096 caffe.cpp:275] Batch 39, accuracy = 0.98\n",
      "I0427 15:17:47.645740  1096 caffe.cpp:275] Batch 39, loss = 0.0588592\n",
      "I0427 15:17:47.686324  1096 caffe.cpp:275] Batch 40, accuracy = 0.98\n",
      "I0427 15:17:47.686342  1096 caffe.cpp:275] Batch 40, loss = 0.0417956\n",
      "I0427 15:17:47.727387  1096 caffe.cpp:275] Batch 41, accuracy = 0.98\n",
      "I0427 15:17:47.727404  1096 caffe.cpp:275] Batch 41, loss = 0.0561799\n",
      "I0427 15:17:47.767992  1096 caffe.cpp:275] Batch 42, accuracy = 1\n",
      "I0427 15:17:47.768023  1096 caffe.cpp:275] Batch 42, loss = 0.0239556\n",
      "I0427 15:17:47.808593  1096 caffe.cpp:275] Batch 43, accuracy = 0.99\n",
      "I0427 15:17:47.808609  1096 caffe.cpp:275] Batch 43, loss = 0.0228025\n",
      "I0427 15:17:47.849117  1096 caffe.cpp:275] Batch 44, accuracy = 1\n",
      "I0427 15:17:47.849134  1096 caffe.cpp:275] Batch 44, loss = 0.00848996\n",
      "I0427 15:17:47.889708  1096 caffe.cpp:275] Batch 45, accuracy = 0.98\n",
      "I0427 15:17:47.889725  1096 caffe.cpp:275] Batch 45, loss = 0.046132\n",
      "I0427 15:17:47.930840  1096 caffe.cpp:275] Batch 46, accuracy = 0.99\n",
      "I0427 15:17:47.930857  1096 caffe.cpp:275] Batch 46, loss = 0.0178714\n",
      "I0427 15:17:47.971390  1096 caffe.cpp:275] Batch 47, accuracy = 0.99\n",
      "I0427 15:17:47.971407  1096 caffe.cpp:275] Batch 47, loss = 0.0114664\n",
      "I0427 15:17:48.012976  1096 caffe.cpp:275] Batch 48, accuracy = 0.98\n",
      "I0427 15:17:48.013021  1096 caffe.cpp:275] Batch 48, loss = 0.0817669\n",
      "I0427 15:17:48.053769  1096 caffe.cpp:275] Batch 49, accuracy = 1\n",
      "I0427 15:17:48.053787  1096 caffe.cpp:275] Batch 49, loss = 0.00370423\n",
      "I0427 15:17:48.095286  1096 caffe.cpp:275] Batch 50, accuracy = 1\n",
      "I0427 15:17:48.095304  1096 caffe.cpp:275] Batch 50, loss = 0.000197464\n",
      "I0427 15:17:48.135916  1096 caffe.cpp:275] Batch 51, accuracy = 1\n",
      "I0427 15:17:48.135933  1096 caffe.cpp:275] Batch 51, loss = 0.00307933\n",
      "I0427 15:17:48.176589  1096 caffe.cpp:275] Batch 52, accuracy = 1\n",
      "I0427 15:17:48.176621  1096 caffe.cpp:275] Batch 52, loss = 0.00567323\n",
      "I0427 15:17:48.217545  1096 caffe.cpp:275] Batch 53, accuracy = 1\n",
      "I0427 15:17:48.217567  1096 caffe.cpp:275] Batch 53, loss = 0.000122933\n",
      "I0427 15:17:48.258147  1096 caffe.cpp:275] Batch 54, accuracy = 1\n",
      "I0427 15:17:48.258164  1096 caffe.cpp:275] Batch 54, loss = 0.00874075\n",
      "I0427 15:17:48.298754  1096 caffe.cpp:275] Batch 55, accuracy = 1\n",
      "I0427 15:17:48.298785  1096 caffe.cpp:275] Batch 55, loss = 0.000338732\n",
      "I0427 15:17:48.339345  1096 caffe.cpp:275] Batch 56, accuracy = 1\n",
      "I0427 15:17:48.339362  1096 caffe.cpp:275] Batch 56, loss = 0.0126809\n",
      "I0427 15:17:48.380106  1096 caffe.cpp:275] Batch 57, accuracy = 1\n",
      "I0427 15:17:48.380125  1096 caffe.cpp:275] Batch 57, loss = 0.00260704\n",
      "I0427 15:17:48.420928  1096 caffe.cpp:275] Batch 58, accuracy = 1\n",
      "I0427 15:17:48.420946  1096 caffe.cpp:275] Batch 58, loss = 0.00271622\n",
      "I0427 15:17:48.461467  1096 caffe.cpp:275] Batch 59, accuracy = 0.97\n",
      "I0427 15:17:48.461485  1096 caffe.cpp:275] Batch 59, loss = 0.100013\n",
      "I0427 15:17:48.502116  1096 caffe.cpp:275] Batch 60, accuracy = 1\n",
      "I0427 15:17:48.502135  1096 caffe.cpp:275] Batch 60, loss = 0.00893568\n",
      "I0427 15:17:48.543534  1096 caffe.cpp:275] Batch 61, accuracy = 1\n",
      "I0427 15:17:48.543552  1096 caffe.cpp:275] Batch 61, loss = 0.00418302\n",
      "I0427 15:17:48.584236  1096 caffe.cpp:275] Batch 62, accuracy = 1\n",
      "I0427 15:17:48.584254  1096 caffe.cpp:275] Batch 62, loss = 3.20442e-05\n",
      "I0427 15:17:48.624958  1096 caffe.cpp:275] Batch 63, accuracy = 1\n",
      "I0427 15:17:48.624989  1096 caffe.cpp:275] Batch 63, loss = 0.000255764\n",
      "I0427 15:17:48.665604  1096 caffe.cpp:275] Batch 64, accuracy = 1\n",
      "I0427 15:17:48.665621  1096 caffe.cpp:275] Batch 64, loss = 0.000780839\n",
      "I0427 15:17:48.706120  1096 caffe.cpp:275] Batch 65, accuracy = 0.92\n",
      "I0427 15:17:48.706136  1096 caffe.cpp:275] Batch 65, loss = 0.167216\n",
      "I0427 15:17:48.747423  1096 caffe.cpp:275] Batch 66, accuracy = 0.98\n",
      "I0427 15:17:48.747442  1096 caffe.cpp:275] Batch 66, loss = 0.0705994\n",
      "I0427 15:17:48.788084  1096 caffe.cpp:275] Batch 67, accuracy = 0.99\n",
      "I0427 15:17:48.788101  1096 caffe.cpp:275] Batch 67, loss = 0.0236571\n",
      "I0427 15:17:48.828665  1096 caffe.cpp:275] Batch 68, accuracy = 1\n",
      "I0427 15:17:48.828682  1096 caffe.cpp:275] Batch 68, loss = 0.00327704\n",
      "I0427 15:17:48.869248  1096 caffe.cpp:275] Batch 69, accuracy = 1\n",
      "I0427 15:17:48.869277  1096 caffe.cpp:275] Batch 69, loss = 0.00183907\n",
      "I0427 15:17:48.909761  1096 caffe.cpp:275] Batch 70, accuracy = 1\n",
      "I0427 15:17:48.909791  1096 caffe.cpp:275] Batch 70, loss = 0.00130482\n",
      "I0427 15:17:48.950568  1096 caffe.cpp:275] Batch 71, accuracy = 1\n",
      "I0427 15:17:48.950587  1096 caffe.cpp:275] Batch 71, loss = 0.000444505\n",
      "I0427 15:17:48.991322  1096 caffe.cpp:275] Batch 72, accuracy = 0.99\n",
      "I0427 15:17:48.991358  1096 caffe.cpp:275] Batch 72, loss = 0.0096469\n",
      "I0427 15:17:49.032162  1096 caffe.cpp:275] Batch 73, accuracy = 1\n",
      "I0427 15:17:49.032179  1096 caffe.cpp:275] Batch 73, loss = 0.000162028\n",
      "I0427 15:17:49.072726  1096 caffe.cpp:275] Batch 74, accuracy = 1\n",
      "I0427 15:17:49.072743  1096 caffe.cpp:275] Batch 74, loss = 0.0034054\n",
      "I0427 15:17:49.113298  1096 caffe.cpp:275] Batch 75, accuracy = 1\n",
      "I0427 15:17:49.113315  1096 caffe.cpp:275] Batch 75, loss = 0.00256993\n",
      "I0427 15:17:49.153885  1096 caffe.cpp:275] Batch 76, accuracy = 1\n",
      "I0427 15:17:49.153911  1096 caffe.cpp:275] Batch 76, loss = 0.000244796\n",
      "I0427 15:17:49.194653  1096 caffe.cpp:275] Batch 77, accuracy = 1\n",
      "I0427 15:17:49.194669  1096 caffe.cpp:275] Batch 77, loss = 0.000277154\n",
      "I0427 15:17:49.235414  1096 caffe.cpp:275] Batch 78, accuracy = 1\n",
      "I0427 15:17:49.235431  1096 caffe.cpp:275] Batch 78, loss = 0.00142131\n",
      "I0427 15:17:49.276242  1096 caffe.cpp:275] Batch 79, accuracy = 1\n",
      "I0427 15:17:49.276271  1096 caffe.cpp:275] Batch 79, loss = 0.00390524\n",
      "I0427 15:17:49.316838  1096 caffe.cpp:275] Batch 80, accuracy = 0.99\n",
      "I0427 15:17:49.316856  1096 caffe.cpp:275] Batch 80, loss = 0.012702\n",
      "I0427 15:17:49.357458  1096 caffe.cpp:275] Batch 81, accuracy = 1\n",
      "I0427 15:17:49.357476  1096 caffe.cpp:275] Batch 81, loss = 0.00361456\n",
      "I0427 15:17:49.398146  1096 caffe.cpp:275] Batch 82, accuracy = 1\n",
      "I0427 15:17:49.398164  1096 caffe.cpp:275] Batch 82, loss = 0.000450954\n",
      "I0427 15:17:49.438819  1096 caffe.cpp:275] Batch 83, accuracy = 0.99\n",
      "I0427 15:17:49.438838  1096 caffe.cpp:275] Batch 83, loss = 0.0147965\n",
      "I0427 15:17:49.479651  1096 caffe.cpp:275] Batch 84, accuracy = 0.99\n",
      "I0427 15:17:49.479668  1096 caffe.cpp:275] Batch 84, loss = 0.0154673\n",
      "I0427 15:17:49.520279  1096 caffe.cpp:275] Batch 85, accuracy = 0.99\n",
      "I0427 15:17:49.520297  1096 caffe.cpp:275] Batch 85, loss = 0.0233588\n",
      "I0427 15:17:49.560979  1096 caffe.cpp:275] Batch 86, accuracy = 1\n",
      "I0427 15:17:49.561009  1096 caffe.cpp:275] Batch 86, loss = 7.36738e-05\n",
      "I0427 15:17:49.602387  1096 caffe.cpp:275] Batch 87, accuracy = 1\n",
      "I0427 15:17:49.602406  1096 caffe.cpp:275] Batch 87, loss = 6.85151e-05\n",
      "I0427 15:17:49.643029  1096 caffe.cpp:275] Batch 88, accuracy = 1\n",
      "I0427 15:17:49.643045  1096 caffe.cpp:275] Batch 88, loss = 1.98799e-05\n",
      "I0427 15:17:49.683712  1096 caffe.cpp:275] Batch 89, accuracy = 1\n",
      "I0427 15:17:49.683740  1096 caffe.cpp:275] Batch 89, loss = 5.99768e-05\n",
      "I0427 15:17:49.724328  1096 caffe.cpp:275] Batch 90, accuracy = 0.96\n",
      "I0427 15:17:49.724342  1096 caffe.cpp:275] Batch 90, loss = 0.126685\n",
      "I0427 15:17:49.765036  1096 caffe.cpp:275] Batch 91, accuracy = 1\n",
      "I0427 15:17:49.765070  1096 caffe.cpp:275] Batch 91, loss = 4.53485e-05\n",
      "I0427 15:17:49.805873  1096 caffe.cpp:275] Batch 92, accuracy = 1\n",
      "I0427 15:17:49.805917  1096 caffe.cpp:275] Batch 92, loss = 0.000115884\n",
      "I0427 15:17:49.846576  1096 caffe.cpp:275] Batch 93, accuracy = 1\n",
      "I0427 15:17:49.846593  1096 caffe.cpp:275] Batch 93, loss = 0.00166215\n",
      "I0427 15:17:49.887145  1096 caffe.cpp:275] Batch 94, accuracy = 1\n",
      "I0427 15:17:49.887162  1096 caffe.cpp:275] Batch 94, loss = 0.000789941\n",
      "I0427 15:17:49.927759  1096 caffe.cpp:275] Batch 95, accuracy = 1\n",
      "I0427 15:17:49.927777  1096 caffe.cpp:275] Batch 95, loss = 0.00399414\n",
      "I0427 15:17:49.968356  1096 caffe.cpp:275] Batch 96, accuracy = 0.97\n",
      "I0427 15:17:49.968374  1096 caffe.cpp:275] Batch 96, loss = 0.0685061\n",
      "I0427 15:17:50.009011  1096 caffe.cpp:275] Batch 97, accuracy = 0.96\n",
      "I0427 15:17:50.009028  1096 caffe.cpp:275] Batch 97, loss = 0.109657\n",
      "I0427 15:17:50.050714  1096 caffe.cpp:275] Batch 98, accuracy = 1\n",
      "I0427 15:17:50.050750  1096 caffe.cpp:275] Batch 98, loss = 0.00602563\n",
      "I0427 15:17:50.091464  1096 caffe.cpp:275] Batch 99, accuracy = 1\n",
      "I0427 15:17:50.091496  1096 caffe.cpp:275] Batch 99, loss = 0.00901524\n",
      "I0427 15:17:50.091506  1096 caffe.cpp:280] Loss: 0.0277724\n",
      "I0427 15:17:50.091523  1096 caffe.cpp:292] accuracy = 0.9911\n",
      "I0427 15:17:50.091538  1096 caffe.cpp:292] loss = 0.0277724 (* 1 = 0.0277724 loss)\n"
     ]
    }
   ],
   "source": [
    "!./caffe/build/tools/caffe test --model=./lenet/lenet_train_test.prototxt --weights=./lenet/lenet_iter_10000.caffemodel --iterations=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting aa.prototxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile aa.prototxt\n",
    "adfasdfasdfasd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_mnist.sh\t\t      lenet_iter_5000.caffemodel   mnist_test_lmdb\r\n",
      "lenet5.png\t\t      lenet_iter_5000.solverstate  mnist_train_lmdb\r\n",
      "lenet_iter_10000.caffemodel   lenet_solver.prototxt\r\n",
      "lenet_iter_10000.solverstate  lenet_train_test.prototxt\r\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "description": "Define, train, and test the classic LeNet with the Python interface.",
  "example_name": "Learning LeNet",
  "include_in_docs": true,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "priority": 2
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
